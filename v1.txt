!pip install -q transformers accelerate datasets peft trl bitsandbytes
!pip install -U bitsandbytes

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from transformers import Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model
import torch

# 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù€ Dataset
dataset = load_dataset("json", data_files="db_questions_dataset.jsonl")

# 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Tokenizer Ùˆ Model (Ù…Ù† Qwen)
model_name = "Qwen/Qwen2.5-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ VRAM
    torch_dtype=torch.float16,
    device_map="auto"
)

# 3ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØµÙŠØºØ© ChatML
def format_chatml(example):
    return {
        "text": f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
                f"<|im_start|>user\n{example['instruction']}<|im_end|>\n"
                f"<|im_start|>assistant\n{example['output']}<|im_end|>"
    }

dataset = dataset.map(format_chatml)
dataset = dataset["train"]

# 4ï¸âƒ£ Tokenization
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# 5ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ LoRA (Ù…Ù† PEFT)
lora_config = LoraConfig(
    r=16,               # Ø¹Ø¯Ø¯ Ø§Ù„Ù€ rank dimensions
    lora_alpha=32,      # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªÙŠ Ù†Ø·Ø¨Ù‚ Ø¹Ù„ÙŠÙ‡Ø§ LoRA
    lora_dropout=0.05,  # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© drop
    bias="none",        # Ù‡Ù„ Ù†Ø¯Ø±Ù‘Ø¨ Ø§Ù„Ù€ bias Ø£Ù… Ù„Ø§
    task_type="CAUSAL_LM"  # Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©: ØªÙˆÙ„ÙŠØ¯ Ù†ØµÙˆØµ
)

model = get_peft_model(model, lora_config)

# 6ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ TrainingArguments
training_args = TrainingArguments(
    output_dir="./qwen_db_finetuned",  # Ù…Ø¬Ù„Ø¯ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    per_device_train_batch_size=2,     # Ø­Ø¬Ù… Ø§Ù„Ø¨Ø§ØªØ´ Ù„ÙƒÙ„ GPU
    gradient_accumulation_steps=4,     # Ø¯Ù…Ø¬ Ø§Ù„Ø¬Ø±ÙŠØ¯Ù†ØªØ§Øª
    warmup_steps=50,                   # Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ³Ø®ÙŠÙ†
    max_steps=200,                     # Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… num_train_epochs)
    learning_rate=2e-5,                # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    fp16=True,                         # ØªØ´ØºÙŠÙ„ Ø¨Ù€ Half Precision
    logging_steps=10,                  # ÙƒÙ„ ÙƒØ§Ù… Ø®Ø·ÙˆØ© ÙŠØ·Ø¨Ø¹ Ù„ÙˆØ¬
    save_steps=100,                    # ÙƒÙ„ ÙƒØ§Ù… Ø®Ø·ÙˆØ© ÙŠØ­ÙØ¸ Ù…ÙˆØ¯ÙŠÙ„
    save_total_limit=2,                # Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
    optim="paged_adamw_8bit",          # Optimizer Ù…Ù† bitsandbytes
    report_to="none"                   # ØªØ¹Ø·ÙŠÙ„ wandb
)

# 7ï¸âƒ£ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù€ Data Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Ù„Ø£Ù†Ù†Ø§ Ù†Ø¯Ø±Ù‘Ø¨ Ø¹Ù„Ù‰ causal language modeling ÙˆÙ„ÙŠØ³ Masked LM
)

# 8ï¸âƒ£ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

# 9ï¸âƒ£ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
trainer.train()

# ğŸ”Ÿ Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
trainer.save_model("./qwen_db_finetuned")
tokenizer.save_pretrained("./qwen_db_finetuned")

print("âœ… Fine-tuning completed successfully!")






# Ø¥Ù†Ø´Ø§Ø¡ Prompt Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
prompt = (
    "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
    "<|im_start|>user\nWhat is normalization in databases?<|im_end|>\n"
    "<|im_start|>assistant\n"
)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
outputs = model.generate(
    **inputs,
    max_new_tokens=150,
    temperature=0.0,   # Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (Ø£Ù‚Ù„ = Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©)
    top_p=0.9,         # Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠ
    repetition_penalty=1.1,  # Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    do_sample=True
)

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†Ø§ØªØ¬
print("\nğŸ§  Model Output:\n")
print(tokenizer.decode(outputs[0], skip_special_tokens=True))



