# ============================================================
# launch_sql_lora_ULTRA_FAST.py ‚Äî Maximum Speed for Tesla T4 / RTX 4080
# ============================================================
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
import torch
import re
from datetime import datetime

# ------------------------------------------------------
# 0Ô∏è‚É£ GPU Setup
# ------------------------------------------------------
if not torch.cuda.is_available():
    raise SystemError("‚ùå No CUDA GPU detected.")

device = torch.device("cuda")
print(f"‚ö° GPU: {torch.cuda.get_device_name(0)}")

# Maximum performance
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('high')

print("=" * 70)

# ------------------------------------------------------
# 1Ô∏è‚É£ Load LoRA Model Properly
# ------------------------------------------------------
lora_model_path = "./qwen3_8B_sql_lora"

# Detect LoRA
try:
    peft_config = PeftConfig.from_pretrained(lora_model_path)
    is_lora = True
    base_model_name = peft_config.base_model_name_or_path
    print(f"‚úÖ Detected LoRA adapter!")
    print(f"üì¶ Base model: {base_model_name}")
except:
    is_lora = False
    print("‚úÖ Detected full model (not LoRA)")

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

print("\n‚è≥ Loading tokenizer...")
if is_lora:
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
else:
    tokenizer = AutoTokenizer.from_pretrained(lora_model_path, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

if is_lora:
    print("‚è≥ Loading base model in 4-bit...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        dtype=torch.float16,
        device_map="auto",
        attn_implementation="sdpa",
        use_cache=True,
        trust_remote_code=True
    )
    
    print("‚è≥ Loading and merging LoRA adapter...")
    model = PeftModel.from_pretrained(base_model, lora_model_path, torch_dtype=torch.float16)
    model = model.merge_and_unload()  # üöÄ Merge for speed
else:
    print("‚è≥ Loading model in 4-bit...")
    model = AutoModelForCausalLM.from_pretrained(
        lora_model_path,
        quantization_config=bnb_config,
        dtype=torch.float16,
        device_map="auto",
        attn_implementation="sdpa",
        use_cache=True,
        trust_remote_code=True
    )

# Torch compile for max speed (PyTorch 2.0+)
try:
    model = torch.compile(model, mode="max-autotune")
    print("‚úÖ Model compiled for speed")
except Exception as e:
    print(f"‚ö†Ô∏è torch.compile unavailable: {e}")

model.eval()
print("‚úÖ Model Ready!")
print("=" * 70)

# ------------------------------------------------------
# 2Ô∏è‚É£ SQL Guard
# ------------------------------------------------------
SQL_KEYWORDS = frozenset([
    "sql", "database", "query", "table", "join", "select", "where",
    "insert", "update", "delete", "index", "primary key", "foreign key",
    "schema", "constraint", "transaction", "view", "trigger", "stored procedure"
])

def guard_filter(q: str) -> bool:
    return any(kw in q.lower() for kw in SQL_KEYWORDS)

# ------------------------------------------------------
# 3Ô∏è‚É£ History Management
# ------------------------------------------------------
history = []

def add_history(q, a):
    history.append({"q": q, "a": a})
    if len(history) > 2:
        history.pop(0)

def format_history():
    return "".join([
        f"<|im_start|>user\n{h['q']}<|im_end|>\n<|im_start|>assistant\n{h['a']}<|im_end|>\n"
        for h in history
    ])

THINKING_RE = re.compile(r'<think>.*?</think>', re.DOTALL)

# ------------------------------------------------------
# 4Ô∏è‚É£ Warmup
# ------------------------------------------------------
print("\nüî• Running warmup...")
warmup_prompt = "<|im_start|>system\nYou are helpful.<|im_end|>\n<|im_start|>user\nHi<|im_end|>\n<|im_start|>assistant\n"
warmup_inputs = tokenizer(warmup_prompt, return_tensors="pt").to(device)

with torch.inference_mode():
    _ = model.generate(**warmup_inputs, max_new_tokens=20, do_sample=False)

print("‚úÖ Warmup complete!")
print("=" * 70)

# ------------------------------------------------------
# 5Ô∏è‚É£ Ultra-Fast SQL Assistant Loop
# ------------------------------------------------------
print("\nü§ñ SQL Assistant Ready! Commands: 'history', 'quit'\n")

SYSTEM = "<|im_start|>system\nYou are a helpful SQL expert. Be concise.<|im_end|>\n"

for i in range(20):
    print(f"\n{'='*70}")
    print(f"üü¶ Question {i+1}/20")
    print(f"{'='*70}")
    
    question = input("SQL question: ").strip()
    
    if question.lower() == "quit":
        print("\nüëã Goodbye!")
        break
    if question.lower() == "history":
        for h in history:
            print(f"\nQ: {h['q']}\nA: {h['a']}")
        continue
    if not guard_filter(question):
        print("‚ùå Please ask about SQL topics.")
        continue
    
    print("üü¢ Processing...\n")
    
    # Build prompt
    prompt = f"{SYSTEM}{format_history()}<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1536).to(device)
    input_len = inputs.input_ids.shape[1]
    
    # Generation (greedy for max speed)
    torch.cuda.synchronize()
    start = datetime.now()
    
    with torch.inference_mode():
        output = model.generate(
            **inputs,
            max_new_tokens=2048,
            do_sample=False,          # Greedy = fastest
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True
        )
    
    torch.cuda.synchronize()
    elapsed = (datetime.now() - start).total_seconds()
    
    # Decode
    full_text = tokenizer.decode(output[0], skip_special_tokens=True)
    if "<|im_start|>assistant" in full_text:
        answer = full_text.split("<|im_start|>assistant")[-1].strip()
    else:
        answer = full_text[len(prompt):].strip()
    
    answer = THINKING_RE.sub('', answer).strip()
    add_history(question, answer)
    
    tokens = len(output[0]) - input_len
    speed = tokens / elapsed if elapsed > 0 else 0
    
    print("=" * 70)
    print("üß† ANSWER:")
    print("=" * 70)
    index = answer.find("</think>")
    answer = answer[index+8:]
    print(answer)
    print("=" * 70)
    print(f"‚ö° {tokens} tokens | {elapsed:.2f}s | {speed:.1f} tok/s")
    print("=" * 70)

print("\n‚úÖ Session complete!")
