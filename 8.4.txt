# ============================================================
# launch_sql_lora_FP16_MAXIMUM_SPEED.py ‚Äî RTX 4080 Super
# ============================================================
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
import torch
import re
from datetime import datetime

# ------------------------------------------------------
# 0Ô∏è‚É£ GPU Setup
# ------------------------------------------------------
if not torch.cuda.is_available():
    raise SystemError("‚ùå No CUDA GPU detected.")

device = torch.device("cuda")
print(f"‚ö° GPU: {torch.cuda.get_device_name(0)}")

# Maximum performance
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('high')

# Clear cache
torch.cuda.empty_cache()

print("=" * 70)

# ------------------------------------------------------
# 1Ô∏è‚É£ Load Model in PURE FP16 (NO QUANTIZATION)
# ------------------------------------------------------
lora_model_path = "./qwen3_8B_sql_lora"

# Detect LoRA
try:
    peft_config = PeftConfig.from_pretrained(lora_model_path)
    is_lora = True
    base_model_name = peft_config.base_model_name_or_path
    print(f"‚úÖ Detected LoRA adapter!")
    print(f"üì¶ Base model: {base_model_name}")
except:
    is_lora = False
    print("‚úÖ Detected full model (not LoRA)")

print("\n‚è≥ Loading tokenizer...")
if is_lora:
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
else:
    tokenizer = AutoTokenizer.from_pretrained(lora_model_path, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

if is_lora:
    print("‚è≥ Loading base model in FP16 (NO quantization)...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    print("‚è≥ Loading and merging LoRA adapter...")
    model = PeftModel.from_pretrained(
        base_model, 
        lora_model_path, 
        torch_dtype=torch.float16
    )
    print("‚è≥ Merging LoRA weights into base model...")
    model = model.merge_and_unload()
else:
    print("‚è≥ Loading model in FP16 (NO quantization)...")
    model = AutoModelForCausalLM.from_pretrained(
        lora_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

# Move to GPU and set to eval
model = model.to(device)
model.eval()

# Check VRAM usage
print(f"üìä VRAM allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
print(f"üìä VRAM reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB")

print("‚úÖ Model Ready!")
print("=" * 70)

# ------------------------------------------------------
# 2Ô∏è‚É£ SQL Guard
# ------------------------------------------------------
SQL_KEYWORDS = frozenset([
    "sql", "database", "query", "table", "join", "select", "where",
    "insert", "update", "delete", "index", "primary key", "foreign key",
    "schema", "constraint", "transaction", "view", "trigger", "stored procedure"
])

def guard_filter(q: str) -> bool:
    return any(kw in q.lower() for kw in SQL_KEYWORDS)

# ------------------------------------------------------
# 3Ô∏è‚É£ History Management
# ------------------------------------------------------
history = []

def add_history(q, a):
    history.append({"q": q, "a": a})
    if len(history) > 2:
        history.pop(0)

def format_history():
    return "".join([
        f"<|im_start|>user\n{h['q']}<|im_end|>\n<|im_start|>assistant\n{h['a']}<|im_end|>\n"
        for h in history
    ])

THINKING_RE = re.compile(r'<think>.*?</think>', re.DOTALL)

# ------------------------------------------------------
# 4Ô∏è‚É£ Warmup (CRITICAL for speed)
# ------------------------------------------------------
print("\nüî• Running warmup (3 iterations for GPU optimization)...")
warmup_prompt = "<|im_start|>system\nYou are helpful.<|im_end|>\n<|im_start|>user\nHi<|im_end|>\n<|im_start|>assistant\n"
warmup_inputs = tokenizer(warmup_prompt, return_tensors="pt").to(device)

for _ in range(3):
    with torch.inference_mode():
        _ = model.generate(
            **warmup_inputs, 
            max_new_tokens=50,
            do_sample=False,
            use_cache=True
        )

torch.cuda.synchronize()
print("‚úÖ Warmup complete!")
print("=" * 70)

# ------------------------------------------------------
# 5Ô∏è‚É£ Ultra-Fast SQL Assistant Loop
# ------------------------------------------------------
print("\nü§ñ SQL Assistant Ready! Commands: 'history', 'quit'\n")

SYSTEM = "<|im_start|>system\nYou are a helpful SQL expert. Be concise.<|im_end|>\n"

for i in range(20):
    print(f"\n{'='*70}")
    print(f"üü¶ Question {i+1}/20")
    print(f"{'='*70}")
    
    question = input("SQL question: ").strip()
    
    if question.lower() == "quit":
        print("\nüëã Goodbye!")
        break
    
    if question.lower() == "history":
        for h in history:
            print(f"\nQ: {h['q']}\nA: {h['a']}")
        continue
    
    if not guard_filter(question):
        print("‚ùå Please ask about SQL topics.")
        continue
    
    print("üü¢ Processing...\n")
    
    # Build prompt (keep it shorter for speed)
    prompt = f"{SYSTEM}{format_history()}<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    
    # Tokenize
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        truncation=True, 
        max_length=1024  # Reduced from 1536 for speed
    ).to(device)
    input_len = inputs.input_ids.shape[1]
    
    # Generation (maximum speed settings)
    torch.cuda.synchronize()
    start = datetime.now()
    
    with torch.inference_mode(), torch.autocast(device_type='cuda', dtype=torch.float16):
        output = model.generate(
            **inputs,
            max_new_tokens=512,  # Reduced from 2048 for faster initial response
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True,
            num_beams=1,
            temperature=None,
            top_p=None,
            top_k=None
        )
    
    torch.cuda.synchronize()
    elapsed = (datetime.now() - start).total_seconds()
    
    # Decode
    full_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    if "<|im_start|>assistant" in full_text:
        answer = full_text.split("<|im_start|>assistant")[-1].strip()
    else:
        answer = full_text[len(prompt):].strip()
    
    answer = THINKING_RE.sub('', answer).strip()
    
    add_history(question, answer)
    
    tokens = len(output[0]) - input_len
    speed = tokens / elapsed if elapsed > 0 else 0
    
    print("=" * 70)
    print("üß† ANSWER:")
    print("=" * 70)
    
    # Clean output
    if "</think>" in answer:
        index = answer.find("</think>")
        answer = answer[index+8:]
    
    print(answer)
    print("=" * 70)
    print(f"‚ö° {tokens} tokens | {elapsed:.2f}s | {speed:.1f} tok/s")
    print(f"üìä VRAM used: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
    print("=" * 70)

print("\n‚úÖ Session complete!")