[
     { "role": "user", "content": "The subject's professor is Dr. Anwar Asal , Chapter 1 — Database Fundamentals | 1.1 Database System — Databases and Database Users | A Database: It is a collection of related data. By data, we mean known facts that can be recorded and that have implicit meaning (e.g., the names, telephone numbers, and addresses of all the people you know). A database has the following implicit properties: A database is a logically coherent collection of data with inherent meaning. A database is designed, built, and populated with data for a specific purpose. It has an intended group of users and some preconceived applications, which these users are interested in. A database represents some aspects of the real world, sometimes called the mini world. Changes to the mini world are reflected in the database. A Database Management System (DBMS): The DBMS is a collection of programs that enables users to create and maintain a database. The DBMS is a general-purpose software system that facilitates the following processes for various applications: Defining a database involves specifying the types of data to be stored, along with detailed descriptions of each data type. Constructing the database is the process of storing the data itself on some storage medium controlled by the DBMS. Manipulating a database includes querying, updating, and generating reports. | Database Versus Traditional File Processing | The database approach has several advantages: 1. Self-contained Nature of a Database System — A fundamental characteristic of the database approach is that the database system contains not only the database itself but also a complete definition or description of the database. This definition is stored in the system catalog. The information stored in the catalog is called metadata. The catalog is used by the DBMS software and occasionally by users who need information about the database structure. In traditional file processing, programs are tied to one specific file, while in DBMS, multiple databases can be accessed through metadata in the catalog. 2. Insulation between Programs and Data — In traditional file systems, data structure is embedded in programs, so any structural change requires code changes. In contrast, a DBMS separates data structure from the application logic. 3. Data Abstraction — A DBMS provides a conceptual view of data, hiding low-level details. A Data Model provides this abstraction using logical concepts like entities and relationships rather than physical storage concepts. 4. Support for Multiple Views — A database typically supports many users, each with different views of the same data — easily managed through a multi-user DBMS. | 1.2 Data Models and Database Design | A Data Model is a type of data abstraction that provides a conceptual representation for data. There are three levels (models) of database design: 1. High-Level Data Model (External or Conceptual Schema) — A concise description of user data requirements. It includes data types, relationships, and constraints using conceptual entities and relationships. It’s implementation-independent and easy for non-technical users to understand. 2. Implementation Data Model — Represents the actual database implementation using a commercial DBMS. Common models include Relational, Network, and Hierarchical. Mapping is required to transform the conceptual schema into the implementation model. 3. Physical Data Model (Internal Schema) — Describes the physical storage structure of the database, including file organization and access paths. | 1.3 Data Modeling Using the Entity-Relationship Model | Introduction: The database design process involves several steps: 1. Requirement Collection and Analysis — Database designers interview users to document and abstract requirements. 2. Creation of the Conceptual Schema — This schema concisely describes the database requirements and includes detailed entities, relationships, and constraints. The output is an ER model (Entity-Relationship model). 3. Database Implementation — The conceptual model is mapped to an implementation model (e.g., relational model) using a chosen DBMS. 4. Physical Database Design — Choosing appropriate storage structures for efficiency and response time. | Entity-Relationship (ER) Model Concepts | The ER Model provides a way to describe data at a high level of abstraction. It is mainly used in the database design phase to model data requirements and translate them into a logical schema. Key concepts: 1. Entity — An entity is a real-world object distinguishable from other objects. Examples: Student, Course, Department. Entities have attributes, which describe their properties — for instance, a Student entity may have attributes like Name, Student_ID, and Email. 2. Entity Set — A collection of entities of the same type. Example: The Students table containing all students in a university. 3. Attribute — Attributes can be: Simple (cannot be divided): e.g., Age, Gender; Composite (can be divided): e.g., Full Name → First Name + Last Name; Derived (computed from other attributes): e.g., Age from Date of Birth; Multivalued (can hold multiple values): e.g., Phone Numbers. 4. Relationship — A relationship is an association between two or more entities. Example: Student enrolls in Course. 5. Relationship Set — A collection of similar relationships. Example: All instances of enrollment between students and courses. 6. Degree of a Relationship — The number of participating entity types: Binary (2 entities) — most common, Ternary (3 entities), Higher degree — rare. | Cardinality Constraints (Mapping Cardinalities) | Cardinality constraints define how many entities participate in a relationship. 1. One-to-One (1:1): Example: Each Department has one Head, and each Head leads one Department. 2. One-to-Many (1:N): Example: One Instructor teaches many Courses. 3. Many-to-Many (M:N): Example: Many Students enroll in many Courses. These constraints are critical in defining the logical schema of the database. | Keys in the ER Model | Keys are attributes (or sets of attributes) used to identify entities uniquely. Super Key: A set of one or more attributes that uniquely identify an entity. Candidate Key: A minimal super key (no redundant attributes). Primary Key: The chosen candidate key to uniquely identify entities in a table. Foreign Key: An attribute that creates a link between two entities. Example: In Students(Student_ID, Name, Email, Course_ID): Student_ID is the primary key, and Course_ID can be a foreign key referencing Courses. | ER Diagrams | ER diagrams are graphical representations of entities, relationships, and constraints. They consist of: Rectangles → Entities, Ellipses → Attributes, Diamonds → Relationships, Lines → Connections between them, Double ellipses → Multivalued attributes, Dashed ellipses → Derived attributes. Example: A Student–Course ER diagram shows Student and Course as entities connected by an Enrolls relationship. | 1.4 Database Languages | Database systems provide several types of languages: 1. Data Definition Language (DDL) — Used to define the database schema. Example: CREATE TABLE Student (Student_ID INT PRIMARY KEY, Name VARCHAR(50), Email VARCHAR(100)); 2. Data Manipulation Language (DML) — Used to query and update data. Procedural DML: User specifies what data and how to get it. Declarative DML (Nonprocedural): User specifies what data only. Example: SELECT Name FROM Student WHERE Student_ID = 10; 3. Data Control Language (DCL) — Used to control access to the database. Example: GRANT SELECT ON Student TO user123;. | 1.5 Database Users and Administrators | Types of database users: 1. End Users — Casual Users: Occasionally access data using queries. Naive Users: Interact through prewritten applications (like banking apps). Parametric Users: Use special commands (e.g., tellers, reservation agents). Stand-alone Users: Maintain personal databases using tools like MS Access. 2. Database Designers — Responsible for identifying data, relationships, and constraints. 3. Database Administrators (DBAs) — Responsible for: Schema definition and modification, Security and authorization, Backup and recovery, Monitoring performance. |  1.6 Advantages of Using the DBMS Approach | 1. Data Independence — Application programs are independent of data storage and structure. 2. Efficient Data Access — DBMSs use indexing, query optimization, and buffering. 3. Data Integrity and Security — Enforces constraints like uniqueness and referential integrity. 4. Concurrent Access — Supports multiple users without conflict. 5. Crash Recovery — DBMS ensures atomic transactions and recovery after failures. 6. Reduced Application Development Time — Because of reusable schema, query languages, and centralized management. | 1.7 Database System Environment | Main components: 1. Hardware — Storage devices, servers, network. 2. Software — DBMS software, OS, application programs. 3. Data — Stored, processed, and managed. 4. Users — Administrators, designers, end-users. 5. Procedures — Guidelines for using and maintaining the system. | 1.8 Classification of Database Management Systems | DBMSs can be classified as: 1. Based on Data Model — Relational DBMS (RDBMS), Hierarchical DBMS, Network DBMS, Object-oriented DBMS. 2. Based on Number of Users — Single-user, Multi-user. 3. Based on Distribution — Centralized DBMS, Distributed DBMS (data across multiple sites). 4. Based on Cost — Open-source (MySQL, PostgreSQL), Commercial (Oracle, SQL Server). | Chapter 2 Normalization for Relational Databases 2.1 Normalization Theory Normalization theory is a set of Concepts and rules that can help in the process of data base design, to ensure the correctness of that design. Normal forms (NF) A relation is said to be in a normal form if and only if it satisfies a specific set of constrains: • Normalized Relation in the (1st NF). • Normalized Relation in the (2nd NF). • Normalized Relation in the (3rd NF). • Boyce-Codd Normal form (BCNF). The First Normal Form Relations First normal form (1NF) states that a relation is said to be in the 1st if and only if all attribute values are atomic. In other words, 1NF disallows relations within relations or relations as attribute values within tuples. The only attribute values permitted by 1NF are single atomic (or indivisible) values. Full Function Dependence (FFD): An attribute Y is Full Function Dependent on attribute X if and only if Y is dependent on X and not dependent on any proper subset of X. Example: If (SSN, PNO) -------> Hours and SSN -------> Hours then Hours is not FFD on (SSN, PNO) because Hours dependent on SSN which is a subset of (SSN, PNO). Consider the DEPARTMENT relation schema shown in Figure 14.1, whose primary key is Dnumber, and suppose that we extend it by including the Dlocations attribute as shown in Figure 2.1 (a). We assume that each department can have a number of locations. The DEPARTMENT schema and a sample relation state are shown in Figure 14.9. As we can see, this is not in 1NF because Dlocations is not an atomic attribute, as illustrated by the first tuple in Figure 2.1 (b). There are two ways we can look at the Dlocations attribute: 27 Normalization for Relational Databases • The domain of Dlocations contains atomic values, but some tuples can have a Set of these values. In this case, Dlocations is not functionally dependent on the primary key Dnumber. • The domain of Dlocations contains sets of values and hence is non-atomic. In this case, Dnumber → Dlocations because each set is considered a single member of the attribute domain. Figure 2.1 DEPARTMENT schema In either case, the DEPARTMENT relation in Figure 2.1 is not in 1NF; in fact, it does not even qualify as a relation. There are three main techniques to achieve first normal form for such a relation: 1. Remove the attribute Dlocations that violates 1NF and place it in a separate relation DEPT_LOCATIONS along with the primary key Dnumber of DEPARTMENT. The primary key of this newly formed relation is the combination {Dnumber, Dlocation}, as shown in Figure 14.2. A distinct tuple in DEPT_LOCATIONS exist for each location of a department. This decomposes the non-1NF relation into two 1NF relations. Figure 2.2 1NF 1st Technique 2. Expand the key so that there will be a separate tuple in the original DEPARTMENT relation for each location of a DEPARTMENT, as shown in Figure 14.9(c). In this case, the primary key becomes the combination {Dnumber, Dlocation}. 28 Database Systems II This solution has the disadvantage of introducing redundancy in the relation and hence is rarely adopted. Figure 2.3 1NF 2nd Technique 3. If a maximum number of values is known for the attribute—for example, if it is known that at most three locations can exist for a department—replace the Dlocations attribute by three atomic attributes: Dlocation1, Dlocation2, and Dlocation3. This solution has the disadvantage of introducing NULL values if most departments have fewer than three locations. Querying on this attribute becomes more difficult. For all these reasons, it is best to avoid this alternative. Of the three solutions above, the first is generally considered best because it does not suffer from redundancy, and it is completely general; it places no maximum limit on the number of values. The Second Normal Form Relations A Relation R is said to be in the (2nd NF) if and only if all non-key attributes are FFD on the Primary Key of the relation. Remark: Any relation with a simple Primary Key is in the (2nd NF). The Third Normal Form Relations A relation is said to be in the (3rd NF) if and only if: • All non-key attributes are FFD on the Primary Key. • All non-key attributes are Mutually Independent. Example: In the Relation Employee (SSN, Name, Sex, Dno, Salary) SSN is the Primary Key and all employees in one department have the same salary. The First Constraint: • SSN  Name • SSN  Sex 29 Normalization for Relational Databases • SSN  Dno • SSN  Salary The Second Constraint: • Dno  Salary Name SSN Sex Dno Salary Mutual Dependence So, it’s not in 3rd NF This relation in the (2nd NF) but not in the (3rd NF). The solution is to divide the relation into two relations without losing information as follows: The Employee (SSN, Name, Sex, Dno, Salary) can be broken into: • The 3rd NF Relation R1 3rd NF (SNN, Name, Sex, Dno) & • The 3rd NF Relation R2 (Dno, Salary). R1 (3rd NF ) SSN Name Sex Dno R2 (3rd NF ) Dno Salary Example: The relation R Employee (SSN, Name, Sex, Dno, Salary, Hours) (SSN,Pno)is the Primary Key (because of the added Hours attribute) and all employees in one department have the same salary. 30 Database Systems II The relation graph: Hours Name SSN Pno Dno Salary This relation is not in the 2nd NF & respectively not in the 3rd NF Solution: We divide the above un-normalized relation into normalized relations as follows: • R1 (SSn, Name, Dno, Salary) in the 2nd NF • R2 (SSN, Pno, hours) in the 3rd NF. Relation R1 (2nd NF) Name SSN Dno SSN Relation R2 (3rd NF) Hours Pno Salary The relations R1 must be divided again into: • R3 (SSn, Name, Dno) in the 3rd NF. • R4 (Dno, Salary) in the 3rd NF. Relation R3 (3rd NF) Name Relation R4 (3rd NF) SSN Dno Dno 31 Salary Normalization for Relational Databases Example: Suppose the Relation R (Dno, Dname, MSSN). Both Dno and Dname are unique. Dname Dno MSSN This relation is not in the 3rd NF but it is normalized since the division of the relation will be un-normalized: Dno Dname Dname The Boyce-Codd Normal form (BCNF) MSSN The 3rd NF is not suitable to check the correctness of the design if the relation has more than one candidate key which can be used as a primary key. The BCNF is suitable to check the correctness of the design for any relation regardless the number of the candidate keys. Determinant: Any attribute (or attribute combinations) on which some other attribute is fully function dependent is a Determinant. Example: The relation R (SSN, Name, Dno, salary, Pno, Hours) mentioned before has three determinants: 1. (SSN, Pno) 2. Dno 3. SSN Definition of the BCNF: A relation R is said to be in BCNF if and only if every Determinant is a Candidate Key. The relation R mentioned in the previous example has three determinants but only one of them is a Candidate Key, so it is not in BCNF. Example: 32 Database Systems II The Relation WORKS (SSN, Name, Pno, Hours). Assume that: • SSN  is unique for each employee and • Name  is also unique for each employee SSN Pno Solution: The Candidate Keys are: • (SNN, Pno) • (Name, Pno) The Determinants are: • (SNN,Pno) Hours • (Name, Pno) Hours • SNN Name Hours Name Pno • Name SNN So, the relation WORKS is not in BCNF and it must be broken into group of relations: 1. The first suggestion is: R1 (SNN, Name) and R2 (SSN, Pno, Hours) 2. The second suggestion is: R1 (SNN, Name) and R2 (Name, Pno, Hours) " } ,
      {"role" : "user" , "content" :"The Second Normal Form Relations A Relation R is said to be in the (2nd NF) if and only if all non-key attributes are FFD on the Primary Key of the relation. Remark: Any relation with a simple Primary Key is in the (2nd NF). The Third Normal Form Relations A relation is said to be in the (3rd NF) if and only if: • All non-key attributes are FFD on the Primary Key. • All non-key attributes are Mutually Independent. Example: In the Relation Employee (SSN, Name, Sex, Dno, Salary) SSN is the Primary Key and all employees in one department have the same salary. The First Constraint: • SSN  Name • SSN  Sex 29 Normalization for Relational Databases • SSN  Dno • SSN  Salary The Second Constraint: • Dno  Salary Name SSN Sex Dno Salary Mutual Dependence So, it’s not in 3rd NF This relation in the (2nd NF) but not in the (3rd NF). The solution is to divide the relation into two relations without losing information as follows: The Employee (SSN, Name, Sex, Dno, Salary) can be broken into: • The 3rd NF Relation R1 3rd NF (SNN, Name, Sex, Dno) & • The 3rd NF Relation R2 (Dno, Salary). R1 (3rd NF ) SSN Name Sex Dno R2 (3rd NF ) Dno Salary Example: The relation R Employee (SSN, Name, Sex, Dno, Salary, Hours) (SSN,Pno)is the Primary Key (because of the added Hours attribute) and all employees in one department have the same salary. 30 Database Systems II The relation graph: Hours Name SSN Pno Dno Salary This relation is not in the 2nd NF & respectively not in the 3rd NF Solution: We divide the above un-normalized relation into normalized relations as follows: • R1 (SSn, Name, Dno, Salary) in the 2nd NF • R2 (SSN, Pno, hours) in the 3rd NF. Relation R1 (2nd NF) Name SSN Dno SSN Relation R2 (3rd NF) Hours Pno Salary The relations R1 must be divided again into: • R3 (SSn, Name, Dno) in the 3rd NF. • R4 (Dno, Salary) in the 3rd NF. Relation R3 (3rd NF) Name Relation R4 (3rd NF) SSN Dno Dno 31 Salary Normalization for Relational Databases Example: Suppose the Relation R (Dno, Dname, MSSN). Both Dno and Dname are unique. Dname Dno MSSN This relation is not in the 3rd NF but it is normalized since the division of the relation will be un-normalized: Dno Dname Dname The Boyce-Codd Normal form (BCNF) MSSN The 3rd NF is not suitable to check the correctness of the design if the relation has more than one candidate key which can be used as a primary key. The BCNF is suitable to check the correctness of the design for any relation regardless the number of the candidate keys. Determinant: Any attribute (or attribute combinations) on which some other attribute is fully function dependent is a Determinant. Example: The relation R (SSN, Name, Dno, salary, Pno, Hours) mentioned before has three determinants: 1. (SSN, Pno) 2. Dno 3. SSN Definition of the BCNF: A relation R is said to be in BCNF if and only if every Determinant is a Candidate Key. The relation R mentioned in the previous example has three determinants but only one of them is a Candidate Key, so it is not in BCNF. Example: 32 Database Systems II The Relation WORKS (SSN, Name, Pno, Hours). Assume that: • SSN  is unique for each employee and • Name  is also unique for each employee SSN Pno Solution: The Candidate Keys are: • (SNN, Pno) • (Name, Pno) The Determinants are: • (SNN,Pno) Hours • (Name, Pno) Hours • SNN Name Hours Name Pno • Name SNN So, the relation WORKS is not in BCNF and it must be broken into group of relations: 1. The first suggestion is: R1 (SNN, Name) and R2 (SSN, Pno, Hours) 2. The second suggestion is: R1 (SNN, Name) and R2 (Name, Pno, Hours) Chapter 3 The Enhanced Entity Relationship Model EER The ER modeling concepts discussed in Chapter 1 are enough for representing many database schemas for traditional database applications, which include many data-processing applications in business and industry. Since the late 1970s, however, designers of database applications have tried to design more accurate database schemas that reflect the data properties and constraints more precisely. This was particularly important for newer applications of database technology, such as databases for engineering design and manufacturing (CAD/CAM), telecommunications, complex software systems, and geographic information systems (GISs), among many other applications. These types of databases have requirements that are more complex than the more traditional applications. This led to the development of additional semantic data modeling concepts that were incorporated into conceptual data models such as the ER model. The EER goal is achieved by incorporation of a semantic data modeling concepts into the conceptual ER Model. These semantic concepts are: 1. Object Oriented Concept a. Superclass & Subclass Relationship b. Attribute & Relationships inheritance 2. The concept of Specialization Looking for the real world from different point of views 3. The concept of Categories Generation of a class which represents the union of entities of other classes. Remark: The cooperating of the previews concepts has the following advantages: 1. Storage Saving 2. Performance Enhancement 3.1 Object Oriented Concepts Features of the Superclass /Subclass Relationship concept on ER: 1. An entity in a subclass is related via the key attribute to its superclass entity. 2. An entity cannot exist in a database by being a member of a subclass unless it is a member in superclass. 3. An entity may be a member in many subclasses, but it is not necessary that every entity in a superclass is a member in subclass. 4. An entity that is a member of a subclass inherits all the attributes of its superclass & also inherits its relationships as well. 5. A member entity of the subclass represents the same real-world entity in the related superclass but in distinct specific role. 35 The Enhanced Entity Relationship Model EER 3.2 Specialization Concept Specialization is the process of defining a set of subclasses of an entity type; this entity type is called the superclass of the specialization. The set of subclasses that forms a specialization is defined based on some distinguishing characteristic of the entities in the superclass. For example, the set of subclasses {SECRETARY, ENGINEER, TECHNICIAN} is a specialization of the superclass EMPLOYEE that distinguishes among employee entities based on the job type of each employee. We may have several specializations of the same entity type based on different distinguishing characteristics. For example, another specialization of the EMPLOYEE entity type may yield the set of subclasses {SALARIED_EMPLOYEE, HOURLY_EMPLOYEE}; this specialization distinguishes among employees based on the method of pay. Figure 3.1 EER diagram notation to represent subclasses and specialization. 36 Database Systems II Remarks: In the previous example in figure 3.1. We can determine three Specifications based on the following characteristics: • Job Type • Rank • Method of Pay 2. The subclasses that define a specialization are attached by lines to a circle which is connected to superclass. 3. The subset symbol on each line connecting a subclass to the circle indicates the direction of the superclass/subclass relationships. 4. Attributes that apply only to entities of a particular subclass are attached to the rectangle representing those subclass & these attributes are called Specific Attributes (or Local Attribute) ex: Typing_Speed of SECRETARY 5. Subclass can participate in a specific relationship types ex: The relationship named Belongs_to in previous example. Constraint and Characteristics of specialization: 1. Definition Constraints: a. Predicate Defined Specialization It is process of defining a condition to determine exactly the entities that will become members of each subclass by placing a condition on the value of some attribute of the superclass, which is called Defining Attribute of the related subclass. b. User Defined Specialization When we have not any condition in order to determine membership in a subclass hence membership is specified individually for each entity by the user and not by any condition that can be evaluated automatically. 2. Disjoints Constraints: a. Disjoint Specialization This means that an entity can be a member of at most one subclass of a specialization (Figure 3.2) 37 The Enhanced Entity Relationship Model EER Figure 3.2 b. Overlapped specialization: This means that an entity can be a member in any number of subclasses of specialization (Figure 3.3) Figure 3.3 3. Participation Constraints: a. Total Participation Specialization It specifies that every entity in a superclass must be a member of at least one subclass in the specialization. b. Partial Participation Specialization It allows an entity in superclass not to belong to any of its subclasses in the specialization. 38 Database Systems II Specialization Hierarchies and Lattices A subclass itself may have further subclasses specified on it, forming a hierarchy or a lattice of specializations (Figure 3.4). 1. Specialization Hierarchy (Tree Inheritance) It is the constraint that every subclass participates as a subclass in only subclass/class relationship. 2. Specialization Lattice (Multiple Inheritance) It is the constraint that a subclass can be a subclass in more than one class/subclass relation. 3.3 Generalization Concept It is the process of defining a generalized entity type from a set of specialized entity types. Generalization is a bottom-up process, the reverse of the specialization process. The set of entity types is said to be generalized into a higher-level entity type. For example, we can define a higher-level entity type EMPLOYEE from the lower-level entity types {SECRETARY, ENGINEER, TECHNICIAN}. In this case, we can call EMPLOYEE a generalization of SECRETARY, ENGINEER, and TECHNICIAN, and these three entity types are specializations of EMPLOYEE. It is important to note that generalization and specialization are inverse processes, and they may be applied interchangeably. Generalization is usually used to reduce the number of relationships and redundancy in the model. 3.4 Categories (Union Types) A category (also called a union type) represents a single superclass/subclass relationship with more than one superclass. A category can be viewed as a subclass of the union of the superclasses. A category is used when an entity type is a subset of the union of two or more entity types. For example, consider a database that keeps track of vehicles owned by a company. There are two entity types, CAR and TRUCK. Both of these have common attributes such as Vehicle_ID, Model, and Make, but TRUCK also has attributes such as Capacity. Now suppose that there is another entity type called VEHICLE_OWNER that represents the owner of a vehicle, and this owner may be a PERSON or a COMPANY. In this case, VEHICLE_OWNER is a category (union type) that is a subset of the union of PERSON and COMPANY. 3.5 Attributes of Superclass/Subclass Relationships Attributes can be defined at any level of the hierarchy, and the inheritance property allows lower-level entities to inherit higher-level attributes. For example, if EMPLOYEE has the attributes Name and Address, and ENGINEER is a subclass, ENGINEER automatically inherits these attributes. Subclasses can also have additional specific attributes such as Degree for ENGINEER or Typing_Speed for SECRETARY. 3.6 Relationship Inheritance Relationships that apply to a superclass are automatically inherited by all its subclasses. For example, if EMPLOYEE is related to DEPARTMENT through a relationship WORKS_FOR, then all subclasses of EMPLOYEE such as ENGINEER and TECHNICIAN also participate in the WORKS_FOR relationship by inheritance. 39 The Enhanced Entity Relationship Model EER Remarks: Inheritance in EER modeling greatly reduces redundancy and improves the clarity of the model by grouping shared attributes and relationships in superclasses and only defining specific details at the subclass level. 3.7 Constraints on Specialization and Generalization When designing EER schemas, constraints are used to specify the semantics of specialization and generalization. These include disjointness constraints, participation constraints, and completeness constraints. Disjointness Constraint specifies whether an entity can be a member of more than one subclass of a specialization. Participation Constraint specifies whether every entity in the superclass must be a member of at least one subclass. Completeness Constraint ensures that the specialization covers all possible entities in the superclass. 3.8 Specialization and Generalization Examples Example 1: Consider an EMPLOYEE entity type with attributes such as Name, Address, and Salary. The company may have different types of employees, such as MANAGER, ENGINEER, and TECHNICIAN. If each subclass has specific attributes (for instance, MANAGER has Bonus, ENGINEER has Degree, TECHNICIAN has Shift), then we can define a specialization based on Job_Type. Another specialization can be based on Payment_Type, with subclasses SALARIED_EMPLOYEE and HOURLY_EMPLOYEE. These specializations can coexist for the same superclass. Example 2: Consider a UNIVERSITY database that has entity types PERSON, STUDENT, and FACULTY. A specialization can be created from PERSON into STUDENT and FACULTY based on Role. Additionally, a generalization can be made from entity types UNDERGRAD_STUDENT and GRAD_STUDENT into STUDENT to reduce redundancy. 3.9 Design Choices for Specialization and Generalization When designing specializations and generalizations, the database designer must decide: 1. Whether to use specialization or generalization. 2. Whether the specialization is disjoint or overlapping. 3. Whether participation is total or partial. 4. Whether to use predicate-defined or user-defined specialization. These choices affect how the data is represented and constrained in the final database schema. 3.10 Mapping EER Model Concepts to the Relational Model The EER model can be implemented in a relational database by mapping its constructs to relational tables. Each entity type becomes a table, and each attribute becomes a column. Subclass-superclass relationships can be mapped in several ways: 1. Create a separate table for each subclass with a primary key that is also a foreign key referencing the superclass table. 2. Combine the superclass and subclass attributes into a single table, with null values for irrelevant attributes. 3. Use one table for the superclass and add indicator attributes to represent subclass membership. Each approach has trade-offs in terms of performance, redundancy, and query complexity. 3.11 EER Diagram Notations In EER diagrams, a circle is used to represent specialization/generalization, with lines connecting the superclass to its subclasses. The subset symbol (⊆) is used to indicate inheritance direction. Disjointness and participation constraints can be labeled as (d) for disjoint, (o) for overlapping, (t) for total participation, and (p) for partial participation. 3.12 EER Model vs ER Model The Enhanced Entity Relationship (EER) model extends the basic ER model by adding concepts like specialization, generalization, and categories. These additions allow database designers to represent more complex real-world situations in a more structured and meaningful way. The ER model focuses mainly on entities and relationships, while the EER model adds hierarchical and inheritance concepts. In short, the EER model provides more expressive power for modeling complex applications. 3.13 Advantages of EER Modeling • Improved expressiveness: EER models allow for the representation of inheritance, specialization, and generalization, which provide more flexibility. • Reduced redundancy: Shared attributes and relationships can be moved to superclasses, reducing duplication. • Better organization: The hierarchical structure improves the logical organization of data. • Semantic clarity: EER diagrams clearly represent complex constraints and relationships that are difficult to express in basic ER models. 3.14 Example of EER in Practice Consider a HOSPITAL database system. The main entity types are PERSON, DOCTOR, NURSE, and PATIENT. The entity PERSON includes general attributes such as Name, Address, and Phone. The entity DOCTOR is a subclass of PERSON and adds attributes such as Specialization and License_Number. Similarly, NURSE is also a subclass of PERSON with attributes such as Shift and Department. PATIENT is a separate entity type with attributes like Admission_Date and Room_Number. Relationships can be defined such as TREATED_BY between PATIENT and DOCTOR, and ASSIGNED_TO between NURSE and PATIENT. This EER design enables flexible and efficient management of hospital data while maintaining normalization and clarity. 3.15 Design Guidelines for EER Models 1. Use specialization and generalization only when they simplify the model. 2. Avoid unnecessary hierarchies that make the schema complex. 3. Clearly define disjointness and participation constraints. 4. Make sure subclass attributes and relationships are truly specific to that subclass. 5. When mapping EER to relational models, choose the method that best balances redundancy and query simplicity. 3.16 Limitations of EER Models Despite their expressiveness, EER models have some limitations: • Implementation complexity: Mapping EER to relational databases can be complicated. • Querying difficulty: Queries involving multiple inheritance levels may become complex. • Tool support: Not all database tools or ORMs support EER concepts directly. • Performance impact: If implemented poorly, EER hierarchies can slow down queries. 3.17 Summary In this chapter, we discussed the Enhanced Entity Relationship (EER) model and its role in advanced database design. We explored how EER extends the traditional ER model by introducing specialization, generalization, and category (union type) concepts. These features allow for more accurate and semantically rich representations of data. We examined examples demonstrating specialization (top-down) and generalization (bottom-up) processes and explained how disjointness, participation, and completeness constraints define subclass behavior. Furthermore, we explored various design and mapping strategies for implementing EER models into relational databases, including table-per-class, table-per-hierarchy, and mixed approaches. Each method has trade-offs related to redundancy, normalization, and performance. Finally, we discussed the advantages and limitations of the EER model in comparison to the basic ER model. The EER model provides a powerful conceptual framework that bridges the gap between high-level data modeling and relational database implementation. It is widely used in modern database design to create schemas that are both semantically rich and structurally efficient. Review Questions: 1. What is the difference between specialization and generalization? 2. Explain the concept of inheritance in EER models. 3. What are the main constraints used in specialization/generalization? 4. Describe the purpose of categories (union types). 5. How can specialization/generalization be represented in EER diagrams? 6. List and explain the advantages of EER over ER. 7. Discuss the limitations of EER modeling. 40 Database Systems II "   } ,
      {"role" : "user" , "content" : "The name SQL is presently expanded as Structured Query Language. Originally, SQL was called SEQUEL (Structured English QUEry Language) and was designed and implemented at IBM Research as the interface for an experimental relational database system called SYSTEM R. SQL is now the standard language for commercial relational DBMSs. The American National Standards Institute (ANSI) and the International Organization for Standardization (ISO) have agreed upon a standard version of SQL, called SQL-86 or SQL1. A revised and much expanded standard called SQL2 or SQL-92 was subsequently developed. The SQL standard has evolved since then, and the latest standard is referred to as SQL:2011 (ISO/IEC 9075). SQL is a comprehensive database language: It has statements for data definitions, queries, and updates. It also includes facilities for defining views, specifying security and authorization, defining integrity constraints, and specifying transaction controls. In this chapter, we present an overview of the SQL language. We start with data definition statements, then introduce queries, inserts, deletes, and updates. Later sections discuss constraints, views, and schema modification statements. Figure 4.1 shows the schema for the COMPANY database, which will be used in examples throughout this chapter. The schema consists of six relations: EMPLOYEE, DEPARTMENT, PROJECT, WORKS_ON, DEPENDENT, and DEPT_LOCATIONS. For convenience, we reproduce the relational schema from Chapter 3 in Figure 4.1. The COMPANY database keeps track of a company’s employees, departments, and projects. Information about each employee includes name, social security number, address, salary, sex, birth date, and the number of the department that the employee works for. Information about each department includes the department name, number, manager of the department, and the locations of the department. Information about each project includes the project name, number, location, and the department that controls the project. The relation WORKS_ON keeps track of the projects that employees work on, including the number of hours per week that an employee works on each project. The DEPENDENT relation keeps track of each employee’s dependents.Figure mentioned: Figure 4.1 4.1 Data Definition, Constraints, and Schema Changes The SQL DDL allows the specification of not only a set of relations but also information about each relation, including the schema for each relation, the data types of attributes, and integrity constraints. The schema for each relation is specified by a CREATE TABLE statement. The general form of this statement is: CREATE TABLE table_name ( column_name data_type [column_constraints], ... [table_constraints]); The parts enclosed in square brackets [ ] are optional. For example, Figure 4.2 shows the SQL DDL statements for the COMPANY database schema of Figure 4.1. SQL allows a variety of basic data types for attributes, including numeric, character-string, bit-string, Boolean, date, and time. The most commonly used numeric data types are INTEGER (or INT) and SMALLINT for integers of various sizes, and DECIMAL(i, j) or NUMERIC(i, j) for fixed-point (exact) numbers, where i digits are used to store the value, and j of these are to the right of the decimal point. FLOAT(n), REAL, and DOUBLE PRECISION are used for approximate numeric data types. For character-string data types, SQL uses CHAR(n) for a fixed-length character string of length n, and VARCHAR(n) for a variable-length string with a maximum of n characters. The DATE data type has the form DATE 'YYYY-MM-DD'; for example, DATE '2014-09-27' represents September 27, 2014. The TIME data type has the form TIME 'HH:MM:SS' for hours, minutes, and seconds. SQL also has a TIMESTAMP data type, which includes both date and time information. The SQL standard specifies how data of each type should be represented and how operations on such data are to be carried out. SQL provides a way to specify integrity constraints on attributes and tables, such as primary keys, foreign keys, and constraints on attribute values (e.g., domain constraints). Domain constraints are the most elementary form of integrity constraint. They define the permissible values for an attribute. For example, the domain for attribute Sex of EMPLOYEE could be the set of two values: {‘M’, ‘F’}. SQL allows the specification of domains as named data types, but this feature is not widely supported in commercial DBMSs. Instead, most systems specify the data type directly with each attribute definition.Figure mentioned: Figure 4.2 The basic data definition statements in SQL can also include specifications of key attributes and referential integrity constraints. For example, when a relation schema includes a primary key, it is specified by adding PRIMARY KEY (attribute_list) to the CREATE TABLE statement. For foreign keys, the syntax FOREIGN KEY (attribute_list) REFERENCES relation_name(attribute_list) is used. This ensures that a tuple in one relation (the referencing relation) must refer to an existing tuple in another relation (the referenced relation). Figure 4.2 shows sample data definition statements in SQL for the relational database schema shown in Figure 4.1. Notice that some of the constraints, such as primary keys, foreign keys, and NOT NULL attributes, are explicitly specified. For example, the Dnumber attribute of the DEPARTMENT relation is the primary key, and the Dno attribute of EMPLOYEE is a foreign key that references DEPARTMENT(Dnumber). Similarly, the Dnum attribute of PROJECT references DEPARTMENT(Dnumber), and Essn in WORKS_ON references EMPLOYEE(Ssn). The SQL language also allows other constraints to be included in the table definition by using the CHECK clause. For example, one could write CHECK (Salary > 0) to ensure that no employee has a negative salary. Constraints can be specified either at the column level (within the column definition) or at the table level (after all columns have been defined). Column-level constraints refer to a single column, while table-level constraints may refer to multiple columns. SQL also allows DEFAULT values for attributes, which are automatically assigned when no value is specified during insertion. For example, specifying DEFAULT 40 for an Hours attribute in WORKS_ON would automatically assign a value of 40 when no value is provided for Hours in an INSERT statement. Constraints are crucial for maintaining the consistency and integrity of the database. When data is inserted, deleted, or updated, the DBMS checks all integrity constraints. If a constraint is violated, the DBMS rejects the operation and may return an error message. Some systems allow users to temporarily disable constraint checking, for example, during bulk data loading, to improve performance. However, it is generally advisable to ensure all constraints are satisfied once the loading process is complete.Figure mentioned: Figure 4.2 Sometimes it is necessary to modify the schema of a relation after it has been created. SQL provides the ALTER TABLE statement for this purpose. Using this statement, we can add or drop attributes, add or drop constraints, and rename columns. For example, suppose we want to add an attribute called Birthplace to the EMPLOYEE relation. We can write: ALTER TABLE EMPLOYEE ADD Birthplace VARCHAR(25); Similarly, to drop an attribute, we can use: ALTER TABLE EMPLOYEE DROP COLUMN Birthplace; Not all DBMSs support dropping attributes because it may lead to loss of information. The DROP TABLE statement is used to remove a relation (base table) completely, including its definition and all associated constraints and data. For example: DROP TABLE DEPENDENT; This command deletes the table DEPENDENT and all its data permanently from the database. If a table is referenced by a foreign key in another table, the DBMS usually does not allow it to be dropped unless the referencing constraint is also removed or the CASCADE option is specified. To delete all the tuples from a table but keep its definition, SQL provides the DELETE FROM statement (discussed later) or TRUNCATE TABLE, depending on the DBMS. In addition to defining base relations, SQL allows the creation of views using the CREATE VIEW statement. A view is a virtual table that is defined by a query. The view itself does not store data but derives its contents dynamically when accessed. Views are useful for security, convenience, and logical data independence. For example, to create a view that lists employee names and department names, one can write: CREATE VIEW EMP_DEPT AS SELECT Fname, Lname, Dname FROM EMPLOYEE, DEPARTMENT WHERE EMPLOYEE.Dno = DEPARTMENT.Dnumber; This view can now be queried like a regular table. Users can be granted access to views rather than base tables to restrict data exposure. Views can also simplify complex queries and provide a consistent interface even if the underlying schema changes. Figure mentioned: No figure explicitly referenced in this section. 4.2 Basic Queries in SQL The most important aspect of SQL is its capability for data retrieval, which is accomplished using the SELECT statement. SQL uses a single SELECT statement to handle queries involving selection, projection, and join operations. The general form of the SELECT statement is: SELECT [DISTINCT | ALL] attribute_list FROM table_list [WHERE condition] [GROUP BY grouping_attribute_list] [HAVING group_condition] [ORDER BY ordering_attribute_list]; The SELECT clause specifies which attributes (columns) are to appear in the result. The FROM clause specifies the relations (tables) that are to be accessed. The WHERE clause specifies conditions on tuples to be selected. For example, to retrieve the names and salaries of all employees who work in department number 5, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE WHERE Dno = 5; SQL is case-insensitive for keywords, so SELECT, select, and Select are equivalent. String constants, however, are case-sensitive. SQL allows arithmetic operations in the SELECT and WHERE clauses. For instance, to retrieve the names of employees and their annual salaries (assuming 12 months per year), we can write: SELECT Fname, Lname, Salary * 12 AS Annual_Salary FROM EMPLOYEE; The keyword AS allows renaming of attributes in the query result. The keyword DISTINCT can be used to eliminate duplicate tuples from the result. For example, to retrieve all distinct salary values from the EMPLOYEE relation, we can write: SELECT DISTINCT Salary FROM EMPLOYEE; Without DISTINCT, duplicate values would appear. The keyword ALL is the default and is rarely specified explicitly. The WHERE clause can include comparison operators such as =, <, <=, >, >=, and <> (not equal), as well as logical connectives AND, OR, and NOT. Conditions can be nested using parentheses. SQL also supports pattern matching using the LIKE operator with wildcard characters ‘%’ (for any sequence of characters) and ‘_’ (for a single character). For example, to find all employees whose names start with the letter ‘J’, we can write: SELECT Fname, Lname FROM EMPLOYEE WHERE Fname LIKE 'J%'; Conditions can also involve range checking using BETWEEN, membership testing using IN, and null checking using IS NULL. For example, SELECT * FROM EMPLOYEE WHERE Salary BETWEEN 30000 AND 60000; retrieves employees with salaries in that range.Figure mentioned: (No figure explicitly referenced in this section) SQL allows combining multiple conditions in the WHERE clause using logical connectives. For example, to retrieve the names of employees who work in department 5 and whose salary exceeds $30,000, we can write: SELECT Fname, Lname FROM EMPLOYEE WHERE Dno = 5 AND Salary > 30000; To retrieve employees who work either in department 5 or department 4, we use OR: SELECT Fname, Lname FROM EMPLOYEE WHERE Dno = 5 OR Dno = 4; The NOT operator negates a condition. For instance, SELECT Fname, Lname FROM EMPLOYEE WHERE NOT (Dno = 5); retrieves employees who do not work in department 5. SQL supports comparison between attributes of different relations in the WHERE clause, enabling join operations. For example, to retrieve the names of employees and the names of their departments, we can write: SELECT Fname, Lname, Dname FROM EMPLOYEE, DEPARTMENT WHERE EMPLOYEE.Dno = DEPARTMENT.Dnumber; This query performs a Cartesian product of the EMPLOYEE and DEPARTMENT tables, then selects only those tuples where the department numbers match. The result contains all employees along with their corresponding department names. SQL allows renaming of relations using the AS keyword, which can simplify queries with long relation names or multiple self-joins. For example: SELECT E.Fname, E.Lname, D.Dname FROM EMPLOYEE AS E, DEPARTMENT AS D WHERE E.Dno = D.Dnumber; Using aliases like E and D makes the query shorter and more readable. In some cases, we need to compare tuples within the same relation—for example, finding employees who have the same salary as another employee. This requires a self-join. Suppose we want to retrieve pairs of employees who earn the same salary. We can write: SELECT E1.Fname, E1.Lname, E2.Fname, E2.Lname FROM EMPLOYEE AS E1, EMPLOYEE AS E2 WHERE E1.Salary = E2.Salary AND E1.Ssn <> E2.Ssn; The condition E1.Ssn <> E2.Ssn ensures that we do not compare the same employee with themselves. SQL’s flexibility allows complex conditions, and parentheses can be used to control the order of evaluation. The result of a query is always a table, and SQL permits nesting of queries (subqueries) within other queries. We will discuss subqueries later in this chapter.Figure mentioned: (No figure explicitly referenced in this section) SQL provides several additional features for retrieving and organizing data. One of the most useful is the ORDER BY clause, which allows sorting the query results based on one or more attributes. For example, to retrieve employee names and salaries ordered by salary in ascending order, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE ORDER BY Salary; By default, ORDER BY sorts in ascending order (ASC). To sort in descending order, we use the DESC keyword: SELECT Fname, Lname, Salary FROM EMPLOYEE ORDER BY Salary DESC; Multiple sorting attributes can be specified. For instance, to sort employees first by department number and then by salary (within each department), we write: SELECT Fname, Lname, Dno, Salary FROM EMPLOYEE ORDER BY Dno, Salary DESC; SQL allows retrieving all attributes of a relation using an asterisk (*). For example, SELECT * FROM EMPLOYEE; retrieves all columns for all employees. However, using * is generally discouraged in production queries because it can reduce performance and clarity. The GROUP BY clause in SQL is used to divide tuples into groups based on the value of one or more attributes. Aggregate functions such as COUNT, SUM, AVG, MIN, and MAX can be applied to each group. For example, to find the average salary for each department, we can write: SELECT Dno, AVG(Salary) AS Avg_Salary FROM EMPLOYEE GROUP BY Dno; Each group corresponds to a department number (Dno). The result lists one tuple per department with the average salary of its employees. SQL also provides the HAVING clause, which is used to specify conditions on groups rather than on individual tuples. For example, to retrieve departments whose average salary exceeds $40,000, we can write: SELECT Dno, AVG(Salary) AS Avg_Salary FROM EMPLOYEE GROUP BY Dno HAVING AVG(Salary) > 40000; The HAVING clause is similar to WHERE but applies to grouped results. The ORDER BY clause can still be used with GROUP BY to control the output order. For instance, SELECT Dno, AVG(Salary) FROM EMPLOYEE GROUP BY Dno HAVING AVG(Salary) > 40000 ORDER BY AVG(Salary) DESC; SQL queries can thus perform powerful data analysis directly within the database system.Figure mentioned: (No figure explicitly referenced in this section) SQL supports nested queries, also known as subqueries, which allow a query to be embedded within another query. The inner query is executed first, and its result is used by the outer query. Subqueries can appear in various parts of an SQL statement, such as the WHERE, FROM, or HAVING clauses. For example, to retrieve the names of employees who work in the department managed by ‘James Borg’, we can write: SELECT Fname, Lname FROM EMPLOYEE WHERE Dno = (SELECT Dnumber FROM DEPARTMENT WHERE Mgr_ssn = (SELECT Ssn FROM EMPLOYEE WHERE Fname = 'James' AND Lname = 'Borg')); In this example, the innermost subquery retrieves the Ssn of James Borg, the next subquery retrieves the department number managed by that Ssn, and the outer query retrieves the employees who work in that department. Subqueries may return a single value (scalar subquery) or a set of values. When a subquery returns multiple values, it is often used with the IN operator. For instance, to retrieve names of employees who work in departments located in ‘Houston’, we can write: SELECT Fname, Lname FROM EMPLOYEE WHERE Dno IN (SELECT Dnumber FROM DEPARTMENT WHERE Dlocation = 'Houston'); Subqueries can also be used in the FROM clause to define temporary tables for further querying. For example, to retrieve departments with the highest average salary, we can write: SELECT Dno, Avg_Sal FROM (SELECT Dno, AVG(Salary) AS Avg_Sal FROM EMPLOYEE GROUP BY Dno) AS Dept_Avg WHERE Avg_Sal = (SELECT MAX(AVG(Salary)) FROM EMPLOYEE GROUP BY Dno); SQL allows the use of comparison operators such as =, >, <, >=, <=, and <> with subqueries that return a single value. For example, to retrieve employees whose salary is greater than the average salary of all employees, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE WHERE Salary > (SELECT AVG(Salary) FROM EMPLOYEE); Correlated subqueries reference attributes from the outer query and are executed once for each tuple of the outer query. For example, to retrieve employees who earn more than the average salary of their department, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE AS E WHERE Salary > (SELECT AVG(Salary) FROM EMPLOYEE WHERE Dno = E.Dno); Here, the subquery depends on the outer query because it uses E.Dno. Correlated subqueries are powerful but can be less efficient because they require repeated execution.Figure mentioned: (No figure explicitly referenced in this section) " } ,  
      {"role" : "user" , "content" : "SQL includes several operators that allow comparison between a value and the result of a subquery that returns a set of values. These operators include IN, ANY, ALL, and EXISTS. The IN operator tests whether a value matches any value in a list or subquery result. For example, to retrieve the names of employees who work in departments located in ‘Houston’ or ‘Stafford’, we can write: SELECT Fname, Lname FROM EMPLOYEE WHERE Dno IN (SELECT Dnumber FROM DEPARTMENT WHERE Dlocation IN ('Houston', 'Stafford')); The ANY (or SOME) operator compares a value to each value returned by a subquery and returns TRUE if the comparison is TRUE for at least one value. For instance, to find employees whose salary is greater than the salary of at least one employee in department 5, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE WHERE Salary > ANY (SELECT Salary FROM EMPLOYEE WHERE Dno = 5); The ALL operator returns TRUE if the comparison is TRUE for all values returned by the subquery. For example, to find employees whose salary is greater than all employees in department 5, we can write: SELECT Fname, Lname, Salary FROM EMPLOYEE WHERE Salary > ALL (SELECT Salary FROM EMPLOYEE WHERE Dno = 5); The EXISTS operator checks whether the result of a subquery is non-empty. It returns TRUE if the subquery returns at least one tuple. For example, to retrieve the names of departments that have at least one employee, we can write: SELECT Dname FROM DEPARTMENT WHERE EXISTS (SELECT * FROM EMPLOYEE WHERE DEPARTMENT.Dnumber = EMPLOYEE.Dno); The NOT EXISTS operator is the opposite; it returns TRUE if the subquery returns no tuples. For example, to find departments that have no employees, we can write: SELECT Dname FROM DEPARTMENT WHERE NOT EXISTS (SELECT * FROM EMPLOYEE WHERE DEPARTMENT.Dnumber = EMPLOYEE.Dno); Subqueries using EXISTS or NOT EXISTS are often correlated because they depend on attributes from the outer query. The EXISTS construct is particularly useful for expressing universal or existential conditions in SQL. It is important to note that the EXISTS operator ignores the actual values returned by the subquery; it only checks for the existence of tuples. SQL optimizers often handle EXISTS queries efficiently. SQL also provides the capability to perform set operations on the results of multiple SELECT statements. These include UNION, INTERSECT, and EXCEPT (or MINUS in some systems). The UNION operation combines the results of two queries and eliminates duplicate tuples unless UNION ALL is specified. For example, to retrieve the social security numbers of all employees who either work in department 5 or directly supervise an employee, we can write: SELECT Ssn FROM EMPLOYEE WHERE Dno = 5 UNION SELECT Super_ssn FROM EMPLOYEE WHERE Super_ssn IS NOT NULL; The result contains each distinct Ssn that appears in either of the two query results. If we use UNION ALL, duplicates are not removed. The INTERSECT operation returns tuples that appear in both query results. For instance, to find employees who both work in department 5 and supervise someone, we can write: SELECT Ssn FROM EMPLOYEE WHERE Dno = 5 INTERSECT SELECT Super_ssn FROM EMPLOYEE WHERE Super_ssn IS NOT NULL; Similarly, the EXCEPT (or MINUS) operation returns tuples that appear in the first query but not in the second. For example, to find employees who work in department 5 but do not supervise anyone, we can write: SELECT Ssn FROM EMPLOYEE WHERE Dno = 5 EXCEPT SELECT Super_ssn FROM EMPLOYEE WHERE Super_ssn IS NOT NULL; All queries participating in a set operation must produce the same number of columns and compatible data types. SQL also allows nested set operations, with parentheses controlling evaluation order. Some DBMSs may not support INTERSECT or EXCEPT; instead, equivalent results can often be achieved using subqueries with IN, EXISTS, or joins. Another useful SQL feature is the CASE expression, which allows conditional logic within queries. The CASE expression can appear in the SELECT, WHERE, or ORDER BY clauses. For example, to categorize employees based on salary level, we can write: SELECT Fname, Lname, Salary, CASE WHEN Salary < 30000 THEN 'Low' WHEN Salary BETWEEN 30000 AND 60000 THEN 'Medium' ELSE 'High' END AS Salary_Level FROM EMPLOYEE; The CASE construct improves query flexibility by embedding conditional computation directly in SQL. It is similar to the switch or if-else statements in programming languages. SQL includes several features that make it possible to perform updates on data stored in relational tables. The three main data modification commands are INSERT, DELETE, and UPDATE. The INSERT statement is used to add new tuples (rows) to a relation. For example, to insert a new employee record, we can write: INSERT INTO EMPLOYEE (Fname, Lname, Ssn, Bdate, Address, Sex, Salary, Super_ssn, Dno) VALUES ('John', 'Smith', '123456789', '1965-01-09', '731 Fondren, Houston, TX', 'M', 30000, '333445555', 5); If the list of attributes is omitted, values must be provided for all columns in the same order as defined in the schema. It is also possible to insert multiple tuples at once by specifying a list of VALUES clauses separated by commas, or by using an INSERT INTO ... SELECT statement to insert the result of a query. For example, to copy all employees from department 5 into a new table TEMP_EMP, we can write: INSERT INTO TEMP_EMP SELECT * FROM EMPLOYEE WHERE Dno = 5; The DELETE statement removes tuples that satisfy a specified condition. For instance, to delete all employees who work in department 5, we can write: DELETE FROM EMPLOYEE WHERE Dno = 5; If the WHERE clause is omitted, all tuples in the relation are deleted, effectively clearing the table. To delete all tuples efficiently while preserving the table definition, some DBMSs provide the TRUNCATE TABLE command. The UPDATE statement modifies attribute values of tuples that satisfy a given condition. For example, to give a 10 percent raise to all employees in department 5, we can write: UPDATE EMPLOYEE SET Salary = Salary * 1.1 WHERE Dno = 5; Multiple attributes can be updated in one command by separating assignments with commas. SQL allows subqueries in UPDATE and DELETE statements. For example, to increase the salary of employees who earn less than the average salary, we can write: UPDATE EMPLOYEE SET Salary = Salary * 1.1 WHERE Salary < (SELECT AVG(Salary) FROM EMPLOYEE); SQL ensures referential integrity during updates and deletions, preventing violations of foreign key constraints unless cascading actions are defined. When defining foreign key constraints in SQL, it is possible to specify actions that should take place automatically when a referenced tuple is deleted or its key is updated. These are called referential triggered actions. SQL provides several options: CASCADE, SET NULL, SET DEFAULT, and NO ACTION. The default behavior is NO ACTION, which means that the DBMS rejects the modification that would violate referential integrity. For example, if we try to delete a department that still has employees assigned to it, the DBMS will prevent the deletion unless those employees are reassigned or removed first. With CASCADE, the deletion or update propagates automatically. For example, if the DEPARTMENT table’s primary key Dnumber is referenced by the foreign key Dno in EMPLOYEE, we can declare the constraint as: FOREIGN KEY (Dno) REFERENCES DEPARTMENT(Dnumber) ON DELETE CASCADE ON UPDATE CASCADE; In this case, if a department is deleted, all employees in that department will also be deleted automatically. Similarly, if the department number is changed, all corresponding Dno values in EMPLOYEE are updated. The SET NULL option sets the foreign key attribute to NULL when the referenced tuple is deleted or updated, while SET DEFAULT assigns a default value defined in the schema. These options provide flexibility in maintaining referential integrity. However, CASCADE should be used with caution, as it can cause large cascaded deletions unintentionally. SQL also supports the concept of constraints to enforce data consistency. Constraints can be specified at the column level or table level, and they include NOT NULL, UNIQUE, CHECK, PRIMARY KEY, and FOREIGN KEY. For example, to ensure that no employee has a negative salary, we can use: CHECK (Salary >= 0); The CHECK constraint can include arithmetic expressions and logical conditions. SQL also allows the use of domain constraints through user-defined domains. For instance, CREATE DOMAIN SalaryType AS DECIMAL(10,2) CHECK (VALUE >= 0); defines a domain that can then be used for attributes requiring non-negative decimal values. Constraints are automatically enforced by the DBMS whenever data modification statements are executed. Violations result in errors, and the operation is rejected. SQL provides powerful mechanisms for defining access privileges and controlling which users can read or modify data. The GRANT and REVOKE statements are used to manage user authorization. Each relation or view can have specific privileges associated with it, such as SELECT, INSERT, UPDATE, DELETE, and REFERENCES. For example, to allow user ‘U1’ to retrieve and insert data into the EMPLOYEE table, we can write: GRANT SELECT, INSERT ON EMPLOYEE TO U1; The user U1 can now perform SELECT and INSERT operations on EMPLOYEE. To revoke these privileges later, we can use: REVOKE SELECT, INSERT ON EMPLOYEE FROM U1; Privileges can be granted to multiple users, to all users (using the keyword PUBLIC), or to roles that group multiple users. SQL also supports the WITH GRANT OPTION clause, which allows the recipient of a privilege to pass it on to other users. For example, GRANT SELECT ON EMPLOYEE TO U1 WITH GRANT OPTION; allows U1 to grant SELECT privileges on EMPLOYEE to others. The REVOKE statement removes privileges and may cascade to revoke dependent grants. Authorization can be managed at different levels: table, column, view, or even schema level, depending on the DBMS. Views are especially useful for implementing security since users can be granted access to a subset of the data without exposing the full table. For instance, a view may restrict employees’ salary information while allowing access to names and departments. SQL also provides transaction control commands to ensure data consistency and reliability. A transaction is a sequence of SQL statements that must be executed as a single logical unit of work. The main transaction control commands are COMMIT, ROLLBACK, and SAVEPOINT. COMMIT makes all changes made during the transaction permanent, while ROLLBACK undoes all changes since the last COMMIT. For example, if we execute several UPDATE statements and then realize one was incorrect, issuing a ROLLBACK will revert the database to its previous consistent state. SAVEPOINT allows partial rollbacks to specific points within a transaction. Transactions ensure the ACID properties: Atomicity, Consistency, Isolation, and Durability, which are fundamental to reliable database operation. SQL supports mechanisms for concurrent access and recovery to maintain database integrity in multi-user environments. When multiple users or programs access the database simultaneously, it is essential to prevent conflicts and ensure that transactions execute correctly. Most relational DBMSs provide concurrency control using locking or timestamp techniques. SQL includes the concept of isolation levels, which define the degree to which a transaction must be isolated from others. The standard isolation levels are READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE. At the READ UNCOMMITTED level, transactions may see uncommitted changes made by others, leading to dirty reads. READ COMMITTED prevents dirty reads but allows nonrepeatable reads, where a repeated query may return different results if another transaction modifies data in between. REPEATABLE READ prevents both dirty and nonrepeatable reads but may still allow phantom reads, where new rows are inserted by another transaction. The SERIALIZABLE level provides full isolation, ensuring that transactions behave as if executed sequentially. Higher isolation levels reduce concurrency but improve consistency. SQL allows setting the isolation level using the SET TRANSACTION command, for example: SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; The COMMIT and ROLLBACK statements mark transaction boundaries. Many DBMSs operate in autocommit mode, where each individual SQL statement is treated as a separate transaction unless explicitly disabled. To group multiple statements into a single transaction, autocommit must be turned off. For example: SET AUTOCOMMIT = OFF; START TRANSACTION; UPDATE EMPLOYEE SET Salary = Salary * 1.05 WHERE Dno = 5; DELETE FROM DEPENDENT WHERE Essn NOT IN (SELECT Ssn FROM EMPLOYEE); COMMIT; If an error occurs during execution, issuing ROLLBACK will undo all changes since START TRANSACTION. Recovery mechanisms ensure that the database can be restored to a consistent state after system failures, using logs that record all changes. SQL provides the foundation for recovery, while DBMSs implement it internally through checkpoints and write-ahead logging. SQL also provides facilities for defining and managing indexes to improve query performance. An index is a data structure that allows faster access to rows based on the values of one or more columns. Although the creation of indexes is generally the responsibility of the database administrator, SQL includes the CREATE INDEX statement. For example, to create an index on the Salary attribute of the EMPLOYEE table, we can write: CREATE INDEX Emp_Salary_Index ON EMPLOYEE(Salary); Once created, the DBMS automatically maintains the index when data is inserted, updated, or deleted. Indexes can be unique or non-unique; a unique index ensures that no two tuples have the same value for the indexed attribute(s). For example, CREATE UNIQUE INDEX Emp_Ssn_Index ON EMPLOYEE(Ssn); SQL also allows dropping an index using the DROP INDEX command. The choice of which attributes to index depends on the types of queries most frequently executed. Indexes speed up retrieval but may slow down updates due to maintenance overhead. Many DBMSs automatically create indexes for primary keys and unique constraints. SQL additionally supports the creation of schemas, which are logical groupings of related database objects such as tables, views, and indexes. A schema helps organize the database and manage access privileges. The CREATE SCHEMA statement can be used to define a new schema and its contents. For example: CREATE SCHEMA COMPANY AUTHORIZATION JSMITH; Within this schema, tables and other objects can be created and referenced using qualified names like COMPANY.EMPLOYEE. SQL also supports the concept of a catalog, which contains information about all schemas in the database environment. Catalogs and schemas together define the overall structure of a database. Modern relational systems also support procedural extensions to SQL, such as PL/SQL (in Oracle) or Transact-SQL (in SQL Server), which allow programmers to write stored procedures, triggers, and functions that execute SQL statements and procedural logic together. These extensions provide looping, conditional statements, exception handling, and modular programming capabilities, enabling more complex data manipulation and business rules directly within the database. odern SQL standards, such as SQL:1999 and SQL:2003, introduced a variety of advanced features that extend the language beyond basic relational capabilities. Among these are triggers, stored procedures, recursive queries, user-defined types, and object-relational extensions. A trigger is a special type of procedure that is automatically executed by the DBMS when a specified database event occurs, such as an insertion, deletion, or update on a table. Triggers are often used to enforce complex integrity constraints, maintain audit trails, or automatically update derived data. For example, to automatically record any deletion from the EMPLOYEE table into a separate AUDIT table, we can define a trigger as follows: CREATE TRIGGER Audit_Delete AFTER DELETE ON EMPLOYEE FOR EACH ROW INSERT INTO AUDIT(Essn, Action, ActionDate) VALUES (OLD.Ssn, 'DELETE', CURRENT_DATE); In this example, the OLD keyword refers to the deleted row, and the trigger executes after each delete operation. Stored procedures, on the other hand, are named collections of SQL statements and procedural logic that can be invoked by applications or other SQL statements. They are stored in the database and executed on the server side, improving performance and maintainability. For instance, CREATE PROCEDURE Raise_Salary (IN DeptNo INT, IN Percent DECIMAL(3,2)) BEGIN UPDATE EMPLOYEE SET Salary = Salary * (1 + Percent/100) WHERE Dno = DeptNo; END; Once created, the procedure can be invoked using CALL Raise_Salary(5, 10); to give a 10% raise to employees in department 5. SQL also supports recursive queries using the WITH RECURSIVE clause, which enables the expression of hierarchical and graph-related queries. For example, to find all subordinates of a given employee, a recursive query can reference itself to traverse the employee-supervisor relationship. These capabilities make SQL a complete and powerful data manipulation and definition language suitable for a wide range of applications. In addition to traditional query and data definition facilities, SQL also provides mechanisms for defining views that present data in customized ways. A view is a virtual table that does not store data physically but derives its contents from one or more base tables through a query. Views can simplify complex queries, enhance security by restricting access to specific data, and provide logical independence by hiding changes in the underlying schema. For example, the following statement defines a view that lists employees along with their department names: CREATE VIEW EMP_DEPT AS SELECT E.Fname, E.Lname, D.Dname FROM EMPLOYEE E, DEPARTMENT D WHERE E.Dno = D.Dnumber; Once defined, the view can be queried as if it were a base table: SELECT * FROM EMP_DEPT; Updates through views are sometimes allowed, but they are restricted to cases where the update can be unambiguously translated into updates on the underlying base tables. SQL also supports materialized views (or snapshots) in some systems, where the view result is stored physically and periodically refreshed. This can significantly improve performance for complex queries or aggregations. Another key aspect of SQL is authorization and security control. SQL includes commands for granting and revoking privileges to users. The GRANT statement assigns specific rights, such as SELECT, INSERT, UPDATE, or DELETE, on tables or views to particular users or roles. For example, GRANT SELECT, INSERT ON EMPLOYEE TO JSMITH; allows user JSMITH to query and insert tuples into the EMPLOYEE table. To remove privileges, the REVOKE statement is used: REVOKE INSERT ON EMPLOYEE FROM JSMITH; SQL’s security model also includes ownership rules, where the creator of a schema object automatically becomes its owner and has full control over it. Database administrators typically manage user accounts and roles, assigning appropriate privileges to enforce access control and ensure data protection."} ,  
      {"role" : "user" , "content" : "SQL also provides transaction control statements that ensure database consistency and integrity during concurrent operations. A transaction is a logical unit of work that consists of one or more SQL statements executed as a single operation. The fundamental properties of transactions—Atomicity, Consistency, Isolation, and Durability—are collectively known as the ACID properties. SQL uses the COMMIT and ROLLBACK commands to manage transactions. COMMIT makes all changes made during the transaction permanent, while ROLLBACK undoes all changes since the start of the transaction or since the last savepoint. For example, a banking application might execute the following sequence: BEGIN TRANSACTION; UPDATE ACCOUNT SET Balance = Balance - 100 WHERE AccNo = 'A123'; UPDATE ACCOUNT SET Balance = Balance + 100 WHERE AccNo = 'B456'; COMMIT; If any error occurs, the application can issue ROLLBACK instead of COMMIT to restore the database to its previous consistent state. SQL also supports the concept of savepoints, which allow partial rollbacks within a transaction. For instance: SAVEPOINT sp1; UPDATE EMPLOYEE SET Salary = Salary * 1.1 WHERE Dno = 4; ROLLBACK TO sp1; Transactions are crucial in multi-user environments to prevent problems such as lost updates, dirty reads, and inconsistent retrievals. The SQL standard defines different isolation levels—READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE—which control the visibility of intermediate results to other concurrent transactions. These levels represent trade-offs between performance and consistency. Most DBMSs allow setting the isolation level using the SET TRANSACTION command. Proper use of transactions ensures reliable and predictable behavior of database systems, even in the presence of concurrent access or system failures. In addition to transaction control, SQL provides support for integrity constraints to maintain the correctness of data. Integrity constraints are rules that restrict the allowable values for data and relationships among data elements. SQL includes several types of constraints: domain constraints, key constraints, entity integrity, and referential integrity. Domain constraints specify permissible values for an attribute; for example, Salary DECIMAL(10,2) CHECK (Salary > 0) ensures that all salary values are positive. Key constraints ensure uniqueness within a table, as in PRIMARY KEY(Ssn), which guarantees that no two employees have the same social security number. Entity integrity requires that primary key attributes cannot be NULL, ensuring that each tuple can be uniquely identified. Referential integrity enforces consistency among related tables by requiring that foreign key values match primary key values in the referenced table. For example, the constraint FOREIGN KEY(Dno) REFERENCES DEPARTMENT(Dnumber) ensures that every department number in the EMPLOYEE table corresponds to an existing department. If an attempt is made to insert or delete data that violates this rule, the DBMS will reject the operation. SQL provides several options for maintaining referential integrity when deletions or updates occur in the parent table: CASCADE, SET NULL, and SET DEFAULT. CASCADE propagates the change to dependent rows, while SET NULL or SET DEFAULT assign a null or default value to the foreign key. For instance, FOREIGN KEY(Dno) REFERENCES DEPARTMENT(Dnumber) ON DELETE SET NULL; means that if a department is deleted, the Dno field of employees in that department becomes NULL. These features collectively help preserve the logical consistency of the database and reduce the need for application-level checks. SQL also includes mechanisms for enforcing more complex integrity constraints and business rules through assertions and triggers. An assertion is a schema-level constraint that expresses a condition that must always hold true for the database. The syntax for creating an assertion is: CREATE ASSERTION assertion_name CHECK (search_condition); For example, to ensure that no employee’s salary exceeds the salary of their manager, we could write: CREATE ASSERTION Salary_Limit CHECK (NOT EXISTS (SELECT * FROM EMPLOYEE E, EMPLOYEE M WHERE E.Super_ssn = M.Ssn AND E.Salary > M.Salary)); The DBMS automatically checks assertions whenever relevant data is modified. However, because assertion checking can be computationally expensive, many commercial systems provide limited support for them. Triggers offer a more flexible way to enforce complex constraints and automate database actions. A trigger is defined to execute automatically in response to specific events such as INSERT, UPDATE, or DELETE. The trigger definition specifies the event, the timing (BEFORE or AFTER), and the action to perform. For instance, CREATE TRIGGER Salary_Check BEFORE UPDATE ON EMPLOYEE FOR EACH ROW WHEN (NEW.Salary > 100000) SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = 'Salary too high'; This trigger prevents any salary update that exceeds 100,000. Triggers can reference both OLD and NEW row values, enabling sophisticated logic for validation, logging, and auditing. Multiple triggers can be defined on the same table for different events or priorities. The use of triggers should be carefully managed, as excessive or poorly designed triggers can affect performance and make system behavior harder to predict. Nevertheless, when properly used, they provide a powerful mechanism for maintaining data integrity and automating routine tasks. Another important SQL capability is data control through roles and privileges. In large organizations, managing individual user privileges can become complex and error-prone. SQL therefore supports roles, which are named collections of privileges that can be granted to users or other roles. For example, a role named MANAGER may be created with privileges to update employee records and approve salaries, while another role named CLERK may have only read access to specific tables. The CREATE ROLE statement defines a role, and GRANT statements assign privileges to it. For example: CREATE ROLE MANAGER; GRANT SELECT, UPDATE ON EMPLOYEE TO MANAGER; GRANT MANAGER TO JSMITH; Once granted, JSMITH inherits all privileges associated with the MANAGER role. Roles simplify security administration and allow flexible, hierarchical privilege management. SQL also supports the concept of views for access control—users can be granted privileges on views rather than on base tables, allowing sensitive columns or rows to be hidden. In addition, some SQL systems support row-level security or fine-grained access control, where policies define which rows each user can see or modify. For instance, a policy might restrict employees to viewing only data from their own department. These mechanisms help ensure data privacy and regulatory compliance. SQL’s security features are critical for protecting organizational data and ensuring that only authorized users can access or modify it. The database catalog stores all information about users, roles, privileges, and object ownership, enabling the DBMS to enforce security automatically during query processing. Modern SQL systems also provide extensive support for database recovery, concurrency control, and performance optimization. Recovery mechanisms ensure that the database can be restored to a consistent state after failures such as system crashes, power outages, or hardware errors. SQL transaction management is closely tied to recovery, as the COMMIT and ROLLBACK operations define points of consistency. DBMSs typically use log-based recovery, where every change to the database is recorded in a log file. In the event of a failure, the log is used to redo committed transactions and undo uncommitted ones. Concurrency control, on the other hand, deals with the correct execution of multiple transactions simultaneously without interference. SQL provides isolation levels, as discussed earlier, to balance consistency with performance. Most systems implement concurrency control using locking or multiversion concurrency control (MVCC). Locks can be placed on rows, pages, or tables to prevent conflicting updates. Deadlock detection and resolution mechanisms are used to handle situations where two or more transactions wait for each other’s locks. Performance optimization in SQL systems relies heavily on query processing and indexing. The query optimizer determines the most efficient execution plan for a given SQL statement, considering available indexes, data distribution, and system statistics. SQL allows users to provide hints or use EXPLAIN commands to analyze how a query will be executed. Database tuning involves adjusting indexes, rewriting queries, and configuring memory and disk parameters to improve overall system efficiency. These advanced features make SQL-based systems highly robust and scalable for both small applications and large enterprise databases. SQL has also evolved to support advanced data types and object-oriented features, often referred to as object-relational extensions. These include user-defined types (UDTs), inheritance, polymorphism, and methods associated with data objects. A user-defined type allows the creation of complex attributes beyond standard scalar types such as INTEGER or VARCHAR. For instance, one can define a type for an address that includes street, city, and zip code: CREATE TYPE AddressType AS (Street VARCHAR(30), City VARCHAR(20), Zip CHAR(5)); Once defined, this type can be used as an attribute in a table: CREATE TABLE CUSTOMER (CustID INT PRIMARY KEY, Name VARCHAR(30), Address AddressType); SQL:1999 and later standards introduced table inheritance, where a table can inherit columns from another table. For example, CREATE TABLE MANAGER UNDER EMPLOYEE (Bonus DECIMAL(8,2)); defines MANAGER as a specialized version of EMPLOYEE with an additional Bonus attribute. These features support more natural modeling of real-world entities and relationships. Methods can also be defined for user-defined types to implement custom behavior, such as computing derived values or enforcing rules. Object-relational SQL systems provide powerful modeling capabilities that bridge the gap between traditional relational databases and object-oriented programming. However, these extensions are optional and their implementation varies across DBMS products. Some systems, such as PostgreSQL and Oracle, offer strong support for UDTs and inheritance, while others limit object-relational features to maintain simplicity and compatibility. Another major enhancement in modern SQL is support for recursive and hierarchical queries, which allow representation and traversal of relationships such as organizational charts or bill-of-material structures. Recursive queries use the WITH RECURSIVE clause to repeatedly apply a query to its own output until a termination condition is reached. For example, to list all subordinates of a given employee, we can write: WITH RECURSIVE Subordinates(Emp, Manager) AS (SELECT E.Ssn, E.Super_ssn FROM EMPLOYEE E WHERE E.Super_ssn = '123456789' UNION ALL SELECT E.Ssn, E.Super_ssn FROM EMPLOYEE E, Subordinates S WHERE E.Super_ssn = S.Emp) SELECT * FROM Subordinates; This query begins with direct subordinates of the specified manager and repeatedly finds employees reporting to those subordinates. Recursive queries are particularly useful in data warehousing, network analysis, and organizational management applications. SQL:1999 also introduced support for common table expressions (CTEs), which allow temporary result sets to be named and referenced within a query. CTEs simplify complex nested queries and improve readability. Additionally, SQL supports set operations such as UNION, INTERSECT, and EXCEPT for combining results of multiple queries. These operations follow mathematical set semantics, eliminating duplicates unless UNION ALL or similar variants are used. Hierarchical query capabilities have been extended in some systems, like Oracle’s CONNECT BY or PostgreSQL’s recursive CTEs, to efficiently handle tree-structured data. These advanced querying mechanisms make SQL highly expressive and capable of representing sophisticated relationships within data. SQL also provides a rich set of aggregate and analytic functions that enable advanced data analysis directly within the database. Aggregate functions such as COUNT, SUM, AVG, MIN, and MAX summarize data over groups of rows, while analytic functions like RANK, DENSE_RANK, ROW_NUMBER, and WINDOW functions allow calculations across sets of rows related to the current row. For example, to compute each employee’s salary rank within their department, we can write: SELECT Fname, Lname, Dno, RANK() OVER (PARTITION BY Dno ORDER BY Salary DESC) AS SalaryRank FROM EMPLOYEE; This assigns ranks to employees based on their salaries, restarting the ranking for each department. The PARTITION BY clause defines the grouping, and ORDER BY determines the ranking order. Analytic functions are powerful tools for reporting, trend analysis, and decision support. SQL also supports GROUP BY and HAVING clauses to aggregate data across specific attributes. For example: SELECT Dno, AVG(Salary) AS AvgSal FROM EMPLOYEE GROUP BY Dno HAVING AVG(Salary) > 30000; This query returns departments whose average salary exceeds 30,000. In addition, the CASE expression allows conditional logic within queries, similar to IF-THEN-ELSE constructs in programming. For instance: SELECT Fname, Lname, CASE WHEN Salary > 80000 THEN 'High' WHEN Salary BETWEEN 50000 AND 80000 THEN 'Medium' ELSE 'Low' END AS SalaryLevel FROM EMPLOYEE; These features enable complex computations and classifications to be performed directly within SQL without external programming. Many DBMSs also provide additional built-in functions for statistical, date-time, and string processing, further extending SQL’s analytical capabilities. SQL has also evolved to include extensive support for temporal and spatial data management. Temporal features allow databases to maintain historical information by associating time dimensions with data. SQL:2011 introduced temporal tables that can store both valid-time (when data is true in the real world) and transaction-time (when data is stored in the database). For example, a system-versioned table automatically tracks changes to data over time: CREATE TABLE EMPLOYEE_HISTORY (Ssn CHAR(9), Fname VARCHAR(20), Lname VARCHAR(20), Salary DECIMAL(10,2), SysStartTime TIMESTAMP GENERATED ALWAYS AS ROW BEGIN, SysEndTime TIMESTAMP GENERATED ALWAYS AS ROW END, PERIOD FOR SYSTEM_TIME(SysStartTime, SysEndTime)) WITH SYSTEM VERSIONING; With this feature, the DBMS automatically records every update or deletion, preserving old versions for temporal queries. Users can query the database “as of” a past time using constructs like: SELECT * FROM EMPLOYEE_HISTORY FOR SYSTEM_TIME AS OF TIMESTAMP '2024-01-01 00:00:00'; Spatial extensions, on the other hand, allow the storage and manipulation of geometric and geographic data types such as points, lines, and polygons. SQL/MM Spatial defines standard functions for spatial queries, including distance calculations, containment tests, and spatial joins. For example, SELECT * FROM LOCATIONS WHERE ST_Within(geom, ST_GeomFromText('POLYGON((...))')); retrieves all locations within a specific region. These capabilities enable databases to support applications in GIS, navigation, and environmental modeling. Together, temporal and spatial extensions make SQL suitable for managing complex, real-world data that changes over time and space. Another important feature in SQL is the support for views, which are virtual tables defined by queries rather than storing actual data. A view allows users to simplify complex queries, provide security by restricting access to specific columns or rows, and present data in a customized format. For example, CREATE VIEW EMP_SALARY_VIEW AS SELECT FNAME, LNAME, SALARY FROM EMPLOYEE WHERE DNO = 5; creates a view that shows only the first name, last name, and salary of employees in department 5. Users can query the view just like a table: SELECT * FROM EMP_SALARY_VIEW; Views can also be updatable if the DBMS can map changes made to the view back to the underlying base tables. The rules for updatable views usually require that the view reference only a single base table and include key columns. SQL also supports materialized views, which store the results of a query physically and can be refreshed periodically. These are especially useful for summarizing large datasets or performing precomputed aggregations. Materialized views are created using the CREATE MATERIALIZED VIEW statement and can include aggregation functions, joins, and other operations. For instance, CREATE MATERIALIZED VIEW DEPT_SUMMARY AS SELECT DNO, COUNT(*) AS NUM_EMP, SUM(SALARY) AS TOTAL_SAL FROM EMPLOYEE GROUP BY DNO; creates a summary of the number of employees and total salaries per department. SQL provides mechanisms for indexing to improve query performance. Indexes can be created on one or more columns of a table using CREATE INDEX. For example, CREATE INDEX IDX_EMP_LNAME ON EMPLOYEE(LNAME); creates an index on the LNAME column, speeding up searches and retrievals based on last names. There are different types of indexes, such as unique, composite, clustered, and non-clustered indexes, each with specific use cases and performance implications. Finally, SQL allows the definition of triggers, which are procedural code executed automatically in response to specific events on a table or view. Triggers can be fired BEFORE or AFTER INSERT, UPDATE, or DELETE operations. For example, CREATE TRIGGER SALARY_CHECK BEFORE UPDATE ON EMPLOYEE FOR EACH ROW BEGIN IF NEW.SALARY < 0 THEN RAISE EXCEPTION 'Salary cannot be negative'; END IF; END; ensures that no employee salary is set to a negative value. Triggers are widely used for enforcing business rules, auditing changes, and maintaining data integrity automatically. These advanced SQL features—views, materialized views, indexing, and triggers—enhance the flexibility, performance, and robustness of relational database systems, allowing users and applications to interact with complex datasets efficiently while maintaining control over data presentation, integrity, and access. " } ,  
      {"role" : "user" , "content" : "SQL also supports onstraints to enforce data integrity at the table level; constraints ensure that the data adheres to specific rules, preventing invalid or inconsistent data entry. Common constraints include PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, CHECK, and DEFAULT. A PRIMARY KEY uniquely identifies each row in a table and automatically creates a unique index. For example, CREATE TABLE EMPLOYEE (SSN CHAR(9) PRIMARY KEY, FNAME VARCHAR(15) NOT NULL, LNAME VARCHAR(15) NOT NULL, DNO INT NOT NULL, SALARY DECIMAL(10,2)); defines SSN as the primary key while enforcing that first and last names and department numbers cannot be NULL. FOREIGN KEY constraints enforce referential integrity by ensuring that a value in one table matches a value in another table. For instance, CREATE TABLE PROJECT (PNUMBER INT PRIMARY KEY, DNUM INT, FOREIGN KEY (DNUM) REFERENCES DEPARTMENT(DNUMBER)); ensures that each project is associated with an existing department. UNIQUE constraints prevent duplicate values in specified columns. For example, CREATE TABLE DEPARTMENT (DNAME VARCHAR(15) UNIQUE, DNUMBER INT PRIMARY KEY); ensures department names are unique. NOT NULL constraints enforce that columns must have a value, preventing NULL entries. CHECK constraints define custom conditions, e.g., CREATE TABLE EMPLOYEE (SALARY DECIMAL(10,2) CHECK (SALARY > 0)); ensures that salaries are positive. DEFAULT constraints specify a default value for a column when none is provided during insertion, e.g., CREATE TABLE EMPLOYEE (DNO INT DEFAULT 1); assigns department 1 if no department number is provided. Combining multiple constraints in a table definition allows comprehensive data integrity rules. SQL also provides mechanisms for transactions to ensure consistency and reliability of operations, following the ACID properties (Atomicity, Consistency, Isolation, Durability). Transactions can include multiple statements, and if one fails, ROLLBACK ensures that no partial changes remain. COMMIT saves all changes made during a transaction. SAVEPOINT allows marking intermediate points within a transaction to rollback selectively. For example, BEGIN TRANSACTION; UPDATE EMPLOYEE SET SALARY = SALARY * 1.1 WHERE DNO = 5; SAVEPOINT SP1; DELETE FROM EMPLOYEE WHERE SSN = '123456789'; ROLLBACK TO SP1; COMMIT; ensures that only the salary updates are committed, while the deletion is undone. SQL also allows locking mechanisms for concurrency control to prevent conflicts when multiple users access the database simultaneously. Locks can be applied at the table or row level, including shared and exclusive locks, ensuring data consistency. Additionally, SQL supports procedures, functions, and triggers to embed procedural logic within the database, automate routine tasks, enforce business rules, and maintain audit trails. Stored procedures are precompiled routines that can be executed with parameters, while functions return values and can be used in queries. Triggers execute automatically in response to specific events, enhancing integrity and automation. Overall, SQL combines data definition, manipulation, control, and transaction management, providing a comprehensive framework for interacting with relational databases efficiently and reliably. SQL also includes features for views, which are virtual tables derived from queries on one or more base tables; a view presents data in a customized format without duplicating it physically in the database. Views can simplify complex queries, enhance security by restricting access to specific columns or rows, and provide consistent results even if underlying tables change. For example, CREATE VIEW EMP_DEPT AS SELECT E.FNAME, E.LNAME, D.DNAME FROM EMPLOYEE E, DEPARTMENT D WHERE E.DNO = D.DNUMBER; creates a view showing each employee's name along with their department. Views can be updatable if modifications in the view can propagate to base tables, subject to certain constraints, such as avoiding joins or aggregates in the view definition. Additionally, indexes in SQL are used to improve query performance by enabling faster data retrieval. An index creates a data structure that allows the database engine to locate rows efficiently. For example, CREATE INDEX IDX_EMP_LNAME ON EMPLOYEE(LNAME); creates an index on the LNAME column of the EMPLOYEE table. SQL supports composite indexes, spanning multiple columns, which are useful for optimizing queries with multiple conditions. Index types vary by database system and include B-tree, bitmap, and hash indexes, each with trade-offs in performance, storage, and maintenance. Another advanced feature is transactions isolation levels, which control how concurrent transactions interact and see data, preventing anomalies such as dirty reads, non-repeatable reads, or phantom reads. Common isolation levels include READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE, each offering different trade-offs between concurrency and consistency. SQL also supports joins beyond simple equijoins; outer joins (LEFT, RIGHT, FULL) include unmatched rows with NULLs for missing values, enhancing reporting capabilities. For instance, LEFT JOIN returns all rows from the left table and matching rows from the right table, filling unmatched columns with NULL. SQL allows subqueries in SELECT, FROM, and WHERE clauses, enabling dynamic, nested queries that are powerful for analytical tasks. The CASE expression in SQL provides conditional logic within queries, allowing row-level computations, e.g., SELECT FNAME, LNAME, CASE WHEN SALARY > 50000 THEN 'High' ELSE 'Low' END AS SALARY_LEVEL FROM EMPLOYEE; classifies employees by salary. String, date, and numeric functions further enrich SQL, supporting operations like concatenation, substring extraction, rounding, and date arithmetic. SQL also includes set operations (UNION, INTERSECT, EXCEPT) to combine or compare results from multiple queries. Security in SQL involves GRANT and REVOKE commands to assign or remove privileges on tables, views, or procedures. Roles and user management allow fine-grained access control, while constraints and triggers enforce policies automatically. Modern SQL implementations extend these capabilities with recursive queries, window functions (OVER, PARTITION BY), and analytical functions for ranking, cumulative sums, and moving averages, enabling complex business reporting directly in the database. Overall, SQL is a rich, declarative language combining schema definition, data manipulation, transaction control, security, and reporting, making it the standard for relational database management and a cornerstone of enterprise information systems. SQL also allows the use of triggers, which are procedural code automatically executed in response to certain events on a table or view, such as INSERT, UPDATE, or DELETE; triggers are often used to maintain consistency, enforce business rules, or update related tables, for example, a trigger can automatically log changes to a salary column or prevent deletions from a critical table. Triggers have BEFORE, AFTER, and INSTEAD OF variants depending on the timing of execution relative to the triggering event. SQL supports stored procedures and functions, which encapsulate a sequence of SQL statements that can be executed repeatedly, improving modularity, maintainability, and performance by reducing network traffic and precompiling execution plans. For instance, a stored procedure might calculate employee bonuses based on department performance and can be invoked with parameters, returning results or updating tables. SQL also supports cursors, which allow row-by-row processing of query results when set-based operations are insufficient or procedural handling is needed. Cursors can be static, dynamic, forward-only, or scrollable, and they require explicit opening, fetching, and closing to manage memory efficiently. Advanced SQL features include materialized views, which store the results of a query physically and can be refreshed periodically or on demand to improve performance for complex aggregations or joins. SQL provides transaction control statements (COMMIT, ROLLBACK, SAVEPOINT) to ensure atomicity, consistency, isolation, and durability (ACID properties), crucial for reliable multi-user database operations. The language also supports constraint management, including primary key, foreign key, unique, check, and not null constraints, which enforce data integrity rules automatically. SQL’s analytical and reporting capabilities are enhanced with window functions, such as ROW_NUMBER(), RANK(), DENSE_RANK(), and NTILE(), which allow computations across a partition of rows related to the current row, useful for ranking, percentiles, and moving averages. SQL syntax allows recursive common table expressions (CTEs), enabling hierarchical queries, such as retrieving all subordinates under a manager in an organizational chart. Additionally, SQL integrates JSON and XML support, allowing storage, querying, and transformation of semi-structured data directly in relational tables, with functions for parsing, extracting, and modifying JSON or XML elements. Modern SQL engines also include full-text search capabilities, enabling keyword searching across text columns with ranking and relevance scoring. Optimization hints can be provided to the query engine to influence execution plans, join orders, or index usage for performance tuning. SQL’s portability and standardization ensure that core syntax is consistent across different RDBMSs, though vendors often extend SQL with proprietary features, making cross-platform compatibility an important consideration. Finally, SQL remains a foundational tool for relational database design, administration, and application development, offering a unified approach to defining, manipulating, and querying structured data while supporting advanced features for performance, security, analytics, and transactional integrity, thereby maintaining its role as the industry-standard language for managing enterprise data. SQL also supports views, which are virtual tables representing the result of a query and do not store data physically, providing a way to simplify complex queries, encapsulate business logic, and control access to sensitive data; a view can be updatable if it meets certain conditions, allowing INSERT, UPDATE, and DELETE operations that affect the underlying base tables, or it can be read-only when derived from multiple tables with aggregations or joins; for example, a view can display employee names and their total hours worked on projects, while restricting access to salary information, and views can be used in queries just like tables, supporting joins, subqueries, and aggregate functions. SQL includes indexing mechanisms to improve query performance, allowing creation of primary, unique, composite, and full-text indexes, which store data in structures such as B-trees or hash tables to speed up searches, range queries, and ordering operations; for example, an index on EMPLOYEE.SSN can make lookups by social security number much faster, while a composite index on (DNO, SALARY) can optimize queries filtering employees by department and salary simultaneously. SQL also supports security features, including GRANT and REVOKE statements to assign or remove privileges on database objects to users or roles, allowing fine-grained control over access to tables, views, procedures, and columns; for example, a user can be granted SELECT access on EMPLOYEE while being denied INSERT or DELETE privileges. Modern SQL engines provide partitioning features, allowing large tables to be divided into smaller, more manageable pieces, improving query performance, maintenance, and backup efficiency; partitions can be based on range, list, hash, or composite criteria, such as separating orders by year or region. SQL’s analytical functions support OLAP operations, including CUME_DIST(), PERCENT_RANK(), and moving sum or average, useful for reporting, forecasting, and data mining; these functions work over partitions and ordered sets of rows, complementing GROUP BY aggregations by offering more granular analytics. Additionally, SQL supports triggers, procedures, functions, transactions, constraints, cursors, materialized views, CTEs, recursive queries, JSON/XML operations, full-text search, indexing, partitioning, security management, and optimization hints, providing a comprehensive ecosystem for data management, integrity enforcement, performance optimization, and analytical processing. SQL’s standardization by ANSI and ISO, along with vendor-specific extensions, ensures that core SQL features are portable across relational database systems, while proprietary enhancements allow additional functionality, such as Oracle’s PL/SQL, SQL Server’s T-SQL, or PostgreSQL’s procedural language support; this flexibility enables developers and database administrators to leverage SQL for a wide range of applications, from simple data retrieval to complex enterprise-level transaction processing and reporting solutions. Overall, SQL remains the essential language for relational database management, offering tools for data definition, manipulation, control, transaction management, aggregation, analytics, security, and performance tuning, making it indispensable for modern database-driven applications. SQL also provides constraints to ensure data integrity, including PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, CHECK, and DEFAULT constraints, which can be specified at table creation or altered later using ALTER TABLE, ensuring that data adheres to business rules and relationships; for example, a FOREIGN KEY constraint ensures that an EMPLOYEE’s DNO refers to an existing DEPARTMENT.DNUMBER, while a CHECK constraint can ensure that SALARY > 0; constraints can be named explicitly for clarity and management, and database engines enforce them automatically during INSERT, UPDATE, and DELETE operations, sometimes using deferred checking in transactions; SQL allows cascading actions for foreign keys such as ON DELETE CASCADE, ON UPDATE CASCADE, SET NULL, or SET DEFAULT, providing control over the propagation of changes in related tables. SQL also supports joins, which combine rows from two or more tables based on related columns, with types including INNER JOIN, LEFT OUTER JOIN, RIGHT OUTER JOIN, FULL OUTER JOIN, CROSS JOIN, and SELF JOIN, enabling complex queries across multiple relations; INNER JOIN returns rows matching the join condition, LEFT JOIN includes all rows from the left table, RIGHT JOIN from the right table, FULL OUTER JOIN returns all rows from both tables with NULLs where no match exists, CROSS JOIN produces the Cartesian product, and SELF JOIN allows a table to be joined to itself, often with aliases to avoid ambiguity. SQL supports subqueries (nested queries) which can be correlated or uncorrelated, allowing comparisons, existence checks, and aggregation over subsets of data; correlated subqueries reference columns from the outer query and are evaluated once per outer row, while uncorrelated subqueries are evaluated once for the entire outer query; subqueries can be used in WHERE, FROM, or SELECT clauses, providing powerful data retrieval capabilities. SQL also includes set operations such as UNION, INTERSECT, and EXCEPT/MINUS, which operate on two or more SELECT results, requiring union-compatible relations and eliminating duplicates by default; these operations simplify queries that combine multiple sources or conditions. Furthermore, SQL provides string operations, arithmetic expressions, date/time functions, conversion functions, window functions, and ranking functions, allowing flexible manipulation and analysis of data; for example, string concatenation, substring extraction, date arithmetic, and ranking employees by salary within departments can all be achieved within a single query. SQL’s transaction control ensures ACID properties (Atomicity, Consistency, Isolation, Durability), with COMMIT, ROLLBACK, and SAVEPOINT commands enabling safe transaction management, rollback of partial work, and checkpointing within transactions. Finally, SQL supports reporting and ordering with the ORDER BY clause for sorting, GROUP BY for aggregation by categories, HAVING for filtering groups, and DISTINCT for eliminating duplicates, allowing structured and meaningful presentation of query results across diverse applications. SQL also allows views, which are virtual tables representing the result of a query, created with the CREATE VIEW statement; views simplify complex queries, provide security by restricting access to specific columns or rows, and can be updated in some cases depending on the database engine; for example, a view can show only employees in the 'Research' department along with their salaries, hiding other departments’ data, while the underlying table remains unchanged; indexed views, also called materialized views, store the result physically, improving query performance for frequent access, and can be refreshed periodically. SQL supports indexes, which improve retrieval speed by providing a fast lookup mechanism, created using CREATE INDEX, and can be applied to one or more columns; indexes can be unique or non-unique, clustered or non-clustered, and they reduce the number of disk accesses required for query execution, though they incur additional storage and maintenance overhead during INSERT, UPDATE, or DELETE operations. SQL provides constraints not only at table creation but also with ALTER TABLE to maintain data integrity dynamically, including CHECK, PRIMARY KEY, FOREIGN KEY, UNIQUE, NOT NULL, and DEFAULT, which the DBMS enforces automatically; cascading actions such as ON DELETE CASCADE or ON UPDATE CASCADE ensure that related data changes propagate safely across tables, while SET NULL or SET DEFAULT provide alternative behaviors when parent rows are modified or deleted. SQL allows advanced aggregation with GROUP BY and HAVING, supporting functions like SUM, COUNT, AVG, MIN, and MAX, applied to groups of rows; aggregation can be combined with joins to summarize data across multiple tables, such as calculating total salaries per department or counting employees per project. SQL includes string functions like CONCAT, SUBSTRING, LENGTH, TRIM, UPPER, LOWER, and REPLACE for manipulating textual data, date/time functions such as CURRENT_DATE, CURRENT_TIMESTAMP, DATEADD, DATEDIFF, and EXTRACT, arithmetic operators '+', '-', '*', '/', and conversion functions like CAST and CONVERT for transforming data types. SQL supports window functions such as ROW_NUMBER, RANK, DENSE_RANK, NTILE, LEAD, and LAG, which operate over partitions of result sets, enabling ranking, cumulative totals, moving averages, and comparison across rows without collapsing them into single aggregate values; these are essential for analytical queries, trend analysis, and reporting. SQL also provides correlated subqueries, EXISTS and NOT EXISTS, IN, NOT IN, and explicit sets for filtering, allowing sophisticated conditions based on related data; null values are handled using IS NULL and IS NOT NULL, and SQL evaluates them distinctly, meaning comparisons with '=' or '<>' do not apply. Finally, SQL is a practical reporting language, capable of ordering results with ORDER BY, eliminating duplicates with DISTINCT, filtering groups with HAVING, summarizing data with GROUP BY, and producing formatted listings, enabling complex reporting, data validation, and decision support in relational databases." } ,  
      {"role" : "user" , "content" : "SQL also supports transaction control beyond basic DML, allowing multiple statements to be executed as a single atomic unit using BEGIN TRANSACTION or implicit transaction modes, ensuring ACID properties: Atomicity, Consistency, Isolation, and Durability; COMMIT finalizes changes, ROLLBACK undoes uncommitted operations, and SAVEPOINT creates intermediate points to which a transaction can be rolled back without affecting previous actions, enabling complex workflows such as payroll processing, inventory updates, or financial transfers while preserving database integrity; different isolation levels like READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE control visibility of uncommitted data and prevent phenomena like dirty reads, non-repeatable reads, and phantoms, balancing concurrency and consistency according to application needs. SQL also provides authorization and access control via GRANT and REVOKE, specifying which users or roles can perform SELECT, INSERT, UPDATE, DELETE, EXECUTE, or ALTER operations on database objects, supporting role-based security and least-privilege principles; combined with views, this allows restricting access to sensitive information without exposing full base tables. Stored procedures and functions encapsulate SQL code for reuse, maintainability, and modular design; they accept parameters, can return values, and include procedural constructs such as IF, WHILE, LOOP, and CASE, allowing conditional logic, iteration, and complex computations within the database engine, improving performance by reducing client-server communication. Triggers are automated actions executed in response to table events (INSERT, UPDATE, DELETE) and can enforce complex constraints, maintain audit logs, or propagate changes, providing reactive behavior directly in the database. SQL supports joins of multiple types: INNER JOIN, LEFT/RIGHT OUTER JOIN, FULL OUTER JOIN, and CROSS JOIN, allowing retrieval of related data across tables with different inclusion rules; self-joins enable recursive queries, such as finding hierarchical relationships like managers and subordinates. SQL also includes set operations like UNION, INTERSECT, and EXCEPT/MINUS, requiring union-compatible relations, allowing combination or comparison of query results. Advanced filtering using LIKE, BETWEEN, IN, NOT IN, EXISTS, NOT EXISTS, and null handling ensures precise selection of tuples, while aggregate functions, grouping, and window functions support analytics, ranking, and trend analysis. SQL allows commenting with -- for single lines or /* ... */ for multi-line blocks, aiding documentation and collaboration. SQL remains the standard language for relational databases, supported by all major commercial and open-source DBMSs, combining declarative query power, procedural extensions, transaction management, security, and data integrity features to handle a wide range of business, scientific, and administrative applications efficiently and reliably. In addition to standard SQL commands, modern relational DBMSs offer extensions and enhancements for handling complex data types, large objects (LOBs), and arrays, allowing storage and manipulation of images, documents, multimedia, JSON, XML, and spatial data; SQL provides functions to parse, extract, and transform these types, often including built-in operators like JSON_VALUE, XMLTABLE, ST_Within, ST_Intersects, and others to integrate structured, semi-structured, and unstructured data seamlessly; indexing strategies such as B-trees, hash indexes, bitmap indexes, and full-text indexes optimize query performance, reduce retrieval time, and enable fast searching even on very large tables, while composite and unique indexes enforce data integrity; constraints (PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK, NOT NULL, DEFAULT) are enforced at the database level, preventing invalid data entry, maintaining relational integrity, and reducing the need for application-side validation; cascading actions like ON DELETE CASCADE or ON UPDATE SET NULL allow automated propagation of changes across dependent tables; normalization organizes data into 1NF, 2NF, 3NF, BCNF and beyond to minimize redundancy, avoid update anomalies, and maintain logical consistency, while controlled denormalization can improve query performance in reporting and analytical scenarios; query optimization is critical, with DBMS query planners selecting execution plans based on cost estimates, statistics, indexes, and join methods, including nested loops, hash joins, merge joins, and parallel execution strategies to handle large-scale data efficiently; views provide logical representations of one or more tables, abstracting complexity, enabling reusability, enforcing security, and supporting virtual computed columns, while materialized views store precomputed results for performance improvement in analytical queries; transaction management ensures that multi-statement operations comply with ACID properties, using commit, rollback, and savepoints, combined with locking mechanisms (shared, exclusive, row-level, table-level) to prevent deadlocks and maintain concurrency; triggers automatically enforce business rules, maintain audit trails, or synchronize tables upon specific data changes, while stored procedures encapsulate complex logic and provide controlled access to database operations; functions, user-defined types, and procedural extensions (PL/SQL, T-SQL, PL/pgSQL) allow computations, validations, and manipulations within the database engine, reducing client-server data traffic; set-based operations such as UNION, INTERSECT, EXCEPT, combined with subqueries (correlated or uncorrelated), exists/not exists, and aggregate functions (SUM, COUNT, AVG, MAX, MIN) provide a powerful declarative approach to data analysis, enabling grouping, filtering, ranking, and reporting; window functions like ROW_NUMBER, RANK, LEAD, LAG, and cumulative sums offer analytic capabilities over ordered partitions; finally, SQL includes ordering, pagination, pattern matching, and conditional expressions (CASE, COALESCE, NULLIF) to provide complete control over query results, making SQL a flexible, declarative, and procedural standard for both operational and analytical database management in diverse applications. 97 Complex Queries & Active Databases Notes: 1. Triggers can be assigned to tables or views. 2. It is not possible to create a trigger to fire when a data modification occurs in two or more tables. A trigger can be associated only with a single table. Why Use Triggers? To improve data integrity, trigger can be used. When an action is performed on data, it is possible to check if the manipulation of the data concurs with the underlying business rules, and thus avoids erroneous entries in a table. For example: • We might want to ship a free item to a client with the order, if it totals more than $1000. A trigger will be built to check the order total upon completion of the order, to see if an extra order line needs to be inserted. • In a banking scenario, when a request is made to withdraw cash from a cash point, the stored procedure will create a record on the client's statement table for the withdrawal, and the trigger will automatically reduce the balance as required. The trigger may also be the point at which a check is made on the client's balance to verify that there is enough balance to allow the withdrawal. By having a trigger on the statement table, we are secure in the knowledge that any statement entry made, whether withdrawal or deposit, will be validated and processed in one central place. Another use of triggers can be to carry out an action when a specific criterion has been met. One example of this is a case where an e-mail requesting more items to be delivered is sent, or an order for processing could be placed, when stock levels reach a preset level. We must be careful that the table we insert into does not have a trigger that will cause this first trigger to fire. It is possible to code triggers that result in an endless loop, as we can define a trigger on TableA, which inserts into TableB, and a trigger for TableB, which updates TableA. This scenario will ultimately end in an error being generated by the SQL Server. Figure 5.1 demonstrate this scenario. The sequences of events as follows: 1. A stored procedure, A, updates TableA. 2. This fires a trigger from TableA. 3. The defined trigger on TableA updates TableB. 4. TableB has a trigger which fires. 5. This trigger from TableB updates TableA. 98 Database Systems II Figure 5.1 Trigger Syntax CREATE TRIGGER name ON table [BEFORE/AFTER/INSTEAD OF] [INSERT, UPDATE, DELETE] AS BEGIN -- SQL statements END Note: Microsoft SQL Server does not have BEFORE but has INSTEAD OF which can do the same function. Trigger Examples Assume that we need to track the DML actions that are performed on a specific table and write these logs in a history table, where the ID of the inserted, updated or deleted record and the action that is performed will be written to the history table. The CREATE TABLE T-SQL statements below can be used to create both the source and history tables: CREATE TABLE TriggerDemo_Parent ( ID INT IDENTITY (1,1) PRIMARY KEY, Emp_First_name VARCHAR (50), Emp_Last_name VARCHAR (50), Emp_Salary INT ) GO CREATE TABLE TriggerDemo_History ( HID INT IDENTITY (1,1) ParentID INT, PerformedAction VARCHAR (50) ) GO PRIMARY KEY, Chapter 5 Complex Queries & Active Databases This chapter describes more advanced features of the SQL language for relational databases. In Chapter 4, we described some basic types of retrieval queries in SQL. Because of the generality and expressive power of the language, there are many additional features that allow users to specify more complex retrievals from the database. 5.1 SQL Implementations There are many SQL Implementations, the most popular Implementations are:  Standard SQL is the standard language to query a database.  PL SQL basically stands for \"Procedural Language extensions to SQL.\" This is the extension of Structured Query Language (SQL) that is used in Oracle.  T-SQL basically stands for \" Transact-SQL.\" This is the extension of Structured Query Language (SQL) that is used in Microsoft. Difference between SQL and T-SQL SQL T-SQL • SQL is a programming language which focuses on managing relational databases. • T-SQL is a procedural extension used by SQL Server. • This is used for controlling and manipulating data where large amounts of information are stored about products, clients, etc. • T-SQL has some features that are not available in SQL. Like procedural programming elements and a local variable to provide more flexible control of how the application flows. • SQL queries submitted individually to the database server. • T-SQL writes a program in such a way that all commands are submitted to the server in a single go • The syntax was formalized for many commands; some of these are SELECT, INSERT, UPDATE, DELETE, CREATE, and DROP. • It also includes special functions like the converted date () and some other functions which are not part of the regular SQL. Complex Queries & Active Databases Difference Between T-SQL and PL-SQL T-SQL PL-SQL • T-SQL is a Microsoft product. • Full Form of TL SQL is Transact Structure Query language. • T-SQL gives a high degree of control to programmers. • T-SQL performs best with Microsoft SQL server • It is easy and simple to understand. • T-SQL allows inserting multiples rows into a table using the BULK INSERT statement. • SELECT INTO statement used in T SQL • In T-SQL NOT EXISTS clause used along with SELECT statements. Varchar vs nVarchar • PL-SQL is developed by Oracle. • Full Form of PL SQL is Procedural Language Structural Query Language. • It is a natural programming language that blends easily with the SQL • PL-SQL performs best with Oracle database server. • PL-SQL is complex to understand. • PL/SQL supports oops concepts like data encapsulation, function overloading, and information hiding. • INSERT INTO statement must be used in PL/SQL • In PL/SQL, there is a MINUS operator, which could be used with SELECT statements nvarchar stores UNICODE data. If you have requirements to store UNICODE or multilingual data, nvarchar is the choice. Varchar stores ASCII data and should be your data type of choice for normal use. Regarding memory usage, nvarchar uses 2 Bytes per character, whereas varchar uses 1 Byte. JOIN-ing a VARCHAR to NVARCHAR has a considerable performance hit. Some experts recommend nvarchar always because: since all modern operating systems and development platforms use Unicode internally, using nvarchar rather than varchar, will avoid encoding conversions every time you read from or write to the database. 80 Database Systems II 5.2 Joined Tables in SQL The concept of a joined table (or joined relation) was incorporated into SQL to permit users to specify a table resulting from a join operation in the FROM clause of a query. This construct may be easier to comprehend than mixing all the select and join conditions in the WHERE clause. For example, consider Query 1, which retrieves the name and address of every employee who works for the 'Research' department. It may be easier to specify the join of the EMPLOYEE and DEPARTMENT relations in the WHERE clause, and then to select the desired tuples and attributes. This can be written in SQL as in Query 1A: Query 1A: SELECT Fname, Lname, Address FROM (EMPLOYEE JOIN DEPARTMENT ON Dno = Dnumber) WHERE Dname = 'Research'; The FROM clause in Query 1A contains a single joined table. The attributes of such a table are all the attributes of the first table, EMPLOYEE, followed by all the attributes of the second table, DEPARTMENT. The concept of a joined table also allows the user to specify different types of joins, such as NATURAL JOIN and various types of OUTER JOIN. In a NATURAL JOIN on two relations R and S, no join condition is specified; an implicit EQUIJOIN condition for each pair of attributes with the same name from R and S is created. Each such pair of attributes is included only once in the resulting relation . If the names of the join attributes are not the same in the base relations, it is possible to rename the attributes so that they match, and then to apply NATURAL JOIN. In this case, the AS construct can be used to rename a relation and all its attributes in the FROM clause. This is illustrated in Query 1B, where the DEPARTMENT relation is renamed as DEPT and its attributes are renamed as Dname, Dno (to match the name of the desired join attribute Dno in the EMPLOYEE table), Mssn, and Msdate. The implied join condition for this NATURAL JOIN is EMPLOYEE.Dno = DEPT.Dno, because this is the only pair of attributes with the same name after renaming: Query 1B: SELECT Fname, Lname, Address FROM (EMPLOYEE NATURAL JOIN (DEPARTMENT AS DEPT (Dname, Dno, Mssn, Msdate))) WHERE Dname = 'Research'; The default type of join in a joined table is called an inner join, where a tuple is included in the result only if a matching tuple exists in the other relation. 81 Complex Queries & Active Databases For example, in Query 2A, only employees who have a supervisor are included in the result; an EMPLOYEE tuple whose value for Super_ssn is NULL is excluded. Query 2A: SELECT E.Lname AS Employee_name, S.Lname AS Supervisor_name FROM (EMPLOYEE AS E INNER JOIN EMPLOYEE AS S ON E.Super_ssn = S.Ssn); If the user requires that all employees be included, a different type of join called OUTER JOIN must be used explicitly. There are several variations of OUTER JOIN, as we shall see. In the SQL standard, this is handled by explicitly specifying the keyword OUTER JOIN in a joined table, as illustrated in Query 2B: Query 2B: SELECT E.Lname AS Employee_name, S.Lname AS Supervisor_name FROM (EMPLOYEE AS E LEFT OUTER JOIN EMPLOYEE AS S ON E.Super_ssn = S.Ssn); In SQL, the options available for specifying joined tables include INNER JOIN (only pairs of tuples that match the join condition are retrieved, same as JOIN), LEFT OUTER JOIN (every tuple in the left table must appear in the result; if it does not have a matching tuple, it is padded with NULL values for the attributes of the right table), RIGHT OUTER JOIN (every tuple in the right table must appear in the result; if it does not have a matching tuple, it is padded with NULL values for the attributes of the left table), and FULL OUTER JOIN. In the latter three options, the keyword OUTER may be omitted. If the join attributes have the same name, one can also specify the natural join variation of outer joins by using the keyword NATURAL before the operation (for example, NATURAL LEFT OUTER JOIN). The keyword CROSS JOIN is used to specify the CARTESIAN PRODUCT operation, although this should be used only with the utmost care because it generates all possible tuple combinations. It is also possible to nest join specifications; that is, one of the tables in a join may itself be a joined table. This allows the specification of the join of three or more tables as a single joined table, which is called a multiway join. For example, Query 3 is a different way of specifying Query 3 from Chapter 2 using the concept of a joined table: Query 3: SELECT Pnumber, Dnum, Lname, Address, Bdate FROM ((PROJECT JOIN DEPARTMENT ON Dnum = Dnumber) JOIN EMPLOYEE ON Mgr_ssn = Ssn) WHERE Plocation = 'Stafford'; "} ,  
      {"role" : "user" , "content" : "82 Database Systems II Not all SQL implementations have implemented the new syntax of joined tables. In some systems, a different syntax was used to specify outer joins by using the comparison operators + =, = +, and + = + for left, right, and full outer join, respectively, when specifying the join condition. For example, this syntax is available in Oracle. To specify the left outer join in Query 2B using this syntax, we could write the query 2C as follows: Query 2C: SELECT E.Lname, S.Lname FROM EMPLOYEE E, EMPLOYEE S WHERE E.Super_ssn + = S.Ssn; 5.3 More Complex SQL Retrieval Queries there are many additional features that allow users to specify more complex retrievals from the database. Such as: • Nested Queries • Join Types • Aggregate Functions • Grouping Nested Queries, Tuples, and Set/Multiset Comparisons Some queries require that existing values in the database be fetched and then used in a comparison condition. Such queries can be conveniently formulated by using nested queries, which are complete select-from-where blocks within another SQL query. That another query is called the outer query. These nested queries can also appear in the WHERE clause or the FROM clause or the SELECT clause or other SQL clauses as needed. SQL allows the use of tuples of values in comparisons by placing them within parentheses. To illustrate this, consider the following query: Query 4 Select the Essns of all employees who work the same (project, hours) combination on some project that employee 'John Smith' (whose Ssn = '123456789') works on. SELECT DISTINCT Essn FROM WORKS_ON WHERE (Pno, Hours) IN ( SELECT Pno, Hours FROM WORKS_ON WHERE Essn = '123456789' ); 83 Complex Queries & Active Databases In this example, the IN operator compares the subtuple of values in parentheses (Pno, Hours) within each tuple in WORKS_ON with the set of type-compatible tuples produced by the nested query. In addition to the IN operator, several other comparison operators can be used to compare a single value v (typically an attribute name) to a set or multiset v (typically a nested query). The = ANY (or = SOME) operator returns TRUE if the value v is equal to some value in the set V and is hence equivalent to IN. The two keywords ANY and SOME have the same effect. Other operators that can be combined with ANY (or SOME) include >, >=, <, <=, and <>. The keyword ALL can also be combined with each of these operators. Query 5 Returns the names of employees whose salary is greater than the salary of all the employees in department 5. SELECT Lname, Fname FROM EMPLOYEE WHERE Salary > ALL ( SELECT Salary FROM EMPLOYEE WHERE Dno = 5 ); In general, we can have several levels of nested queries. We can once again be faced with possible ambiguity among attribute names if attributes of the same name exist—one in a relation in the FROM clause of the outer query, and another in a relation in the FROM clause of the nested query. The rule is that a reference to an unqualified attribute refers to the relation declared in the innermost nested query. The EXISTS and UNIQUE Functions in SQL EXISTS and UNIQUE are Boolean functions that return TRUE or FALSE; hence, they can be used in a WHERE clause condition. The EXISTS function in SQL is used to check whether the result of a nested query is empty (contains no tuples) or not. The result of EXISTS is a Boolean value TRUE if the nested query result contains at least one tuple, or FALSE if the nested query result contains no tuples. The SQL function UNIQUE(Q) Returns TRUE if there are no duplicate tuples in the result of query Q. Query 6 Retrieve the name of each employee who works on all the projects controlled by department number 5 can be written using EXISTS and NOT EXISTS in SQL systems. SELECT Fname, Lname FROM EMPLOYEE WHERE NOT EXISTS ( ( SELECT Pnumber FROM PROJECT WHERE Dnum = 5) EXCEPT ( SELECT Pno FROM WORKS_ON WHERE Ssn = Essn) ); 84 Database Systems II the first subquery (which is not correlated with the outer query) selects all projects controlled by department 5, and the second subquery (which is correlated) selects all projects that the particular employee being considered works on. If the set difference of the first subquery result MINUS (EXCEPT) the second subquery result is empty, it means that the employee works on all the projects and is therefore selected. More Subqueries Query 7 Subquery can be used in SELECT. SELECT SalesOrderNumber, SubTotal, OrderDate, ( SELECT SUM(OrderQty) FROM SalesOrderDetail WHERE SalesOrderID = 43659 ) AS TotalQuantity FROM SalesOrderHeader WHERE SalesOrderID = 43659; Query 8 Subquery can be used in FROM. SELECT sub.* FROM ( SELECT * FROM sf_crime_incidents_2014_01 WHERE day_of_week = 'Friday' ) sub WHERE sub.resolution = 'NONE' Subqueries can be used to aggregate in multiple stages. For example, what if you wanted to figure out how many incidents get reported on each day of the week? Better yet, what if you wanted to know how many incidents happen, on average, on a Friday in December? In January? There are two steps to this process: counting the number of incidents each day (inner query), then determining the monthly average (outer query): 85 Complex Queries & Active Databases 86 Query 9 SELECT LEFT(sub.date, 2) AS cleaned_month, sub.day_of_week, AVG(sub.incidents) AS average_incidents FROM ( SELECT day_of_week, date, COUNT(incidnt_num) AS incidents FROM sf_crime_incidents GROUP BY day_of_week, date ) sub GROUP BY cleaned_month, day_of_week ORDER BY cleaned_month, day_of_week Query 9A SELECT cleaned_month, day_of_week, average_incidents FROM (SELECT LEFT(sub.date, 2) AS cleaned_month, sub.day_of_week, AVG(sub.incidents) AS average_incidents FROM ( SELECT day_of_week, date, COUNT(incidnt_num) AS incidents FROM sf_crime_incidents GROUP BY day_of_week, date ) sub) sub1 GROUP BY cleaned_month, day_of_week ORDER BY cleaned_month, day_of_week Query 10 Subqueries can be used in conditional logic. You can use subqueries in conditional logic (in conjunction with WHERE, JOIN/ON). The following query returns all of the entries from the earliest date in a sales dataset: SELECT p.BusinessEntityID, p.FirstName, p.LastName, s.SalesQuota FROM Person p INNER JOIN SalesPerson s ON p.BusinessEntityID = s.BusinessEntityID WHERE s.SalesQuota IS NOT NULL AND s.SalesQuota > ( SELECT AVG(SalesQuota) FROM SalesPerson ); Query 11 It's common to join a subquery that hits the same table as the outer query rather than filtering in the WHERE clause. SELECT p.ProductID, p.Name AS ProductName, p.ProductSubcategoryID AS SubcategoryID, ps.Name AS SubcategoryName FROM Production.Product p INNER JOIN (SELECT ProductSubcategoryID, Name FROM Production.ProductSubcategory WHERE Name LIKE '%bikes%' ) AS ps ON p.ProductSubcategoryID = ps.ProductSubcategoryID; Database Systems II T-SQL Well-known String Functions The following table summarize the most well-known string functions. Functions Description CHARINDEX(substring, string, start) searches for a substring in a string, and returns the position PATINDEX(%pattern%, string) returns the position of a pattern in a string CONCAT(string1, …, stringN) LEN(string) adds two or more strings together returns the length of a string REPLACE(string, old_str, new_str) replaces all occurrences of a substring within a string, with a new substring REVERSE(string) LEFT(string, number_of_chars) reverses a string and returns the result extracts a number of characters from a string (starting from left) RIGHT(string, number_of_chars) extracts a number of characters from a string (starting from right) SUBSTRING(string, start, length) UPPER(text) extracts some characters from a string converts a string to upper-case LOWER(text) converts a string to lower-case 5.4 Views in SQL A view in SQL terminology is a single table that is derived from other tables. These other tables could be base tables or previously defined views. A view is considered as a virtual table in contrast to base tables whose tuples are stored in the database. As an example, we may frequently issue queries that retrieve the employee name and project names that the employee works on. Rather than having to specify the join of the EMPLOYEE, WORKS_ON, and PROJECT tables every time we issue that query, we can define a view that is a result of these joins and hence already includes the attributes we wish to retrieve frequently. We can now issue queries on the view, which are specified as a single table retrieval rather than retrievals involving two joins on three tables. Note that, a view is always up to date; if we modify the tuples in the base tables on which the view is defined, the view automatically reflects these changes. Hence, the view is not realized at the time of a view definition but rather at the time we specify a query on the view. It the responsibility of the DBMS and not the user to make sure that the view is up to date. 87 Complex Queries & Active Databases Specification of Views in SQL The command to specify a view is CREATE VIEW. We give the view a name, a list of attribute names, and a query to specify the contents of the view. If none of the view attributes result from applying functions or arithmetic operations, we do not have to specify attribute names for the view as they will be the same as the names of the attributes of the query which forming the view. Example 1 Create a view to retrieve the employee name, project name, and number of hours. CREATE VIEW WORKS_ON_VIEW AS SELECT FNAME, LNAME, PNAME, HOURS FROM EMPLOYEE, PROJECT, WORKS_ON WHERE SSN=ESSN AND PNO=PNUMBER; In this example we did not specify any new attribute names for the view WORKS_ON_VIEW. So, in this case the WORKS_ON_VIEW inherits the names of the view attributes from the defining tables. Example 2 Create a view to retrieve the department name, and number of employees in that department and their total salaries: CREATE VIEW DEPT_INFO ( DEPT_NAME, NO_OF_EMPS,TOTAL_SAL) AS SELECT DNAME, COUNT(*), SUM(SALARY) FROM EMPLOYEE, DEPARTMENT WHERE DNO=DNUMBER; GROUP BY DNAME; In this example we gave new names for the attributes of the view. We can specify SQL queries on a view see the following example: Example 3 Retrieve the employee names who work in the project name 'ProjectX', using the created view in Example 1: SELECT FNAME,LNAME FROM WORKS_ON_VIEW WHERE PNAME='ProjectX'; If we do not need the view anymore, we can use the DROP VIEW command to dispose of it as in the following example: 88 Database Systems II Example 4 Drop the view WORKS_ON_VIEW DROP VIEW WORKS_ON_VIEW; Remarks on Updating Views Updating of views is complicated and can be ambiguous. In summary, we can make the following observations: • A view with a single defining table is updateable if the view attributes contain the primary key or some other candidate key of the base relation because this maps each virtual view tuple to a single tuple. • Views defined on multiple tables using joins are generally not updateable. Views defined using grouping an aggregate function are not updateable. Examine the following examples in order to illustrate potential problems with updating a view defined on multiple tables: Consider the WORKS_ON1 view of Example1 and suppose we issue the command to update the PNAME attribute of 'John Smith' from 'ProductX' to 'ProductY'. So, the view update will be as follows: UPDATE WORKS_ON_VIEW SET PNAME='ProjectY' WHERE LNAME='Smith' AND FNAME='John' AND PNAME='ProjectX'. This query can be mapped into several updates on the base tables to give the desired update on the View. There are two possible updates corresponding to the previous view update: The First UPDATE UPDATE WORKS_ON SET PNO = ( SELECT PNUMBER FROM PROJECT WHERE PNAME='ProjectY') WHERE ESSN = ( SELECT SSN FROM EMPLOYEE WHERE LNAME='Smith' AND FNAME='John' ) AND PNO = (SELECT PNUMBER FROM PROJECT WHERE PNAME = 'ProjectX' ) The Second UPDATE UPDATE PROJECT SET PNAME = 'ProjectY' WHERE PNAME = 'ProjectX' 89 Complex Queries & Active Databases Remarks: a. The first update (update 1) relates 'John Smith' to the 'ProductY' project tuple in place of the 'ProductX' and it is the most likely update. However, The second update (update 2) would also give the desired update effect on the view, but it accomplishes this by changing the name of the 'ProductX' tuple in the PROJECT table to 'ProductY'. b. Some view updates may not make much sense; for example, modifying the TOTAL_SAL attribute of DEPT_INFO does not make sense because TOTAL_SAL is defined to be the sum of the individual employee salaries: UPDATE DEPT_INFO SET TOTAL_SAL = 100000 WHERE DNAME='Research' This view update can be satisfied through many possible updates on the related base tables, and no one can guess what such an update might mean and decide how to execute it? In general, we cannot guarantee that any view can be updated. 5.5 Specifying Indexes in SQL SQL has statement to create and drop indexes dynamically. These commands are generally considered to be part of the SQL DDL. An index makes accessing tuples based on conditions that involve its indexing attributes more efficient. This means that, in general, executing a query will take less time if some attributes involved in the query conditions (selection & join) were indexed. This improvement can be dramatic for queries where large tables are involved. The CREATE INDEX command is used to specify an index. Each index is given a name, which is used to drop the index when we do not need it anymore. For example, to create an index on LNAME attribute of the EMPLOYEE base table: CREATE INDEX LNAME_INDEX ON EMPLOYEE (LNAME); In general, the index is arranged in ascending order of the indexing attribute values. For example, to create an index on the combination of FNAME (descending), MINIT (ascending), and LNAME (ascending), we write the following command: CREATE INDEX NAMES_INDEX ON EMPLOYEE (LNAME ASC, FNAME DESC, MINT); There are two additional options on indexes in SQL: 90 Database Systems II 91 1. To specify the key constraint on the indexing attribute, or combination of attributes. CREATE UNIQUE INDEX SSN_INDEX ON EMPLOYEE (SSN); An attempt to create a unique index on an existing base table will fail if the current tuples in the table do not obey the uniqueness constraint on the indexing attribute. Also, notice creating an index on primary key is preferable. Two NULL values are considered equal for unique indexing purposes. 2. To specify a CLUSTER index, the key word CLUSTER is added at the end of the CREATE INDEX command. Join and selection condition are even more efficient when specified on attribute with cluster index. A base table can have at most one clustering index but any number of non-clustering indexes. The following example create an index on the DNO attribute of the EMPLOYEE table: CREATE INDEX DNO_INDEX ON EMPLOYEE (DNO) CLUSTER; In order to drop the DNO_INDEX we write the following command: DROP INDEX DNO_INDEX; 5.6 Specifying Constraints in SQL Because SQL allows NULLs as attribute values, a constraint NOT NULL may be specified if NULL is not permitted for a particular attribute. This is always implicitly specified for the attributes that are part of the primary key of each relation, but it can be specified for any other attributes whose values are required not to be NULL. It is also possible to define a default value for an attribute by appending the clause DEFAULT <value> to an attribute definition. The default value is included in any new tuple if an explicit value is not provided for that attribute. CREATE TABLE DEPARTMENT ( Mgr_ssn CHAR(9) NOT NULL DEFAULT '888665555', … , CONSTRAINT DEPTPK PRIMARY KEY(Dnumber), CONSTRAINT DEPTSK UNIQUE (Dname), CONSTRAINT DEPTMGRFK FOREIGN KEY (Mgr_ssn) REFERENCES EMPLOYEE(Ssn) ON DELETE SET DEFAULT ON UPDATE CASCADE ); Complex Queries & Active Databases Another type of constraint can restrict attribute or domain values using the CHECK clause following an attribute. For example, suppose that department numbers are restricted to integer numbers between 1 and 20; then, we can change the attribute declaration of Dnumber in the DEPARTMENT table Dnumber INT NOT NULL CHECK (Dnumber > 0 AND Dnumber < 21); 5.7 Specifying Constraints as Assertions In SQL, users can specify general constraints—those that do not fall into any of the categories described in SQL Data Definition and SQL Constraints —via declarative assertions, using the CREATE ASSERTION statement of the DDL. Each assertion is given a constraint name and is specified via a condition similar to the WHERE clause of an SQL query. For example, to specify the constraint that the salary of an employee must not be greater than the salary of the manager of the department that the employee works for in SQL, we can write the following assertion: CREATE ASSERTION SALARY_CONSTRAINT CHECK ( NOT EXISTS ( SELECT * FROM EMPLOYEE E, EMPLOYEE M, DEPARTMENT D WHERE E.Salary>M.Salary AND E.Dno=D.Dnumber AND D.Mgr_ssn=M.Ssn ) ); The constraint name SALARY_CONSTRAINT is followed by the keyword CHECK, which is followed by a condition in parentheses that must hold true on every database state for the assertion to be satisfied. The DBMS is responsible for ensuring that the condition is not violated. Any WHERE clause condition can be used, but many constraints can be specified using the EXISTS and NOT EXISTS style of SQL conditions. Whenever some tuples in the database cause the condition of an ASSERTION statement to evaluate to FALSE, the constraint is violated. The constraint is satisfied by a database state if no combination of tuples in that database state violates the constraint. The basic technique for writing such assertions is to specify a query that selects any tuples that violate the desired condition. By including this query inside a NOT EXISTS clause, the assertion will specify that the result of this query must be empty so that the condition will always be TRUE. Thus, the assertion is violated if the result of the query is not empty. In the preceding example, the query selects all employees whose salaries are greater than the salary of the manager of their department. If the result of the query is not empty, the assertion is violated."} ,  
      {"role" : "user" , "content" : "92 Database Systems II Note that the CHECK clause and constraint condition can also be used to specify constraints on individual attributes and domains and on individual tuples. A major difference between CREATE ASSERTION and the individual domain constraints and tuple constraints is that the CHECK clauses on individual attributes, domains, and tuples are checked in SQL only when tuples are inserted or updated. Hence, constraint checking can be implemented more efficiently by the DBMS in these cases. The schema designer should use CHECK on attributes, domains, and tuples only when he or she is sure that the constraint can only be violated by insertion or updating of tuples. On the other hand, the schema designer should use CREATE ASSERTION only in cases where it is not possible to use CHECK on attributes, domains, or tuples, so that simple checks are implemented more efficiently by the DBMS. 5.8 Stored Procedures Stored Procedures are program modules that are stored by the DBMS at the database server. it is sometimes useful to create database program modules—procedures or functions—that are stored and executed by the DBMS at the database server. These are historically known as database stored procedures, although they can be functions or procedures. The term used in the SQL standard for stored procedures is persistent stored modules because these programs are stored persistently by the DBMS, similarly to the persistent data stored by the DBMS. Stored procedures are useful in the following circumstances: • If a database program is needed by several applications, it can be stored at the server and invoked by any of the application programs. This reduces duplication of effort and improves software modularity. • Executing a program at the server can reduce data transfer and communication cost between the client and server in certain situations. • These procedures can enhance the modeling power provided by views by allowing more complex types of derived data to be made available to the database users via the stored procedures. • Additionally, they can be used to check for complex constraints that are beyond the specification power of assertions and triggers. In general, many commercial DBMSs allow stored procedures and functions to be written in a general-purpose programming language or simple SQL commands such as retrievals and updates. The general form of declaring stored procedures is as follows: CREATE PROCEDURE <procedure name> (<parameters>) <local declarations> <procedure body> ; 93 Complex Queries & Active Databases The parameters and local declarations are optional and are specified only if needed. For declaring a function, a return type is necessary, so the declaration form is: CREATE FUNCTION <function name> (<parameters>) RETURNS <return type> <local declarations> <function body> ; In general, each parameter should have a parameter type that is one of the SQL data types. Each parameter should also have a parameter mode, which is one of IN, OUT, or INOUT. These correspond to parameters whose values are input only, output (returned) only, or both input and output, respectively. Because the procedures and functions are stored persistently by the DBMS, it should be possible to call them from the various SQL interfaces and programming techniques. The EXEC or EXECUTE statement can be used to invoke a stored procedure. The format of the statement is as follows: EXECUTE <procedure or function name> (<argument list>) ; Example 1 The following SQL statement creates a stored procedure that selects Customers from a particular City from the \"Customers\" table: CREATE PROCEDURE SelectAllCustomers @City nvarchar(30) AS SELECT * FROM Customers WHERE City = @City GO Execute the stored procedure above as follows: EXEC SelectAllCustomers City = \"London\"; Example 2 The following SQL statement creates a stored procedure that selects Customers from a particular City with a particular PostalCode from the \"Customers\" table: CREATE PROCEDURE SelectAllCustomers @City nvarchar(30), @PostalCode nvarchar(10) AS SELECT * FROM Customers WHERE City = @City AND PostalCode = @PostalCode GO Execute the stored procedure above as follows: EXEC SelectAllCustomers City = \"London\", PostalCode = \"WA1 1DP\"; 94 Database Systems II Example 3 The following procedure returns two result sets. CREATE PROCEDURE MultipleResults AS SELECT TOP(10) BusinessEntityID, Lastname, FirstName FROM Person; SELECT TOP(10) CustomerID, AccountNumber FROM Customer; GO 99 Complex Queries & Active Databases 100 T-SQL Trigger Example 1 Insert Trigger CREATE TRIGGER AfterInsertTrigger ON TriggerDemo_Parent AFTER INSERT AS INSERT INTO TriggerDemo_History VALUES ((SELECT TOP 1 inserted.ID FROM inserted), 'Insert') GO Delete Trigger CREATE TRIGGER AfterDeleteTrigger ON TriggerDemo_Parent AFTER DELETE AS INSERT INTO TriggerDemo_History VALUES ((SELECT TOP 1 deleted.ID FROM deleted), 'Delete') GO Update Trigger CREATE TRIGGER AfterUPDATETrigger ON TriggerDemo_Parent AFTER UPDATE AS INSERT INTO TriggerDemo_History VALUES ((SELECT TOP 1 inserted.ID FROM inserted), 'UPDATE') GO T-SQL Trigger Example 2 The following DML trigger prints a message to the client when anyone tries to add or change data in the Customer table. CREATE TRIGGER reminder1 ON Customer AFTER INSERT, UPDATE AS RAISERROR ('Notify Customer Relations', 16, 10); GO Note: 16 is the Severity Level & 10 is the user state Database Systems II Remarks 1. severity Is the user-defined severity level associated with this message. Severity levels from 0 through 18 can be specified by any user. Severity levels from 19 through 25 can only be specified by members of the sysadmin. For severity levels from 19 through 25, the WITH LOG option is required. Severity levels from 20 through 25 are considered fatal. If a fatal severity level is encountered, the client connection is terminated after receiving the message, and the error is logged in the error and application logs. 2. State Is an integer from 0 through 255. Negative values default to 1. Values larger than 255 should not be used. If the same user-defined error is raised at multiple locations, using a unique state number for each location can help find which section of code is raising the errors. Because CHECK constraints can reference only the columns on which the column-level or table-level constraint is defined, any cross-table constraints (in this case, business rules) must be defined as triggers. The Example 3 creates a DML trigger in the AdventureWorks2012 database. This trigger checks to make sure the credit rating for the vendor is good (not 5) when an attempt is made to insert a new purchase order into the PurchaseOrderHeader table. T-SQL Trigger Example 3 To obtain the credit rating of the vendor, the Vendor table must be referenced. If the credit rating is too low, a message is displayed, and the insertion does not execute. 101 Complex Queries & Active Databases CREATE TRIGGER LowCredit ON PurchaseOrderHeader AFTER INSERT AS IF (@@ROWCOUNT_BIG = 0) RETURN; IF EXISTS (SELECT * FROM inserted AS i JOIN Vendor AS v ON v.BusinessEntityID = i.VendorID WHERE v.CreditRating = 5 ) BEGIN RAISERROR ('A vendor''s credit rating is too low to accept new purchase orders.', 16, 1); ROLLBACK TRANSACTION; RETURN END; GO Remarks: 1. This Trigger can be written using BEFORE OR INSTEAD_OF 2. @@ROWCOUNT_BIG Returns the number of rows affected by the last query The second type of DML triggers is the INSTEAD OF DML trigger. As mentioned previously, the INSTEAD OF trigger will override the statement of the action that fires the trigger with the statement provided in the trigger. Assume that we need to log the DML actions that users are trying to perform on a specific table, without allowing them to perform that action. The CREATE TABLE T-SQL statements below can be used to create both the source and alternative tables: CREATE TABLE TriggerDemo_NewParent ( ID INT IDENTITY (1,1) Emp_First_name VARCHAR (50), Emp_Last_name VARCHAR (50), Emp_Salary INT ) GO CREATE TABLE TriggerDemo_InsteadParent ( ID PRIMARY KEY, INT IDENTITY (1,1) ParentID INT, PerformedAction VARCHAR (50), ) GO PRIMARY KEY, 102 Database Systems II T-SQL Trigger Example 4 Insert Trigger CREATE TRIGGER InsteadOfInsertTrigger ON TriggerDemo_NewParent INSTEAD OF INSERT AS INSERT INTO TriggerDemo_InsteadParent VALUES ((SELECT TOP 1 inserted.ID FROM inserted), 'Trying to Insert new ID') GO Update Trigger CREATE TRIGGER InsteadOfUpdateTrigger ON TriggerDemo_NewParent INSTEAD OF UPDATE AS INSERT INTO TriggerDemo_InsteadParent VALUES ((SELECT TOP 1 inserted.ID FROM inserted), 'Trying to Update an existing ID') GO T-SQL Trigger Example 5 Insert data in view and verify functionality of INSTEAD OF trigger USE AdventureWorks GO CREATE TRIGGER INSTEADOF_TR_I_EmpQualification ON vw_EmpQualification INSTEAD OF INSERT AS BEGIN DECLARE @Code TINYINT SELECT @Code = qualificationCode FROM lib_Qualification L INNER JOIN INSERTED I ON L.qualification = I.qualification IF (@code is NULL ) BEGIN RAISERROR (N'The provided qualification does not exist in qualification library', 16, 1) RETURN END INSERT INTO employees (empcode, name, designation,qualificationCode,deleted) SELECT empcode, name, designation, @code, 0 FROM inserted END GO 103 Complex Queries & Active Databases 104 T-SQL Trigger Example 6 This Example Returns a Boolean value that indicates whether an INSERT or UPDATE attempt was made on a specified column of a table or view. UPDATE() is used anywhere inside the body of a T-SQL INSERT or UPDATE trigger to test whether the trigger should execute certain actions. Update data in view and verify functionality of INSTEAD OF trigger USE AdventureWorks GO CREATE TRIGGER INSTEADOF_TR_U_EmpQualification ON vw_EmpQualification INSTEAD OF UPDATE AS BEGIN IF (UPDATE(qualification)) -- If qualification is updated BEGIN DECLARE @code TINYINT UPDATE employees SET @code = L.qualificationcode FROM lib_qualification L INNER JOIN inserted I ON L.qualification = I.qualification IF (@code is NULL ) BEGIN RAISERROR (N'The provided qualification does not exist in qualification library', 16, 1) RETURN END UPDATE employees SET qualificationCode = @code FROM inserted I INNER JOIN employees E ON I.empcode = E.empcode END IF (UPDATE(EmpCode)) -- If employee code is updated BEGIN RAISERROR (N'You cannot edit employee code, Transaction has been failed', 16, 1) RETURN END IF (UPDATE(name)) -- If name is updated BEGIN UPDATE employees SET name = I.name FROM inserted I INNER JOIN employees E ON I.empcode = E.empcode WHERE E.empcode = I.empcode END Database Systems II 105 IF (UPDATE(designation)) -- If designation is updated BEGIN UPDATE employees SET designation = I.designation FROM inserted I INNER JOIN employees E ON I.empcode = E.empcode WHERE E.empcode = I.empcode END END GO Construction Database Trigger T-SQL Example Consider the Construction Enterprise Database (Chapter 1, Exercise 1). In Example prevent \"Equipment to be assigned to more than one project at a time\". This trigger can be written using AFTER and INSTEAD OF as follows: CREATE TRIGGER T_OVERLAP_EQUIPMENT_ASSIGNMENT ON tbl_Assignment AFTER INSERT AS BEGIN SET NOCOUNT ON; -- Hide number of rows affected IF (( SELECT COUNT(*) FROM Inserted AS I INNER JOIN tbl_Assignment AS a ON i.EID = a.EID WHERE (i.StartDate >= a.StartDate AND i.EndDate <= a.EndDate) OR (i.StartDate < a.StartDate AND i.EndDate <= a.EndDate) OR (i.StartDate >= a.StartDate AND i.EndDate > a.EndDate) OR (i.StartDate < a.StartDate AND i.EndDate > a.EndDate)) >= 2) BEGIN RAISERROR (N'Equipment already assigned to another project', 16, 1) ROLLBACK TRANSACTION END END Complex Queries & Active Databases 106 Same functionality could be done using the INSTEAD OF Trigger: CREATE TRIGGER T_OVERLAP_EQUIPMENT_ASSIGNMENT ON tbl_Assignment INSTEAD OF INSERT AS BEGIN SET NOCOUNT ON; -- Hide number of rows affected IF (( SELECT COUNT(*) FROM Inserted AS I INNER JOIN tbl_Assignment AS a ON i.EID = a.EID WHERE (i.StartDate >= a.StartDate AND i.EndDate <= a.EndDate) OR (i.StartDate < a.StartDate AND i.EndDate <= a.EndDate) OR (i.StartDate >= a.StartDate AND i.EndDate > a.EndDate) OR (i.StartDate < a.StartDate AND i.EndDate > a.EndDate)) >= 1) BEGIN RAISERROR (N'Equipment already assigned to another project', 16, 1) ELSE BEGIN INSERT INTO tbl_Assignment SELECT PID, EID, StartDate, EndDate FROM Inserted END END Chapter 6  Database Security 6.1 Types of Security Database security is a broad area that addresses many issues, including the following: • Various legal and ethical issues regarding the right to access certain information  For example, some information may be deemed to be private and cannot be accessed legally by unauthorized organizations or persons.  • Policy issues at the governmental, institutional, or corporate level regarding what kinds of information should not be made publicly available. For example, credit ratings and personal medical records. • System-related issues such as the system levels at which various security functions should be enforced. For example, whether a security function should be handled at the physical hardware level, the operating system level, or the DBMS level. • The need in some organizations to identify multiple security levels and to categorize the data and users based on these classifications. For example, top secret, secret, confidential, and unclassified. The security policy of the organization with respect to permitting access to various classifications of data must be enforced. Threats to Databases • Loss of integrity. Database integrity refers to the requirement that information be protected from improper modification.  o Modification of data includes creating, inserting, and updating data; changing the status of data; and deleting data.  o Integrity is lost if unauthorized changes are made to the data by either intentional or accidental acts. If the loss of system or data integrity is not corrected, continued use of the contaminated system or corrupted data could result in inaccuracy, fraud, or erroneous decisions. • Loss of availability. Database availability refers to making objects available to a human user or a program who/which has a legitimate right to those data objects.  o Loss of availability occurs when the user or program cannot access these objects. • Loss of confidentiality. Database confidentiality refers to the protection of data from unauthorized disclosure.  o The impact of unauthorized disclosure of confidential information can range from violation of the Data Privacy Act to the jeopardization of national security.  108 Database Systems II o Unauthorized, unanticipated, or unintentional disclosure could result in loss of public confidence, embarrassment, or legal action against the organization. In a multiuser database system, the DBMS must provide techniques to enable certain users or user groups to access selected portions of a database without gaining access to the rest of the database.  For example, sensitive information such as employee salaries or performance reviews should be kept confidential from most of the database system's users. A DBMS typically includes a database security and authorization subsystem that is responsible for ensuring the security of portions of a database against unauthorized access.  It is now ordinary to refer to two types of database security mechanisms: • Discretionary security mechanisms. These are used to grant privileges to users, including the capability to access specific data files, records, or fields in a specified mode (such as read, insert, delete, or update). • Mandatory security mechanisms. These are used to enforce multilevel security by classifying the data and users into various security classes (or levels) and then implementing the appropriate security policy of the organization. o For example, a typical security policy is to permit users at a certain classification (or clearance) level to see only the data items classified at the user's own (or lower) classification level.  o An extension of this is role-based security, which enforces policies and privileges based on the concept of organizational roles. "} , 
      {"role" : "user" , "content" : "Control Measures Four main control measures are used to provide security of data in databases: • Access control o The security mechanism of a DBMS must include provisions for restricting access to the database system as a whole.  o is handled by creating user accounts and passwords to control the login process by the DBMS. • Inference control o Statistical database users such as government statisticians or market research firms are allowed to access the database to retrieve statistical information about a population but not to access the detailed confidential information about specific individuals. • Flow control o prevents information from flowing in such a way that it reaches unauthorized users. • Data encryption 109 Database Security  o used to protect sensitive data (such as credit card numbers) that is transmitted via some type of communications network.  o The data is encoded using some coding algorithm. An unauthorized user who accesses encoded data will have difficulty deciphering it, but authorized users are given decoding or decrypting algorithms (or keys) to decipher the data. Database Security and the DBA The database administrator (DBA) is the central authority for managing a database system. The DBA's responsibilities include granting privileges to users who need to use the system and classifying users and data in accordance with the policy of the organization.  The DBA has a DBA account in the DBMS, sometimes called a system or superuser account, which provides powerful capabilities that are not made available to regular database accounts and users. DBA-privileged commands include commands for granting and revoking privileges to individual accounts, users, or user groups and for performing the following types of actions: 1. Account creation. This action creates a new account and password for a user or a group of users to enable access to the DBMS. 2. Privilege granting. This action permits the DBA to grant certain privileges to certain accounts. 3. Privilege revocation. This action permits the DBA to revoke (cancel) certain privileges that were previously given to certain accounts. 4. Security level assignment. This action consists of assigning user accounts to the appropriate security clearance level. Access Control, User Accounts, and Database Audits The DBA will then create a new account number and password for the user if there is a legitimate need to access the database. The user must log in to the DBMS by entering the account number and password whenever database access is needed. The database system must also keep track of all operations on the database that are applied by a certain user throughout each login session, which consists of the sequence of database interactions that a user performs from the time of logging in to the time of logging off.  When a user logs in, the DBMS can record the user's account number and associate it with the computer or device from which the user logged in. To keep a record of all updates applied to the database and of particular users who applied each update, we can modify the system log. (Recall Recovery Chapter) that the system log includes an entry for each operation applied to the database that may be required for recovery from a transaction failure or system crash. We can expand the log entries so that they also include 110 Database Systems II the account number of the user and the online computer or device ID that applied each operation recorded in the log.  If any tampering with the database is suspected, a database audit is performed, which consists of reviewing the log to examine all accesses and operations applied to the database during a certain time period. When an illegal or unauthorized operation is found, the DBA can determine the account number used to perform the operation. A database log that is used mainly for security purposes serves as an audit trail. 6.2 Discretionary Access Control The typical method of enforcing discretionary access control in a database system is based on the granting and revoking of privileges. Types of Discretionary Privileges Informally, there are two levels for assigning privileges to use the database system: 1. The account levels. At this level, the DBA specifies the particular privileges that each account holds independently of the relations in the database. 2. The relation (or table) level. At this level, the DBA can control the privilege to access each individual relation (or table) or view in the database. Privileges, Roles, Schema and Database Users: Privileges:  Privileges are the right to execute SQL statement. The database administrator is a high-level user with the ability to grant user access to the database and its objects. The users require system privileges to gain access to the database and object privileges to manipulate the contents of the objects in the database.  Users also can be given the privileges to grant additional privileges to other users or to roles.  1. Roles Roles are named groups of related privileges.  2. Schema is a collection of objects such as tables, Views, and indexes. The schema is owned by a Database user and has the same name of that user.  3. Creating User The Database Administrator (DBA) creates new user by executing the CREATE USER command with the following syntax:  CREATE USER < username> IDENTIFIED BY < password >; 111 Database Security  When a user is created, he doesn't have any privileges, so DBA allocates a number of system privileges to that user. System privileges in turn determine what the user can do the database level.  Example CREATE USER Hassan IDENTIFIED BY Has; By this command the DBA has created the user and can assign privileges to that user.  Typical User Privileges System Privilege Operations Authorized Create Session Create Table Create View Connect to the database Create tables in the user's Schema Create Views in the user's Schema Granting and Revoking of Privileges Suppose that the DBA creates four accounts—A1, A2, A3, and A4 and wants only A1 to be able to create base relations. To do this, the DBA must issue the following GRANT command in SQL: GRANT CREATETAB TO A1; The CREATETAB (create table) privilege gives account A1 the capability to create new database tables (base relations) and is hence an account privilege. In SQL92, the same effect can be accomplished by having the DBA issue a CREATE SCHEMA command, as follows: CREATE SCHEMA EXAMPLE AUTHORIZATION A1; User account A1 can now create tables under the schema called EXAMPLE. To continue our example, suppose that A1 creates the two base relations EMPLOYEE and DEPARTMENT; A1 is then the owner of these two relations and hence has all the relation privileges on each of them. Next, suppose that account A1 wants to grant to account A2 the privilege to insert and delete tuples in both of these relations.  However, A1 does not want A2 to be able to propagate these privileges to additional accounts. A1 can issue the following command: GRANT INSERT, DELETE ON EMPLOYEE, DEPARTMENT TO A2; 112 Database Systems II Next, suppose that A1 wants to allow account A3 to retrieve information from either of the two tables and to be able to propagate the SELECT privilege to other accounts. A1 can issue the following command: GRANT SELECT ON EMPLOYEE, DEPARTMENT TO A3 WITH GRANT OPTION; The clause WITH GRANT OPTION means that A3 can now propagate the privilege to other accounts by using GRANT.  For example, A3 can grant the SELECT privilege on the EMPLOYEE relation to A4 by issuing the following command: GRANT SELECT ON EMPLOYEE TO A4; Notice that A4 cannot propagate the SELECT privilege to other accounts because the GRANT OPTION was not given to A4. Now suppose that A1 decides to revoke the SELECT privilege on the EMPLOYEE relation from A3; A1 then can issue this command: REVOKE SELECT ON EMPLOYEE FROM A3; The DBMS must now revoke the SELECT privilege on EMPLOYEE from A3, and it must also automatically revoke the SELECT privilege on EMPLOYEE from A4. This is because A3 granted that privilege to A4, but A3 does not have the privilege any more. Next, suppose that A1 wants to give back to A3 a limited capability to SELECT from the EMPLOYEE relation and wants to allow A3 to be able to propagate the privilege. The limitation is to retrieve only the Name, Bdate, and Address attributes and only for the tuples with Dno = 5. A1 then can create the following view: CREATE VIEW A3EMPLOYEE AS SELECT Name, Bdate, Address FROM EMPLOYEE WHERE Dno = 5; After the view is created, A1 can grant SELECT on the view A3EMPLOYEE to A3 as follows: GRANT SELECT ON A3EMPLOYEE TO A3 WITH GRANT OPTION; Finally, suppose that A1 wants to allow A4 to update only the Salary attribute of EMPLOYEE; A1 can then issue the following command: GRANT UPDATE ON EMPLOYEE (Salary) TO A4; 113 Database Security  Granting Object Privileges The DBA can allow users to perform a particular action on a specific table, view by granting the object privileges. These privileges are shown in the following table: Object Privilege Table View ALTER yes DELETE yes no yes INDEX yes INSERT yes no yes SELECT yes UPDATE yes yes 114 yes Chapter 7  Database Recovery & Concurrency 7.1  Transaction Processing Systems  They are systems with large databases and hundreds of concurrent users that are executing database transactions. Transaction: It is an executing program that forms a logical unit of database processing. A Transaction includes one or more database access operations (e.g. insertion, deletion, and modification or retrieval operations). Transaction can also be defined as an atomic unit of work that is either completed entirely or not at all, if it fails for any reason. Examples of Transaction Processing Systems: • Reservation Systems • Banking Systems • Supermarket Checkout Systems In these systems a Concurrency Control Problem arises which occurs when multiple transactions submitted by various users interfere with one another in a way that produces incorrect results. Single-User versus Multi-User Systems In a computer system, with single central processing unit, executing multiple programs (processes or transactions) is allowed (i.e. multiple user environments), using Interleaving Concept. Interleaving keeps the CPU busy when a process requires   CPU is switched to execute another process if the current process has an I/O operation or it has finished the currently allocated CPU slot of time to it. Figure 7.1 shows Interleaving Processing versus Parallel Processing of concurrent transactions. Figure 7.1 116 Database Systems II Basic Database Access Operations  The basic database access operations that a transaction can include are as follows: 1. Read-Item(X): Reads a database item named X into a program variable (also named X for simplicity).  2. Write-Item (X): Writes value of program variable X into the database item named X. Executing Read-Item (X) command includes the following steps: 1. Find the address of the disk block that contains item X. 2. Copy that disk block into a buffer in main memory (if that disk block is not already in some main memory buffer). 3. Copy item X from the buffer to the program variable named X. Executing Write-Item (X) command includes the following steps: 1. Find the address of the disk block that contains item X. 2. Copy that disk block into a buffer in main memory (if that disk block is not already in some main memory buffer). 3. Copy item X from the program variable named X into its location in the buffer. 4. Store the updated block from the buffer back to disk (either immediately or at some later point of time). Why Concurrency Control is Needed? In a Multi-User environment, transaction s submitted by the various users may execute concurrently and may access and update the same database items. If this concurrent execution is uncontrolled, it leads to the following main problems: 1. The Lost Update Problem: This Problem occurs when two transactions that access the same database items have their operations interleaved in a way that makes the value of some database items incorrect (see Figure 7.2a). 2. The Temporary Update (or Dirty Read) Problem: This problem occurs when one transaction updates a database item and then the transaction fails for some reasons. The updated item is accessed by another transaction before it is changed back to its original value (see Figure 7.2b) 3. The Incorrect Summary Problem: If one transaction is calculating an aggregate function on a number of records while other transactions are updating some of these records, the aggregate function may calculate some values before they are updated and other after they are updated (Figure 7.2c). 117 Database Recovery & Concurrency  Figure 7.2 Why Recovery Is Needed? Recovery is required to ensure the following when transaction is submitted to DBMS for execution: 1. All the operations in the transaction are completed successfully and their effect is recorded permanently in the database. 2. It is not allowed to some of transaction operations to update database, if transaction fails for any reasons.  118 Database Systems II Types of Failures: Failures are generally classified as Transaction, System and Media failures. These are several possible reasons for a transaction to fail in the middle of execution: 1. Computer failure (system crash): A hardware, software, or network error occurs in the computer system during transaction execution. Disk failure is one of the main media failures since it contains the system log file.  2. A transaction or system error:  Some operations in a transaction may cause it fails (e.g. divide by zero or logical programming errors.  3. Local errors or exception conditions detected by the transaction:  During transaction execution, certain conditions may occur that necessitate transaction cancellation (e.g. data   for transaction not found or insufficient account balance). 4. Concurrency control enforcement: The concurrency control method may decide to abort the transaction to be restarted later. 5. Physical problems and catastr6ophes Transaction States & Operations As mentioned earlier a transaction is an atomic unit of work that is either completed entirely or not done at all. For recovery purposes, the system needs to keep track of the following operations by DBMS Recovery Manager: 1. Begin Transaction This marks the begging of transaction execution.   2. Read or Write Operations of Database Items in a Transaction  Some operations in a transaction may cause it fails (e.g. divide by zero or logical programming errors.  3. End-transaction This marks the end of transaction execution. At this point it is necessary to check whether the changes introduced by the transaction can be permanently applied to the database (committed) or whether the transaction must be aborted. 4. Commit-transaction This marks a successful end of a transaction, so that any changes executed by the transaction can be safely committed. 5. Rollback (Abort) This marks an unsuccessful end of a transaction so that any changes performed by transaction to the database must be undone. Figure 7.3 shows the sate transition diagram that describes how the transaction moves through its execution states. 119 Database Recovery & Concurrency  Figure 7.3 Remark: When transaction ends, it moves to the Partially Committed State. At this point, some recovery protocols need to ensure that a system failure will not result, so an inability to record the changes of transaction permanently (this done by recording changes in the System Log). Also, at this point the DBMS concurrency control system can force the transaction to a failed state or it may fail and aborted during its active state. Failed or aborted transaction may be restarted later either automatically or after being resubmitted by the user. Aborted or failed transaction changes must be rollback. The System Log To be able to recover from failures that affect transactions, the system maintains a Log to keep track of all transaction operations that affect the values of database items. The records (entries) that are written to the Log have a unique Transaction-ID (T). This key is generated automatically by the system to identify each transaction.  The Log entries have the following forms: 1. [Start-Transaction, T]  2. [Write-Item, T, X, Old-value, [new-value] 3. [Read- Item, T, X]  4. [Commit, T] Indicates that transaction T has completed successfully, and its effect can be committed (recorded permanently to the database). 5.  [Abort, T]  120 Database Systems II Commit Point of a Transaction A transaction T reaches its commit point when all its operations that access the database have been executed successfully and the effect of all the transaction operations on the database have been recorded in the Log. Beyond the Commit point, the transaction is said to be committed, and its effect is assumed to be permanently recorded in the database. The transaction then writes a commit record [commit, T] into the Log. If a system failure occurs, we search back in the Log for all transactions T that have written a [Start-Transaction, T] record in the Log but have not written their [Commit, T] record yet; these transactions may have to be rolled back to Undo their effect on the database during the recovery process s.  Transactions that have written their commit record in the Log must also have recorded all their write operations in the Log, so their effect on the database can be redone from the Log records.  Desirable Properties of Transactions Transactions should possess several properties. These are often called the ACID properties. The following are the ACID properties:   1. Atomicity: A transaction is an atomic unit of processing to be performed entirely or not performed at all. It is the responsibility of the transaction recovery subsystem. 2. Consistency preservation:    A transaction is consistency preserving, execution takes the database from a consistent state to another consistent state. It is the responsibility of the programmers who write the database program to enforce integrity constraints. A consistent state of the database satisfies the constraints specified in the schema. 3. Isolation: A transaction execution should not be interfered with any other transactions executing concurrently. Isolation may be enforced by hiding (invisible) its updates to other transactions until it is committed (this will avoid dirty read problem). It is responsibility of the concurrency control subsystem.  4. Durability: The changes applied to the database by a committed transaction must persist in the database. These changes must not be lost because of any failure. It is responsibility of the recovery subsystem of the DBMS."} , 
      {"role" : "user" , "content" : "121 Database Recovery & Concurrency  Schedules (Histories) of transactions Schedule:  It is order of execution of operations from various transactions that are concurrently executed in an interleaved fashion.  A Schedule (History) S of n transactions (T1, T2, …, Tn) is an ordering of the operations of the transactions subject to the constraint that, for each transaction Ti in S, the operations of Ti must appear in the same order in which they occur in Ti . Schedule Examples:  1. Schedule of Figure (3 (a) ) Sa: r1(X); r2(X); w1(X); r1(Y); w2(X); w1(Y);  2. Schedule of Figure (3 (b))  Sb: r1(X); w1(X); r2(X); w2(X); r1(Y); a1;  Conflict Operations in a Schedule  Two operations in a schedule are said to be conflict if they satisfy all the following three conditions: 1. They belong to different transactions 2. They access the same database item. 3. At least one of the operations is write- item.  Examples of the conflicting operations: r1(X) and w2(X) in schedule Sa. r2(X) and w1(X) in schedule Sa. w1(X) and w2(X) in schedule Sa. Examples of the non-conflicting operations: r1(X) and w1(X) in schedule Sa. w2(X) and w1(Y) in schedule Sa. 122 Database Systems II Complete Schedule  A schedule S of n transactions (T1, T2, …, Tn) is said to be complete if the following conditions hold: 1. The operations in S are exactly those operations in (T1, T2, … , Tn) including a commit or abort operation as the last operation for each transaction in the schedule. 2. For any pair of operations from the same transaction Ti , their order of appearance in S is the same as their order of appearance in Ti. 3. For any two conflicting operations, one of the two must occur before the other in the schedule (i.e., ordering of non-conflicting operations in two different transactions is not important). Committed Projection of a schedule The committed projection c(S) of a     schedules includes only the operation in S that belong to committed transactions (i.e. Ti that has Ci operation) 7.2 Characterizing Schedules Based on Recoverability  Recoverable Schedules: They are schedules which ensure that any committed Transaction. Should never be a necessity to rollback. A schedule S is recoverable if no transaction T in S commits until all transaction T` have written on item that T reads (after the write item of T`)   Examples: SC: r1 (X); w1 (X); r2 (X); r1(Y); w2 (X); c2; a1; Not Recoverable (T2 must abort  T1 abort) Sd: r1 (X); w1(X); r2 (X); r1 (Y); w2(X); w1(Y); c1; c2;  Recoverable Postponed until T1 commits  Se: r1 (X); w1(X); r2(X); r1(Y); w2(X); w1(Y); a1; a2;  Recoverable Cascading Rollback (time consuming) Check the schedule S: r1(X); r2 (X); w1(X); r1(Y); w2(X); c2; w1(Y); c1; Remark: A Schedule is said to be cascade less, or to avoid cascading rollback, if every transaction in the schedule reads only items that were written by committed transactions. In this case, all items read will not be discarded, so no cascading rollback will occur. To satisfy this criterion, the r2(X) command in schedules Sd and Sc must be postponed until after T1 has committed (or aborted), thus delaying T2 ensuring no cascading rollback if T1 aborts. 123 Database Recovery & Concurrency  Another Example: Determine if the following Schedule is recoverable or unrecoverable: S:r1(X);r2(X);w1(X);r1(Y);w2(X);c2;w1(Y);c1; S: r1(X); r2(X); w1(X); r1(Y); w2(X); c2;w1(Y);a1;  Strict Schedules: transactions in these schedules can neither read nor write item x until the last transaction that wrote x has committed (or aborted). Strict Schedules simplify the recovery process. In a strict Schedule, the process of undoing a write item (x) operation of an aborted transaction is simply to restore the before image (old value or BFIM) of data item x (because there is no interference with other transactions). This simply procedure always works correctly for strict schedules, but it may not work for recoverable or cascade less schedules. For example, consider schedule S:w1(x , s);w2(x,8);a1 (it is not strict Schedule). Suppose that the value of x was originally 9, which is the BFIM stored in the system log along with w1(x, s) operation. If T1 aborts, as in S, the recovery procedure will restore the value of x to 9 , even though it has already been changed to 8 by transaction t2 , thus leading to in correct result. Although schedule S is cascade less, it is not a strict schedule since it permits T2 to write item x even though the transaction T1 that last wrote x had not yet committed (or aborted). A strict schedule does not have this problem. We have now characterized schedules according to the following terms: 1. Recoverability 2. Avoidance of cascading rollback 3. strictness We have thus seen that those properties of schedules are successively more stringent condition. Thus condition (2) implies condition (1) and condition (3) implies both (2) and (1).  Thus, all strict schedules are cascade less and all cascade less are recoverable.   7.3 Characterizing Schedules Based ON Serializability                    The concept of Serializability of schedules is used to identify which schedules are correct when transaction executions have interleaving of their operations in the schedules. If no interleaving is considered in the transaction execution, it is called serial schedules otherwise it is called non serial. If a schedule has n transaction, then there are !n serial schedules. Suppose that two users submit to the DBMS transactions T1 & T2 at approximately the same time. So, there will be !2 serial scheduler and money possible orders in which the system can execute the individual operations of the of the transactions T1 & T2 if interleaving execution is considered. Some of these alternatives are shown in figure 7.4. 124 Database Systems II Figure 7.4 Conflict – Serializable Schedules Formally, a schedule S is serial if, for every transaction T participating in the schedule, all the operations of T are executed consecutively (no concurrency) in the schedule, otherwise, the schedule is called non serial.  One reasonable assumption we can make, if we consider the transactions to be independent (no conflicting operations), is that every serial schedule is considered correct. Note that if there is a dependency, serial schedules of the same transaction may have different effect on the database. The main problem with serial schedules that they prevent interleaving. So, we do not utilize CPU time efficiently and they are unacceptable in practice.     Serializable schedule (conflict serializable schedule) A schedule S of N interleaved transactions is Serializable if it is conflict equivalent to some serial schedules of the same N transactions. Conflict equivalence of schedules: Two schedules are said to be conflict equivalent if the order of any two conflicting operations is the same in both schedules. For example, if a read and write operation occur in the order r1(X), w2(X) in schedule S1 and in the reverse order w2(X), r1(X) in schedule S2, the value read by r1(X) can be different in the two schedules.    125 Database Recovery & Concurrency  Remarks: 1. If the schedule S is Serializable, in such a case we can reorder the non-conflicting operations of S until we form the equivalent serial schedule S`. Schedule D (shown in Figure 7.4) is equivalent to the serial schedule A.  2. Result equivalent of schedule cannot be used to define equivalence of schedules (see example Figure 7.5). Figure 7.5 Two serial schedules may have different effect on the Database if they are dependent (generate different results). Algorithm for testing conflict Serializability of schedules:   Algorithm:  Testing Conflict Serializability of a Schedule S 1. For each transaction Ti participating in schedule S, create a node labelled Ti in the precedence graph. 2. For each case in S where Tj executes a read_item(X) after Ti executes a write_item(X), create an edge (Ti → Tj) in the precedence graph. 3. For each case in S where Tj executes a write_item(X) after Ti executes a read_item(X), create an edge (Ti → Tj) in the precedence graph. 4. For each case in S where Tj executes a write_item(X) after Ti executes a write_item(X), create an edge (Ti → Tj) in the precedence graph. 5. The schedule S is serializable if and only if the precedence graph has no cycles. Figure 7.6 126 Database Systems II Figure 7.7 127 Database Recovery & Concurrency  Figure 7.8 7.4 Recovery Concepts Categorization of recovery algorithms: Recovery from transactions failures means that the database is restored to the most recent consistent state just before the time of failure using the transaction information stored in the system log. Conceptually, two techniques for recovery from non-catastrophic transaction failures: 1. Deferred update 2. Immediate update Deferred update: This technique does not physically update the database on disk until after a transaction reaches its commit point. Before reaching commit, all transaction updates are recorded in the local transaction workspace (or buffers). During commit, the updates are recorded first in the log and then written to the database. If a transaction fails before reaching its commit point, it will not have changed the database in any way, so UNDO is not needed. The technique is also called NO_REDO /REDO algorithm, hence some committed transaction may be required to be redone. 128 Database Systems II Immediate update: In this technique the database may be un-updated by some operations of a transaction before the transaction reaches its commit point. However, these operations are recorded in the log on disk by force writing before they are applied to the database, making recovery still possible. So, this algorithm is called UNDO|REDO algorithm. (to give a chance for a transaction to fail and recovery from failure is possible). Caching (Buffering) of disk blocks 1. Caching (buffering) of disk pages is an operating system function but because of its importance to the efficiency of recovery procedures, it's handled by the DBMS through calling low-level operating system routines. 2. DBMS cash: Collection of in-memory buffers to hold database disk pages (blocks). 3. Cache Directory: It's a database table that is used to keep track of which database items are in the DBMS cash. This table may contain <disk page address, buffer location, dirty bit, pin_unpin bit>. 4. Handling cache directory: When the DBMS requests action on some item, it first checks the cache directory to determine whether the disk page containing the item is in the cache. If it's not, then the item must be loaded on disk and the appropriate disk pages are copied into the cache. It may be necessary to replace (or flush) some of the cache buffers to space available for the new item. The replacement strategies may be: • LRU (Least recently used) page. • FIFO page. 5. Dirty Bit: It's bit associated with each buffer in the DBMS cache. When a page is first read from the database disk into a cache buffers, the cache directory is updated with the new disk page address, and the dirty bit is set to zero. As soon as the buffer is modified the dirty bit for the corresponding directory entry is set to 1 (one). When the buffer contents are replaced (flushed) from the cache, the contents must be written back to the corresponding disk page only if its dirty bit is 1. So, the dirty bit is used to indicate whether the cached page (buffer) is modified or not. 6. Pin_Unpin Bit: A page in DBMS cache is pinned (bit=1) if it cannot be written back to disk yet. 129 Database Recovery & Concurrency  Two main strategies can be used when flushing a modified buffer back to the disk: 1. In-place updating 2. Shadowing updating In-place updating Writes the buffer back the same original disk function (overwriting old values). Shadowing Writes an update buffer at a different disk location, so multiple versions of data items can be maintained. The BFIM, the old value of the data item before updating is the before image. The AFIM, the new value of data item after updating (After image). In shadowing both BFIM &AFIM can be kept on disk which facilitates recovery process. Transaction rollback If a transaction fails for whatever reason after updating the database, it may be necessary to roll back the transaction. If any data item values have been changed by the transaction and written to the database, they must be restored to their previous values (BFIMs). The undo type log entries are used to restore the old values of data items that must be rolled back. Definition Cascading Rollback: If a transaction (T) is rolled back, any transaction (S) that has read the value of some item (X) written by (T), then (S) also must be rolled back. The transaction (S) rolling back may affect other transactions to be rolled back as well and so on. This phenomenon is called cascading rollback and can occur when the recovery protocol ensures recoverable schedules but does not ensure strict or cascade less schedules. See example Figure 7.9. 130  Database Systems II   131   Figure 7.9     Database Recovery & Concurrency  Checkpoint It's a type of entry in the log file. A [checkpoint] record is written into the log periodically at that point when the system writes out to the database on disk all DBMS buffers that have been modified. As consequence of this, all transactions that have their [commit, T] entries in the log before a [checkpoint] entry don't need to have their write operations redone in case of a system crash, since all their updates will be recorded in the database on disk during checkpointing. The recovery manager of a DBMS must decide at what intervals to take a checkpoint. The interval may be measured in time (e.g. every m minutes) or the number (t) of committed transactions since the last checkpoint, where the values (m) or (t) are system parameters. Taking a checkpoint consists of the following actions: 1. Suspend execution of transactions temporarily. 2. Force-write all main memory buffers that have been modified to disk. 3. Write a [checkpoint] record to the log, and force-write the log to disk. 4. Resume executing transactions. Fuzzy Checkpointing Techniques: In this technique, the system can resume transaction processing after a [begin_checkpoint] record is written to the log without having to wait for step 2 to finish. When step 2 is completed, an [end_checkpoint, …] record is written in the log with the relevant information collected during checkpointing. However, until step 2 is completed, the previous checkpoint record should remain valid. To accomplish this, the system maintains a file on disk that contains a pointer to the valid checkpoint, which continues to point to the previous checkpoint record in the log. Once step 2 is concluded, that pointer is changed to point to the new checkpoint in the log. 7.5 The ARIES Recovery Algorithm We now describe the ARIES (Algorithm for Recovery and Isolation Exploiting Semantics) algorithm an example of a recovery algorithm used in database systems. ARIES uses a Steal / No_Force approach to specify when a page from the DBMS cache can be written to disk.  • Steal: When protocol allows writing an updated page buffer before transaction commits • No Force: When transaction commits, it's not necessary to write immediately all pages updated by this transaction)  So, this algorithm doesn't require large DBMS cache (steal) and it eliminates the I/O cost to read pages again from disk (No_Force). This algorithm has three phases: 1. Analysis phase 2. Redo phase 3. Undo phase 1. Analysis phase: At crash: 1. Read from log appended file the LSN of last checkpoint (Form special file created during checkpointing). This checkpoint is a fuzzy checkpoint. 2. Read from the log actions starting from LSN of the last checkpoint. 3. Read the End checkpoint and recall both (These tables are created during checkpointing) • Transaction table  • Dirty page table  4. Modify the above tables to determine both the redo & undo sets of operations. 2. Redo phase: 1. Determine M (the smallest LSN of LSNs exits in the dirty page table after the analysis phase)  2. Scan log from LSN equal to M until end of log. 3. For each entry in the log: i. Let N= the LSN of the entry. ii. Let P= the page ID of entry. iii. Let D=the LSN of P in the dirty page table iv. If P is not exists in the Dirty page table: 1. Then this change is already written to disk before the checkpoint and need not to be reapplied. v. Else If   N< D. 1. Then this change has been applied to the page.          vi. Else vii. this change (update) must be re-applied. 3. Undo phase: Starts by scanning backward from the end of the log in the undo transaction Set (active [not committed] transactions). Undo set is determined from transaction table. 1. Starts scanning backward from the end of the log in the undo transaction set (active [Not Committed] transactions)   2. Undo the transaction (the appropriate actions) 3. For each undo operation, compensating these changes in the log. The recovery completes at the end of undo phase. 133 Database Recovery & Concurrency  Figure 7.10 Figure 7.10 shows an example of recovery in ARIES. (a) The log at point of crash. (b) The Transaction and Dirty Page Tables at time of checkpoint. (c) The Transaction and Dirty Page Tables after the analysis phase. Important Remarks: Checkpointing in ARIES creates besides the log, two tables: 1. Transaction table. 2. Dirty page table. • The transaction table contains an entry for each active transaction, with information such as the transaction ID, status and LSN of the most recent log record for the transaction. • The dirty page table contains page (Not performed yet).an entry for each dirty page in the buffer which includes the page ID and LSN corresponding to the earliest update to that." } , 
      {"role" : "user" , "content" : "Chapter 8  Distributed Databases Beside centralized and database, that resides on a single hardware machine, with associated secondary storage devices such as disks for on-line database storage and tapes for backup. In recent years, there has been a rapid trend toward the distribution of computer systems over multiple sites that are connected via communication network. 8.1 Reasons for Distribution and DDBMS functions: A distributed DB is a collection of data that belong logically to some systems, but it is physically spread over the sites of a computer network. Several factors have led to the development of DDBSs. The advantage of DDBSs as follows: 1. Distributed nature of some DB applications: Many database applications are naturally distributed over different locations. 2. Increased reliability and availability: Reliability is broadly defined as the probability that the system is up at a particular moment in time, whereas the availability is the probability that the system is continuously available during a time interval. When the data and DBMS software are distributed over   several sites, one site may fail while other sites continue in operation. Only the data and software that exist at the failed site cannot be accessed, other data and software can still be used. 3. Allowing data sharing while maintaining some measure of local control. In DDBSs, it is possible to control the data and the software locally at each site. However, certain data can be accessed by other remote sites through the DDBMS software. 4. Improve performance by distributing a large database over multiple sites, smaller database will exist at each site. Each site will have a smaller number of transactions executing than if all transactions were submitted to a single centralized database. Distribution leads to increase complexity in the system design and implementation.  To satisfactorily achieve the advantages listed above, the DDBMS software must be able to provide additional functions. Some of these are: 1. To access remote sites and transmit queries and data among the various sites via a communication network. 2. To keep track of the data distribution and replication in the DDBMS catalog. 3. To device execution strategies for quires and transactions that access data from more than one site. 4. To decide on which copy of a replicated data item to access. 5. To maintain the consistency of copies of a replicated data item. 6. To recover from individual site crashes and from new types of failures such as failure of a communication link. 136 Database Systems II Architecture of a DDES At the physical hardware level, the main factors that distinguish a DDBS from a centralized system are the following: • There are multiple computers, called sites or nodded. • These sites must be connected by some type of communication network to transmit data and commands among sites, as shown in the following Figure 8.1: Figure 8.1 The sites may all be in the same building or typically within a mile radius and connected via a local area network, or they may be geographically distributed over large distances and connected via a long-level network. Local area networks typically use cable, whereas long-level networks use telephone lines or satellites. Application Processors and Data Processors To manage the complexity of a DDBMS, it is customary to divide software into three main modules: • The data processor (DP) software is responsible for local data management at a site, much like centralized DBMS software. • The Application processor (AP) software is responsible for most of the distribution functions; it accesses data distribution information from the DDBMS catalog and is responsible for processing all requests access to more than one site. • The communication software provides the communications primitives that are used by the AP to transmit commands and data among the various sites as needed. In a DDBS it is possible that some sites contain both AP and DP software, where as other sites contain only one or the other as illustrated in the previous figure. A site is used mainly for DB function is called a back-machine, and a site that is used primarily for the AP function is called a front-end machine. 137 Distributed Databases  Distribution Transparency An important function of the AP is to hide the details of data distribution from the user; that is; the user should write global quires and transactions as through the database were centralized, without having to specify the sites at which the data referenced in global quires and transactions reside. This property is called Distribution Transparency. 8.2 Data Fragmentation, Replication, and Allocation Techniques  This section discusses the techniques that are used to break up the database into logical units, called fragments that may be assigned for storage at various sites. In addition, it discusses the use of data Replication, so certain data may be stored in more than one site, as well as the process of allocating fragments or replicates of fragments. The information concerning data fragmentation, allocation, and replication is stored in a global system catalog that is accessed by AP as needed. Data Fragmentation Before distributing the data, the logical units of the database must be determined. The simplest logical units are the relations themselves; that is, each whole relation will be stored at a particular site. However, in many cases the relation can be divided into smaller logical units for distribution. We have three different types of fragmentation: 1. Horizontal Fragmentation A horizontal fragmentation of a rotation is a subset of the tuples in that rotation. The tuples that belong to the horizontal fragment are specified by a condition on one or more attributes of the relation. 2. Vertical Fragmentation Another type of fragmentation is called vertical fragmentation. A vertical fragment of a relation keeps only certain attributes in the relation that are related together in some way. 3.  Mixed fragmentation Both vertical and horizontal fragments can be intermixed yielding mixed fragments. Remark: The original relation can be reconstructed by applying union and outer join. 138 Database Systems II Data Replication and Allocation Replication is useful in improving the availability of data. The most extreme case is replication of the whole database at every site in the distributed system, thus creating a fully replicated distributed database.  This can improve availability remarkably because the system can continue to operate if at least one site is up, it also improves performance of retrieval for global quires because the result of such a query can be obtained locally from one site.  The disadvantage of full replication is that it can slow down update operations drastically because a single logical update must be performed on every copy of the database to keep the copies consistent.  Also, full replication makes recovery techniques more expensive than if there was no replication.  The other extreme from full replication is to have no replication; that each fragment is stored at exactly one site. In this case, all fragments must be disjoint, except for the repetition of primary keys among vertical (or mixed) fragments. This technique is called non-redundant allocation.  Between these two extremes, we have a wide spectrum of partial replication of the data; that is some fragments of the database may be replicated whereas others are not.  The number of copies of each fragment can range from one to the number of sites in the distributed system. A description of the replication of fragments is sometimes called a replication schema. Each copy of a fragment and the fragment itself must be assigned to a particular site in the distributed system.  This process is called data distribution. The degree of replication depends on some factors: 1. Performance and availability goals of the system. 2. Types and frequencies of transactions submitted at each site. Example of Fragmentation, Allocation and Replication: We now consider an example of fragmenting and distributing the company database previously mentioned. Suppose that the company has three computer sites -one for each current department. Sites 2 & 3 for departments 5 & 4 respectively. At each of these sites, we expect frequent access to the EMPLOYEE and PROJECT information for the employees who work in that department and the projects controlled by that department.  139 Distributed Databases  Further, we assume that these sites mainly access the NAME, SSN, SALARY, and SUPERSSN attributes of EMPLOYEE. Site 1 is used by company headquarter and accesses all employee and project information regularly, in addition to keeping track of Dependent information for insurance purposes. The following figures explains this example: Figure 8.2 140  Database Systems II   141   Figure 8.3   Distributed Databases  8.3 Types of Distributed Database System The term distributed data base management system can be applied to describe a variety of systems that differ from one another in many respects The main factors that differentiate distributed systems are: • Degree of homogeneity of the DDBMS software. If all DPS use identical software and all APS use the same software, the DDBMS is called homogenous; otherwise is called heterogeneous. • Degree of local autonomy: If all access to DDBMS must go through an AP, then the system has no local autonomy. On the other hand, if direct access by local transaction to a DP is permitted, the system has some degree of local autonomy. • Degree of distribution, transparency or alternatively the degree of schema integration: If the user sees a single integrated schema without any information concerning fragmentation, replication, or distribution, the DDBMS is said to have a high degree of distribution transparency (or schema integration). However, in different case if the user sees all fragmentation, allocation, and replication, the DDBMS has no distribution transparency and no schema integration. Concurrency Control and Recovery in Distributed Database  For concurrency control and recovery purposes, numerous problems arise in distributed DBMS environment. Some of these problems are the following: • Dealing with multiply copies of data items The concurrency   control   method   ids   responsible   for   maintaining consistency among these copies. The recovery method is responsible for:   making a copy consistent with other copies if the site on which the copy is stored fails and recovers later. • Failure of individual site The DDBMS should continue to operate with its running sites if possible when one or more individual sites fails. When a site recovers, its local database must be brought up to date with the rest of the sites before it rejoins the system. • Failure of communication links The system must deal with failure of one or more of the communication links that connect the sites. • Distributed commit 142 Database Systems II Problems arise with committing a transaction that is accessing database stored on multiple sites if some sites fail during the commit process, the two-phase commit protocol is often used to deal with this problem. • Distributed deadlock Deadlock may occur among several sites, so techniques for dealing with deadlock must be extended to take this into account. 8.4 Distributed Recovery The recovery process in distributed database is quite involved. We will give only a very brief idea of some of the issues here. In some cases, it is quite difficult even to determine when a site is down without exchanging numerous messages with other sites.  For example, suppose that site X sends a message to site Y and expects a response from X but does not receive it; there are several possible explanations • The message was not delivered to Y because of failure. • Site Y is down and could not respond.  • Site Y is running and pent a response, but the response was not delivered. Without additional information or sending of additional messages, it is difficult to determine what happened. Another problem with Distributed recovery is distributed commit. When a transaction is updating data at several sites, it cannot commit until it is sure that the effect of transaction on every site cannot be lost. This means that every site must have recorded the local effects of the transactions permanently in the local site log on disk. Chapter 9  Data warehouse & Data Mining 9.1 What is a data warehouse? Data warehousing provides architectures and tools for business executives to systematically organize, understand, and use their data to make strategic decisions. Many organizations have found that data warehouse systems are valuable tools in today's competitive, fast evolving world. In the last several years, many firms have spent millions of dollars in building enterprise-wide data warehouses. Many people feel that with competition mounting in every industry, data warehousing is the latest must-have marketing weapon to keep customers by learning more about their needs. Data Warehouse A Data Warehouse refers to a database that is maintained separately from an organization's operational databases. Data warehouse systems allow for the integration of a variety of data sources. They support information processing by providing a solid platform of consolidated, historical data for analysis. Alternatively, a Data Warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management's decision-making process. Subject-oriented: A data warehouse is organized around major subjects, such as customer, vendor, product, and sales. rather than concentrating on the day-to-day operations and transaction processing of an organization, a data warehouse focuses on the modeling and analysis of data for decision makers. Integrated: A data warehouse is usually constructed by integrating multiple heterogeneous sources, such as relational databases, flat files, and on-line transaction records. Data cleaning and data integration techniques are applied to ensure consistency in naming conventions, encoding structures, attribute measures, and so on. Time-variant: Data are stored to provide information from a historical perspective (e.g., the past 5-10 years).Every key structure in the data warehouse contains, either implicitly or explicitly, an element of time. 145 Data warehouse & Data Mining  Nonvolatile: A data warehouse is always a physically separate store of data transformed from the application data found in the operational environment. Due to this separation, a data warehouse does not require transaction processing and recovery mechanisms. It usually requires only two operations in data accessing: initial loading of data and access of data. 9.2 OLTP & OLAP OLTP Systems: They are on-line operational database systems which perform on-line transaction and query processing. OLAP Systems: They are Data warehouse systems, on the other hand, serve users or \"knowledge workers\"(managers, analysts, executives) in the role of data analysis and decision support. Such systems facilitate summarization, aggregation, present data in various formats in order to accommodate the diverse needs of the different users. The major distinguishing features for OLTP and OLAP 1. Users and system orientation: an OLTP system is customer-oriented and is used for transaction and query processing by clerks, clients, and information technology professionals. An OLAP system is market-oriented and is used for data analysis by knowledge workers, including managers, executives, and analysts. 2. Data contents: An OLTP system manages current data that, typically, are too detailed to be easily used for decision making. An OLAP system manages large amounts of historical data. It provides facilities for summarization and aggregation of data to discover knowledge at different levels of granularity. These features make the data easier for use in decision support. 3. Database design: An OLTP system usually adopts an entity-relationship (ER) data model and an application-oriented database design. An OLAP system typically adopts either a star or snowflake model (to be discussed in next sections). These models are subject-oriented database designs to facilitate data analysis tasks."} , 
      {"role": "user", "content": "146 Database Systems II 4. View (Scope): An OLTP system focuses mainly on the current data within an enterprise or department (operational database ), without referring to historical data or data in different organizations. In contrast, OLAP systems deal with information that originates from different organizations ( by the analysis of large volumes of integrated  historical data). Why separate data warehouse from operational databases is recommended? Since operational databases store huge amounts of data\", you observe, \"why not perform on-line analytical processing directly on such databases instead of spending additional time and resources to construct a separate data warehouse?\" Reasons 1. A major reason for such a separation is to  promote the high performance of both systems. Processing OLAP queries in operational databases would substantially degrade the performance of operational tasks. 2. On the other hand, data warehouse queries are often complex. They involve the computation of large groups of data at summarized levels, and require implementation methods based on multidimensional views (i.e. DW is an appropriate platform for data analysis). Figure 1-1 Architecture of typical data warehouse 147 Data warehouse & Data Mining  Data Sources for the Mining Process Data Repositories for the mining process may be : • Transactional Database In general, a transactional database consists of a file where each record represents a transaction.  A transaction typically includes a unique transaction identity number (trans ID), and a list of the items making up the transaction (such as items purchased in a store). • Relational Database A relational database is a collection of tables, built using RDBMS, each of which is assigned a unique name. Each table consists of a set of attributes (columns or fields) and usually stores many tuples (records or rows). 9.3 Data Warehouse A Data Warehouse is a repository of information Collected from multiple sources, stored under a unified schema, and which usually resides at a single site. Data warehouses are constructed via a process of data cleaning, data transformation, data integration, data loading, and periodic data refreshing. In order to facilitate decision making, the data in a data warehouse are organized around major subjects, such as customer, item, supplier, and activity. The data are stored to provide information from a historical perspective (such as from the past 5-10 years) and are typically summarized. For example, rather than storing the details of each sales transaction, the data warehouse may store a summary of the transactions per item type for each store, or summarized to a higher level, for each sales region. A data warehouse is usually modeled by a multidimensional database structure, where each dimension corresponds to an attribute or a set of attributes in the schema, and each cell stores the value of some aggregate measure such as count or sales amount. It provides 148 Database Systems II a multidimensional view of data and allows the pre-computation and fast accessing of summarized data. • Base Cuboid: it is the data cube structure that stores the primitive or lowest level of information. • (Non-base) Cuboids: they correspond to a higher level multidimensional (cube) structure. • Apex Cuboid: the 0_D cuboid which holds the highest level of summarization i.e., the last possible role up operation. • Data Cube: It is a base cuboid together with (Non-base) Cuboids. • Drill-Down and Roll Up: which allow the user to view the data at differing degrees of summarization. 149 Data warehouse & Data Mining  OLAP Operations • Roll-up • Drop-down • Slice • Dice Roll-Up Roll-up performs aggregation on a data cube in any of the following ways: • By climbing up a concept hierarchy for a dimension. • By dimension reduction. Drill-Down 150 Database Systems II Drill-down is the reverse operation of roll-up. It is performed by either of the following ways: • By stepping down a concept hierarchy for a dimension. • By introducing a new dimension. 151 Data warehouse & Data Mining  Slice The slice operation selects one item from one dimension from a given cube and provides a new sub-cube. Consider the following diagram that shows how slice works. 152 Database Systems II Dice Dice selects two or more items from  one or more dimension(s) from a given cube and provides a new sub-cube. Consider the following diagram that shows the dice operation. Remark By providing multidimensional data views and the pre-computation of summarized data, data warehouse systems are well suited for On-Line Analytical Processing, or  OLAP(simple and basic analysis tools).  9.4 Multiple Dimension Data Model • Data Cube  Allows data to be modeled and viewed in multiple dimensions. 153 Data warehouse & Data Mining  • Dimensions  Are the perspectives or entities (Subject-oriented) with respect to which an organization wants to keep records for knowledge discovery.  For example, \"All Electronics\" company may create a sales data warehouse in order to keep records of the store's sales with respect to the dimensions time, item, branch, and location. • Dimension Table  It is the table which describes the dimension.  For example, a dimension table for item may contain the attributes item name, brand, and type. • Facts  Numerical measures by which we want to analyze relationships between dimensions. Examples of facts for a sales data warehouse include dollars sold (sales amount in dollars), units sold (number of units sold) • Fact Table:  Represents the theme for which a multidimensional data model is organized. it contains the names of facts or measures as well as the keys to each of the related dimension table 154 Database Systems II Examples of data Cubes A 2-D view of sales data for \"All Electronics\" according to the dimensions time and item, where the sales are from branches located in the city of Vancouver. The measure displayed is dollars sold. A 3-D view of sales data for \"All Electronics\", according to the dimensions time, item, and location. The measure displayed is dollars sold, as a series of 2D For supplier  (SUP1) a 3-D data cube representation of the data according to the dimensions time, item, and location. The measure displayed is dollars sold. 155 Data warehouse & Data Mining   156   A 4-D data cube representation of sales data, according to the dimensions time, item, location, and supplier. The measure displayed is dollars sold as a series of 3D. • Base cuboid: The cuboid which holds the lowest level of summarization for every dimension • Apex cuboid: The 0-D cuboid which holds the highest level of summarization ( zero dimension). Database Systems II Note:  Lattice of cuboids, making up a 4-D data cube for the dimensions time, item, location, and supplier. each cuboid represents a different degree of summarization. Schemas for multidimensional Databases. 1. Stars schema. 2. Snowflake schema. 3. Fact Constellations schema. Star schema The star schema is a modeling in which the data warehouse contains:  1. A large central table (fact table) 2. A set of smaller attendant tables (dimension tables), one for each dimension The schema graph resembles a starburst 157 Data warehouse & Data Mining  An example of a star schema. \"All Electronics\" Sales are considered along four dimensions, namely time, item, branch, and location. The schema contains a central fact table for sales which contains keys to each of the four dimensions, along with two measures: dollars sold, and units sold. Notice that in the star schema, each dimension is represented by only one table, and each table contains a set of attributes.  For example, the location dimension table contains the attribute set location key, street, city, state, country. This constraint may introduce some redundancy.  For example, \"Vancouver\" and \"Victoria\" are both cities in the Canadian state British Columbia. Entries for such cities in the location dimension table will create redundancy among the attributes state and country, i.e..., Vancouver, British Columbia, Canada) and (.., Victoria, British Columbia, Canada). 158 Database Systems II Snowflake schema The snowflake schema is a variant of the star schema model, where some dimension tables are normalized, (further splitting the dimension tables into additional tables). The resulting schema graph forms a shape like a snowflake. The major difference between the snowflake and star schema models is that the dimension tables of the snowflake model are in normalized form. Such a table is easy to maintain and saves storage space. Since much of this space is redundant data, creating a normalized structure will reduce the overall space requirement. However, the snowflake structure can reduce the effectiveness of browsing since more joins will be needed to execute a query. Consequently, the system performance may be adversely impacted. Performance benchmarking can be used to determine what is best for your design. An example of a snowflake schema for \"All Electronics\" sales is given in the previous figure. Here, the sales fact table is identical to that of the star schema.  The main difference between the two schemas is in the definition of dimension tables. The single dimension table for item in the star schema is normalized in the snowflake schema, resulting in new item and supplier tables.  For example, the item dimension table now contains the attributes supplier key, type, brand, item name, and item key, the latter of which is linked to the supplier dimension table, containing supplier type and supplier key information.  Similarly, the single dimension table for location in the star schema can be normalized into two tables: new location and city. The location key of the new location table now links to the city dimension.  Notice that further normalization can be performed on province or state and country in the snowflake schema when desirable. Remark A compromise between the star schema and the snowflake schema is to adopt a mixed schema where only the very large dimension tables are normalized. Normalizing large dimension tables saves storage space, while keeping small dimension tables unnormalized may reduce the cost and performance degradation due to joins on multiple dimension tables. Doing both may lead to an overall performance gain. However, careful performance tuning could be required to determine which dimension tables should be normalized and split into multiple tables. 160 Database Systems II Fact Constellation Schema Sophisticated applications may require multiple fact tables to share dimension tables.This kind of schema can be viewed as a collection of stars, and hence is called a galaxy schema or a fact constellation. An example of a fact constellation schema, this schema specifies two fact tables, sales, and shipping. The sales table definition is identical to that of the star schema The shipping table has five dimensions, or keys: time key, item key, shipper key, from location, and to location, and two measures: dollars cost, and units shipped.  A fact constellation schema allows dimension tables to be shared between fact tables. For example, the dimensions tables for time, item, and location, are shared between both the sales and shipping fact tables. Remarks • A Data Warehouse: collects information about subjects that span the entire organization, such as customers, items, sales, assets, and personnel, and thus its scope is enterprise wide. For data warehouses, the fact constellation schema is commonly used since it can model multiple, interrelated subjects.  • A Data Mart: is a department subset of the data warehouse that focuses on selected subjects, and thus its scope is department wide. For data marts, the star or snowflake schema are popular since each are geared towards modeling single subjects." } 
]