!pip install -q transformers accelerate datasets peft trl bitsandbytes
!pip install -U bitsandbytes

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from transformers import Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model
import torch
import re

# ------------------------------------------------------
# 1Ô∏è‚É£ Load Dataset
# ------------------------------------------------------
dataset = load_dataset("json", data_files="db_questions_dataset.jsonl")

# ------------------------------------------------------
# 2Ô∏è‚É£ Load Tokenizer + Model
# ------------------------------------------------------
model_name = "Qwen/Qwen2.5-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ------------------------------------------------------
# 3Ô∏è‚É£ Convert Data To ChatML
# ------------------------------------------------------
def format_chatml(example):
    return {
        "text": f"<|im_start|>system\nYou are a helpful SQL assistant.<|im_end|>\n"
                f"<|im_start|>user\n{example['instruction']}<|im_end|>\n"
                f"<|im_start|>assistant\n{example['output']}<|im_end|>"
    }

dataset = dataset.map(format_chatml)
dataset = dataset["train"]

# ------------------------------------------------------
# 4Ô∏è‚É£ Tokenization
# ------------------------------------------------------
def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# ------------------------------------------------------
# 5Ô∏è‚É£ LoRA Config
# ------------------------------------------------------
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# ------------------------------------------------------
# 6Ô∏è‚É£ TrainingArguments
# ------------------------------------------------------
training_args = TrainingArguments(
    output_dir="./qwen_db_finetuned",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    max_steps=200,
    learning_rate=2e-5,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    optim="paged_adamw_8bit",
    report_to="none"
)

# ------------------------------------------------------
# 7Ô∏è‚É£ Data Collator
# ------------------------------------------------------
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# ------------------------------------------------------
# 8Ô∏è‚É£ Trainer
# ------------------------------------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

# ------------------------------------------------------
# 9Ô∏è‚É£ Train
# ------------------------------------------------------
trainer.train()

# ------------------------------------------------------
# üîü Save Model
# ------------------------------------------------------
trainer.save_model("./qwen_db_finetuned")
tokenizer.save_pretrained("./qwen_db_finetuned")

print("‚úÖ Fine-tuning completed successfully!")

# =====================================================================
# =====================================================================
#                üö® STRICT SQL GUARD LAYER
# =====================================================================
# =====================================================================

def is_sql_question(question: str) -> bool:
    """
    Returns True ONLY if the user question is about SQL or Databases.
    """
    sql_keywords = [
        "sql", "database", "databases", "query", "queries", "table", "tables",
        "join", "inner join", "left join", "right join", "full join",
        "primary key", "foreign key", "composite key", "normalization",
        "schema", "index", "indexes", "insert", "update", "delete",
        "select", "where", "group by", "order by", "having",
        "transaction", "commit", "rollback", "constraints"
    ]

    q = question.lower()
    return any(word in q for word in sql_keywords)


def guard_filter(user_question: str) -> bool:
    """Return True if model is allowed to respond."""
    return is_sql_question(user_question)


# =====================================================================
#  üß† ASK USER 20 TIMES (GUARDED)
# =====================================================================

for i in range(20):
    print(f"\nüü¶ Question {i+1}/20")
    user_question = input("Enter your SQL question: ")

    # üö® Guard Check
    if not guard_filter(user_question):
        print("‚ùå This model only answers SQL-related questions.")
        continue  # skip to next question

    print("üü¢ Allowed ‚Äî SQL question detected!")

    # =================================================================
    #  üî• Build Prompt
    # =================================================================
    prompt = (
        "<|im_start|>system\nYou are a helpful SQL assistant. Only answer SQL questions.<|im_end|>\n"
        f"<|im_start|>user\n{user_question}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    #Prepare Input
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # =================================================================
    #  üöÄ Generate Answer
    # =================================================================
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.1,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True
    )

    print("\nüß† Model Output:\n")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
